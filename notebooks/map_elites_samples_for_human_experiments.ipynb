{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from argparse import Namespace\n",
    "from ast import literal_eval\n",
    "import copy\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import textwrap\n",
    "import typing\n",
    "\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "import duckdb\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.axes\n",
    "import matplotlib.pyplot as plt\n",
    "from Levenshtein import distance as _edit_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import tatsu\n",
    "import tatsu.ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src.ast_utils import _extract_game_id, deepcopy_ast, replace_child\n",
    "from src.ast_printer import ast_to_lines\n",
    "from src import fitness_energy_utils as utils\n",
    "from src.fitness_energy_utils import NON_FEATURE_COLUMNS\n",
    "from src.fitness_features import *\n",
    "from src.ast_counter_sampler import *\n",
    "from src.evolutionary_sampler import *\n",
    "from src import fitness_features_by_category, latest_model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='..'))\n",
    "real_game_texts = [ast_printer.ast_to_string(ast, '\\n') for ast in game_asts]\n",
    "# regrown_game_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples.pddl'))\n",
    "# regrown_game_1024_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples-1024.pddl'))\n",
    "# print(len(real_game_texts), len(regrown_game_texts), len(regrown_game_texts) / 98, len(regrown_game_1024_texts), len(regrown_game_1024_texts) / 98)\n",
    "\n",
    "fitness_df = utils.load_fitness_data('../data/fitness_features_1024_regrowths.csv.gz')\n",
    "print(fitness_df.src_file.unique())\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBPLOTS_ADJUST_PARAMS = dict(top=0.925)\n",
    "DEFAULT_IGNORE_METRICS = ['Timestamp']\n",
    "\n",
    "\n",
    "FIGURE_TEMPLATE = r'''\\begin{{figure}}[!htb]\n",
    "% \\vspace{{-0.225in}}\n",
    "\\centering\n",
    "\\includegraphics[width=\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "% \\vspace{{-0.2in}}\n",
    "\\end{{figure}}\n",
    "'''\n",
    "WRAPFIGURE_TEMPLATE = r'''\\begin{{wrapfigure}}{{r}}{{0.5\\linewidth}}\n",
    "\\vspace{{-.3in}}\n",
    "\\begin{{spacing}}{{1.0}}\n",
    "\\centering\n",
    "\\includegraphics[width=0.95\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "\\end{{spacing}}\n",
    "% \\vspace{{-.25in}}\n",
    "\\end{{wrapfigure}}'''\n",
    "\n",
    "SAVE_PATH_PREFIX = './figures'\n",
    "\n",
    "\n",
    "def save_plot(save_path, bbox_inches='tight', should_print=False):\n",
    "    if save_path is not None:\n",
    "        save_path_no_ext = os.path.splitext(save_path)[0]\n",
    "        if should_print:\n",
    "            print('Figure:\\n')\n",
    "            print(FIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "            print('\\nWrapfigure:\\n')\n",
    "            print(WRAPFIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "            print('')\n",
    "        \n",
    "        if not save_path.startswith(SAVE_PATH_PREFIX):\n",
    "            save_path = os.path.join(SAVE_PATH_PREFIX, save_path)\n",
    "        \n",
    "        save_path = os.path.abspath(save_path)\n",
    "        folder, filename = os.path.split(save_path)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=bbox_inches, facecolor=plt.gcf().get_facecolor(), edgecolor='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_REPRESENTATIVE_INDICES = [\n",
    "    42,  # throw\n",
    "    58,  # funny throw with setup\n",
    "    28,  # throw, throw attempt, with setup \n",
    "    88,  # throw, throw attempt, no setup\n",
    "    31,  # throw without in/on\n",
    "    64,  # dropping ball in bin, drop attempt\n",
    "    52,  # ball-less throwing game\n",
    "    86,  # weird throw, with setup\n",
    "    6,  # two weird throwing preferences, setup\n",
    "    17,  # anoter weird multi throwing preference game\n",
    "    96,  # three throwing preferences, setup\n",
    "    \n",
    "    14,  # castle building\n",
    "    45,  # weird building game\n",
    "    49,  # another weird building game\n",
    "    51,  # hybrid throwing/building game\n",
    "    \n",
    "    23,  # single placement preference\n",
    "    44,  # three different placement preferencs \n",
    "\n",
    "    26,  # single preferece that matches none of the others\n",
    "]\n",
    "\n",
    "REAL_GAME_INDICES_TO_IGNORE = [\n",
    "    2,  # used in the context for backtranslation\n",
    "    5,  # used in the context for backtranslation\n",
    "    18, # used in the context for backtranslation\n",
    "    33, # used in the context for backtranslation\n",
    "    44, # used in the context for backtranslation\n",
    "]\n",
    "\n",
    "REAL_GAME_INDICES_TO_INCLUDE = [\n",
    "    0, 4, 6, 7, 11,\n",
    "    14, 17, 23, 26, 28,\n",
    "    31, 32, 35, 37, 40,\n",
    "    41, 42, 45, 49, 51,\n",
    "    52, 55, 58, 59, 64,\n",
    "    74, 88, 90, 94, 96,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace_filter_results_path = '../samples/trace_filter_results_max_exemplar_preferences_by_bcs_with_expected_values_2023_11_29_2023_12_05_1.pkl.gz'\n",
    "model_key = 'max_exemplar_preferences_by_bcs_with_expected_values'\n",
    "model_spec = latest_model_paths.MAP_ELITES_MODELS[model_key]\n",
    "model = typing.cast(MAPElitesSampler, model_spec.load())\n",
    "\n",
    "key_to_real_game_index = defaultdict(list)\n",
    "real_game_index_to_key = {}\n",
    "real_game_fitness_scores = []\n",
    "ALL_REAL_GAME_KEYS = []\n",
    "for i, ast in enumerate(game_asts):\n",
    "    fitness_score, features = model._score_proposal(ast, return_features=True)  # type: ignore\n",
    "    real_game_fitness_scores.append(fitness_score)\n",
    "    key = model._features_to_key(ast, features)\n",
    "    key_to_real_game_index[key].append(i)\n",
    "    real_game_index_to_key[i] = key\n",
    "    ALL_REAL_GAME_KEYS.append(key)\n",
    "\n",
    "trace_filter_results = model_spec.load_trace_filter_data()\n",
    "trace_filter_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_game_index_to_key[5]\n",
    "\n",
    "for key, indices in key_to_real_game_index.items():\n",
    "    if key is None or not isinstance(key, tuple):\n",
    "        continue\n",
    "    if key[2] == real_game_index_to_key[5][2] and key[3] >= 1 and key[1] == real_game_index_to_key[5][1]:\n",
    "        print(key, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out which human games to include\n",
    "* For each cell where there are  > 0 human games, check if there's one in the representative index list that isn't in the ignore list\n",
    "* If so, add it to the list of human games to include\n",
    "* If not, print the list of candidate human games to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, indices in key_to_real_game_index.items():\n",
    "#     if key != None:\n",
    "#         relevant_indices = [i for i in indices if i not in REAL_GAME_INDICES_TO_IGNORE]\n",
    "#         if len(relevant_indices) == 0:\n",
    "#             print('*', key, 'âˆ…')\n",
    "#             continue\n",
    "\n",
    "#         representative_indices = [i for i in relevant_indices if i in NEW_REPRESENTATIVE_INDICES]\n",
    "#         if len(representative_indices) > 0:\n",
    "#             print('*', key, representative_indices, '(r)')\n",
    "\n",
    "#         else:\n",
    "#             print('*', key, relevant_indices)\n",
    "\n",
    "REAL_GAME_KEY_LIST = [real_game_index_to_key[i] for i in REAL_GAME_INDICES_TO_INCLUDE]\n",
    "REAL_GAME_KEYS = set(REAL_GAME_KEY_LIST)\n",
    "print(len(REAL_GAME_KEYS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_real_game_index[(1, 0, 4, 0, 0, 2, 0, 0, 0, 0, 1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 17\n",
    "key = real_game_index_to_key[index]\n",
    "print(index, key)\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "print(ast_printer.ast_to_string(game_asts[index], '\\n'))\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "print(ast_printer.ast_to_string(model.population[key], '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = real_game_index_to_key[83]\n",
    "print(key, '\\n')\n",
    "print(ast_printer.ast_to_string(model.population[key], '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out which model games to include\n",
    "* We're taking the 30 from the keys used in the human games\n",
    "* We want $n = 40$ additional ones:\n",
    "    * All 40 with the behavioral feature expected values key set to 1\n",
    "    * 10 with each preference count\n",
    "    * 20 with/without setup\n",
    "    * With each of the 9 preference BCs on 20 times and off 20 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_FEATURE_INDICES = list(range(3, 12))\n",
    "\n",
    "\n",
    "def sample_novel_map_elites_keys(n: int, model: MAPElitesSampler, trace_filter_results: dict,\n",
    "                                 real_game_keys: typing.Set[KeyTypeAnnotation] = REAL_GAME_KEYS,\n",
    "                                 random_seed: int = DEFAULT_RANDOM_SEED, max_pref_count: int = 4,\n",
    "                                 binary_feature_indices: typing.List[int] = BINARY_FEATURE_INDICES,\n",
    "                                 pref_count_feature_index: int = 2,\n",
    "                                 ) -> typing.List[KeyTypeAnnotation]:\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    sampled_keys = []\n",
    "    candidate_keys = set([key for key in model.population.keys() if key[0] == 1]) - real_game_keys  # type: ignore\n",
    "    trace_filter_failed_keys = set([key for key, result in trace_filter_results['summary'].items() if result == 0])\n",
    "    candidate_keys.difference_update(trace_filter_failed_keys)\n",
    "    valid_pref_counts = list(range(1, max_pref_count + 1))\n",
    "    \n",
    "    pref_count_max_count = n // len(valid_pref_counts) \n",
    "    extra_for_next_pref_count = 0\n",
    "\n",
    "    for pref_count in valid_pref_counts:\n",
    "        sampled_keys_current_pref_count = []\n",
    "        candidate_keys_current_pref_count = set([key for key in candidate_keys if key[pref_count_feature_index] == pref_count])  # type: ignore\n",
    "        \n",
    "        if len(candidate_keys_current_pref_count) < pref_count_max_count:\n",
    "            extra_for_next_pref_count += pref_count_max_count - len(candidate_keys_current_pref_count)\n",
    "            sampled_keys_current_pref_count.extend(list(candidate_keys_current_pref_count))\n",
    "            candidate_keys_current_pref_count.clear()\n",
    "        \n",
    "        else:\n",
    "            count_by_binary_value = {idx: 0 for idx in binary_feature_indices}\n",
    "\n",
    "            while len(sampled_keys_current_pref_count) < pref_count_max_count + extra_for_next_pref_count:\n",
    "                min_value = min(count_by_binary_value.values())\n",
    "                binary_indices_above_min_value = [idx for idx in binary_feature_indices if count_by_binary_value[idx] > min_value]\n",
    "                relevant_keys = [key for key in candidate_keys_current_pref_count if all(key[idx] == 0 for idx in binary_indices_above_min_value)]  # type: ignore\n",
    "                if len(relevant_keys) == 0:\n",
    "                    # Ran out of keys with current min, so increment everywhere to current min\n",
    "                    count_by_binary_value = {idx: min_value for idx in binary_feature_indices}\n",
    "                    continue\n",
    "\n",
    "                key_index = rng.integers(len(relevant_keys))\n",
    "                key = relevant_keys[key_index]\n",
    "                sampled_keys_current_pref_count.append(key)\n",
    "                candidate_keys_current_pref_count.discard(key)\n",
    "                for i in binary_feature_indices:\n",
    "                    count_by_binary_value[i] += key[i] > 0\n",
    "\n",
    "            sampled_keys.extend(sampled_keys_current_pref_count)\n",
    "            extra_for_next_pref_count = 0\n",
    "            \n",
    "    return sampled_keys\n",
    "\n",
    "\n",
    "novel_archive_cell_keys = sample_novel_map_elites_keys(40, model, trace_filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./games_for_human_experiment/human_games.pddl', 'w') as f:\n",
    "    for index in REAL_GAME_INDICES_TO_INCLUDE:\n",
    "        f.write(f'; Index #{index} with key {real_game_index_to_key[index]} \\n')\n",
    "        f.write(ast_printer.ast_to_string(game_asts[index], '\\n'))\n",
    "        f.write('\\n\\n')\n",
    "\n",
    "\n",
    "with open('./games_for_human_experiment/human_cell_archive_games.pddl', 'w') as f:\n",
    "    for key in REAL_GAME_KEYS:\n",
    "        f.write(f'; Key {key}\\n')\n",
    "        f.write(ast_printer.ast_to_string(model.population[key], '\\n'))\n",
    "        f.write('\\n\\n')\n",
    "\n",
    "\n",
    "with open('./games_for_human_experiment/novel_archive_cell_games.pddl', 'w') as f:\n",
    "    for key in novel_archive_cell_keys:\n",
    "        f.write(f'; Key {key}\\n')\n",
    "        f.write(ast_printer.ast_to_string(model.population[key], '\\n'))\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the JSON structures because I messed the keys up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./games_for_human_experiment/human_cell_archive_games_translations.json') as f:\n",
    "    human_cell_archive_games_translations = json.load(f)\n",
    "\n",
    "modified_human_cell_archive_games_translations = {\n",
    "    str(tuple(game['key'])): game['text'] for game in human_cell_archive_games_translations\n",
    "}\n",
    "\n",
    "with open('./games_for_human_experiment/modified_human_cell_archive_games_translations.json', 'w') as f:\n",
    "    json.dump(modified_human_cell_archive_games_translations, f, indent=2)\n",
    "\n",
    "with open('./games_for_human_experiment/human_games_translations.json') as f:\n",
    "    human_games = json.load(f)\n",
    "\n",
    "human_game_texts = [game['text'] for game in human_games]\n",
    "modified_human_game_translations = {\n",
    "    str(real_game_index_to_key[real_game_index]): human_game_texts[i]\n",
    "    for i, real_game_index in enumerate(REAL_GAME_INDICES_TO_INCLUDE)\n",
    "}\n",
    "\n",
    "with open('./games_for_human_experiment/modified_human_games_translations.json', 'w') as f:\n",
    "    json.dump(modified_human_game_translations, f, indent=2)\n",
    "\n",
    "with open('./games_for_human_experiment/novel_archive_cell_games_translations.json') as f:\n",
    "    novel_archive_cell_games_translations = json.load(f)\n",
    "\n",
    "modified_novel_archive_cell_games_translations = {\n",
    "    str(tuple(game['key'])): game['text'] for game in novel_archive_cell_games_translations\n",
    "}\n",
    "\n",
    "with open('./games_for_human_experiment/modified_novel_archive_cell_games_translations.json', 'w') as f:\n",
    "    json.dump(modified_novel_archive_cell_games_translations, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using these to make tables for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATIONS_DIR = '../llm_tests/translations'\n",
    "TRANSLATION_DATE = '2024_01_12'\n",
    "\n",
    "with open(f'{TRANSLATIONS_DIR}/human_games_translations_split_{TRANSLATION_DATE}.json') as f:\n",
    "    human_game_texts = json.load(f)\n",
    "\n",
    "with open(f'{TRANSLATIONS_DIR}/human_cell_archive_games_translations_split_{TRANSLATION_DATE}.json') as f:\n",
    "    human_cell_archive_game_texts = json.load(f)\n",
    "\n",
    "with open(f'{TRANSLATIONS_DIR}/novel_archive_cell_games_translations_split_{TRANSLATION_DATE}.json') as f:\n",
    "    novel_archive_cell_game_texts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing fitness by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_real_game_fitness_scores = [real_game_fitness_scores[i] for i in REAL_GAME_INDICES_TO_INCLUDE]\n",
    "matched_game_fitness_scores = [model.fitness_values[literal_eval(key)] for key in human_cell_archive_game_texts.keys()]\n",
    "unmatched_game_fitness_scores = [model.fitness_values[literal_eval(key)] for key in novel_archive_cell_game_texts.keys()]\n",
    "unmatched_game_fitness_scores_without_bottom_10 = list(sorted(unmatched_game_fitness_scores))[10:]\n",
    "\n",
    "fitness_scores_by_type = dict(real=selected_real_game_fitness_scores, matched=matched_game_fitness_scores, unmatched=unmatched_game_fitness_scores, unmatched_top_30=unmatched_game_fitness_scores_without_bottom_10)\n",
    "\n",
    "tab = '    '\n",
    "for first, second in itertools.combinations(fitness_scores_by_type.keys(), 2):\n",
    "    first_data = fitness_scores_by_type[first]\n",
    "    second_data = fitness_scores_by_type[second]\n",
    "    result = stats.ttest_ind(first_data, second_data)\n",
    "    stars = '*' * int(result.pvalue < 0.05) + '*' * int(result.pvalue < 0.01) + '*' * int(result.pvalue < 0.001)\n",
    "    print(f'{first} (\\u03bc = {np.mean(first_data):.2f}) vs {second} (\\u03bc = {np.mean(second_data):.2f})')\n",
    "    p = result.pvalue\n",
    "    p_value_str = f'p-value < 1e-7 {stars}' if p < 1e-7 else (f'p-value < 1e-5 {stars}' if p < 1e-5 else f'p-value = {p:.4f} {stars}')\n",
    "    print(f'{tab}t-statistic = {result.statistic:.3f}, {p_value_str}')\n",
    "    print()\n",
    "    # print(f'{tab}p-value    = {result.pvalue:.3e} {stars}')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.violinplot(data=fitness_scores_by_type, palette='tab10', inner=None, alpha=0.75)\n",
    "sns.swarmplot(data=fitness_scores_by_type, color='white', dodge=False, size=10, alpha=0.25)\n",
    "sns.pointplot(data=fitness_scores_by_type, errorbar=('ci', 95), linestyle='none', color='black', markers='d', markersize=15)\n",
    "\n",
    "plt.xlabel('Game Type')\n",
    "plt.ylabel('Fitness Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_distance(t1, t2):\n",
    "    return sum(abs(x - y) for x, y in zip(t1, t2))\n",
    "\n",
    "\n",
    "def min_tuple_distance(t, reference_ts):\n",
    "    return min(tuple_distance(t, ref_t) for ref_t in reference_ts)\n",
    "\n",
    "\n",
    "real_game_keys_to_scores = {REAL_GAME_KEY_LIST[i]: real_game_fitness_scores[idx] for i, idx in enumerate(REAL_GAME_INDICES_TO_INCLUDE)}\n",
    "matched_game_key_to_scores = {key: model.fitness_values[literal_eval(key)] for key in human_cell_archive_game_texts.keys()}\n",
    "unmatched_game_key_to_scores = {key: model.fitness_values[literal_eval(key)] for key in novel_archive_cell_game_texts.keys()}\n",
    "unmatched_game_key_to_scores_without_bottom_10 = {key: model.fitness_values[literal_eval(key)] for key in novel_archive_cell_game_texts.keys()}\n",
    "unmatched_game_key_to_scores_without_bottom_10 = {k: v for k, v in unmatched_game_key_to_scores_without_bottom_10.items() if v in unmatched_game_fitness_scores_without_bottom_10}\n",
    "\n",
    "\n",
    "keys_to_fitness_scores = dict(\n",
    "    real=real_game_keys_to_scores, \n",
    "    matched=matched_game_key_to_scores, \n",
    "    unmatched=unmatched_game_key_to_scores, \n",
    "    unmatched_top_30=unmatched_game_key_to_scores_without_bottom_10\n",
    ")\n",
    "\n",
    "df_rows = [\n",
    "    {'full_game_id': f'{key}-{game_category}', 'fitness': score,  'archive_distance': min_tuple_distance(literal_eval(key) if isinstance(key, str) else key, REAL_GAME_KEY_LIST)}\n",
    "    for game_category, key_to_scores in keys_to_fitness_scores.items()\n",
    "    for key, score in key_to_scores.items()\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(df_rows)\n",
    "df.to_csv('./human_evals_data/full_game_id_to_fitness.csv', index=False)\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual tables now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTLISTING_BEGIN = r'\\begin{lstlisting}[aboveskip=-0.4 \\baselineskip,belowskip=-0.8 \\baselineskip]'\n",
    "LSTLISTING_END = r'\\end{lstlisting}'\n",
    "SECTION_HEADERS = ['Setup:', 'Gameplay:', 'Terminal:', 'Scoring:']\n",
    "\n",
    "\n",
    "def combine_close_parens(game_str: str, line_sep: str = '\\n') -> str:\n",
    "    lines = game_str.split(line_sep)\n",
    "    new_lines = []\n",
    "    additional_close_paren_count = 0\n",
    "    indent = ''\n",
    "    for line in lines:\n",
    "        if line.strip() == ')':\n",
    "            additional_close_paren_count += 1\n",
    "            indent = line[:line.index(')')]\n",
    "        else:\n",
    "            if additional_close_paren_count > 0:\n",
    "                new_lines.append(indent + (')' * additional_close_paren_count))\n",
    "                additional_close_paren_count = 0\n",
    "            new_lines.append(line)\n",
    "\n",
    "    if additional_close_paren_count > 0:\n",
    "        new_lines.append(indent + (')' * additional_close_paren_count))\n",
    "        \n",
    "    return line_sep.join(new_lines)\n",
    "\n",
    "\n",
    "def _preprocess_game_text(string: str, game_text_size: str = 'small', remove_line_breaks: bool = True) -> str:\n",
    "    for header in SECTION_HEADERS:\n",
    "        string = string.replace(header, f' \\\\textbf{{{header}}}')\n",
    "\n",
    "    if remove_line_breaks:\n",
    "        string = string.replace('\\n', '')\n",
    "\n",
    "    return f'{{ \\\\{game_text_size} {string}  }}'\n",
    "\n",
    "\n",
    "def _preprocess_game_dsl(string: str) -> str:\n",
    "    s = combine_close_parens(string).replace('-objects-room-v1', '')\n",
    "    return f'{LSTLISTING_BEGIN}\\n{s} {LSTLISTING_END}'\n",
    "\n",
    "\n",
    "def make_comparison_table(real_game_indices: typing.List[int], game_text_size: str = 'small'):\n",
    "    for idx in real_game_indices:\n",
    "        if idx not in REAL_GAME_INDICES_TO_INCLUDE:\n",
    "            raise ValueError(f'Invalid real game index: {idx}')\n",
    "\n",
    "    real_game_keys = [real_game_index_to_key[idx] for idx in real_game_indices]\n",
    "\n",
    "    table_rows = [\n",
    "        r'\\begin{table}[!h]', \n",
    "        r'\\begin{tabular}{|p{\\textcolumnwidth}|p{\\gamecolumnwidth}|p{\\gamecolumnwidth}|p{\\gamecolumnwidth}|}',\n",
    "        r'\\toprule',\n",
    "    ]\n",
    "    # header row\n",
    "    headers = [r'\\textbf{Game ID}'] + [f'\\\\textbf{{Participant Game \\\\#{idx}}} (fitness {real_game_fitness_scores[idx]:.3f})' for idx in real_game_indices]\n",
    "    table_rows.append(' & '.join(headers) + r' \\\\')\n",
    "    table_rows.append(r'\\midrule')\n",
    "    \n",
    "    # real game text row\n",
    "    real_game_texts = [r'\\textbf{Participant Game Text (GPT-4 back-translated)}'] + [_preprocess_game_text(human_game_texts[str(key)], game_text_size) for key in real_game_keys]\n",
    "    table_rows.append(' & '.join(real_game_texts) + r' \\\\')\n",
    "\n",
    "    # real game program row\n",
    "    game_program_strings = [_preprocess_game_dsl(ast_printer.ast_to_string(typing.cast(tatsu.ast.AST, game_asts[idx]), '\\n')) for idx in real_game_indices]\n",
    "    real_game_programs = [r'\\textbf{Participant Game DSL Program}'] + game_program_strings\n",
    "    table_rows.append(' & '.join(real_game_programs) + r' \\\\')\n",
    "    table_rows.append(r'\\midrule')\n",
    "\n",
    "    # model header row\n",
    "    model_headers = [r'\\textbf{Model Sample}'] + [f'\\\\textbf{{Model Sample}} (fitness {model.fitness_values[key]:.3f})' for key in real_game_keys]\n",
    "    table_rows.append(' & '.join(model_headers) + r' \\\\')\n",
    "    table_rows.append(r'\\midrule')\n",
    "\n",
    "    # model game text row\n",
    "    model_game_texts = [r'\\textbf{Model Game Text (GPT-4 back-translated)}'] + [_preprocess_game_text(human_cell_archive_game_texts[str(key)], game_text_size) for key in real_game_keys]\n",
    "    table_rows.append(' & '.join(model_game_texts) + r' \\\\')\n",
    "    \n",
    "    # model game program row\n",
    "    model_program_strings = [_preprocess_game_dsl(ast_printer.ast_to_string(typing.cast(tatsu.ast.AST, model.population[key]), '\\n')) for key in real_game_keys]\n",
    "    model_game_programs = [r'\\textbf{Model Game DSL Program}'] + model_program_strings\n",
    "    table_rows.append(' & '.join(model_game_programs) + r' \\\\')\n",
    "\n",
    "    table_rows.append(r'\\bottomrule')\n",
    "    table_rows.append(r'\\end{tabular}')\n",
    "    table_rows.append(r'\\end{table}')\n",
    "\n",
    "    return '\\n'.join(table_rows)\n",
    "\n",
    "\n",
    "def make_novel_games_table(novel_keys: typing.List[KeyTypeAnnotation], game_text_size: str = 'small', remove_line_breaks: bool = False):\n",
    "\n",
    "    table_rows = [\n",
    "        r'\\begin{table}[!h]', \n",
    "        r'\\begin{tabular}{|p{\\textcolumnwidth}|p{\\gamecolumnwidth}|p{\\gamecolumnwidth}|p{\\gamecolumnwidth}|}',\n",
    "        r'\\toprule',\n",
    "    ]\n",
    "\n",
    "    # model header row\n",
    "    model_headers = [r'\\textbf{Game ID}'] + [f'\\\\textbf{{Model Sample}} (fitness {model.fitness_values[key]:.3f})' for key in novel_keys]\n",
    "    table_rows.append(' & '.join(model_headers) + r' \\\\')\n",
    "    table_rows.append(r'\\midrule')\n",
    "\n",
    "    # model game text row\n",
    "    model_game_texts = [r'\\textbf{Model Game Text (GPT-4 back-translated)}'] + [_preprocess_game_text(novel_archive_cell_game_texts[str(key)], game_text_size, remove_line_breaks=remove_line_breaks) for key in novel_keys]\n",
    "    table_rows.append(' & '.join(model_game_texts) + r' \\\\')\n",
    "    \n",
    "    # model game program row\n",
    "    model_program_strings = [_preprocess_game_dsl(ast_printer.ast_to_string(typing.cast(tatsu.ast.AST, model.population[key]), '\\n')) for key in novel_keys]\n",
    "    model_game_programs = [r'\\textbf{Model Game DSL Program}'] + model_program_strings\n",
    "    table_rows.append(' & '.join(model_game_programs) + r' \\\\')\n",
    "\n",
    "    table_rows.append(r'\\bottomrule')\n",
    "    table_rows.append(r'\\end{tabular}')\n",
    "    table_rows.append(r'\\end{table}')\n",
    "\n",
    "    return '\\n'.join(table_rows)\n",
    "\n",
    "\n",
    "# print(make_comparison_table([0, 14, 31]))\n",
    "# all_novel_keys = list(novel_archive_cell_game_texts.keys())\n",
    "unmatched_top_30_keys = list(unmatched_game_key_to_scores_without_bottom_10.keys())\n",
    "seed = 3342\n",
    "n_novel_keys_to_sample = 3\n",
    "rng = np.random.default_rng(seed)\n",
    "key_indices = rng.choice(len(unmatched_top_30_keys), n_novel_keys_to_sample, replace=False)\n",
    "novel_game_keys = [literal_eval(unmatched_top_30_keys[i]) for i in key_indices]\n",
    "print(make_novel_games_table(novel_game_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipper_merge(*args):\n",
    "    return list(itertools.chain.from_iterable(itertools.zip_longest(*args)))\n",
    "\n",
    "\n",
    "def make_comparison_tables_for_slides(real_game_indices: typing.List[int], game_text_size: str = 'large'):\n",
    "    for idx in real_game_indices:\n",
    "        if idx not in REAL_GAME_INDICES_TO_INCLUDE:\n",
    "            raise ValueError(f'Invalid real game index: {idx}')\n",
    "\n",
    "    real_game_keys = [real_game_index_to_key[idx] for idx in real_game_indices]\n",
    "\n",
    "    table_rows = [[\n",
    "        r'\\begin{table}[!h]', \n",
    "        r'\\begin{tabular}{|p{\\textcolumnwidth}|p{\\gamecolumnwidth}|p{\\gamecolumnwidth}|}',\n",
    "        r'\\toprule',\n",
    "    ] for _ in range(len(real_game_keys))]\n",
    "\n",
    "    # header row\n",
    "    real_game_headers = [f'\\\\textbf{{Participant Game \\\\#{idx}}} (fitness {real_game_fitness_scores[idx]:.3f})' for idx in real_game_indices]\n",
    "    model_game_headers = [f'\\\\textbf{{Model Sample}} (fitness {model.fitness_values[key]:.3f})' for key in real_game_keys]\n",
    "\n",
    "    for i, (real_game_header, model_game_header) in enumerate(zip(real_game_headers, model_game_headers)):\n",
    "        combined_headers = [r'\\textbf{Game ID}', real_game_header, model_game_header]\n",
    "        table_rows[i].append(' & '.join(combined_headers) + r' \\\\')\n",
    "        table_rows[i].append(r'\\midrule')\n",
    "    \n",
    "    # real game text row\n",
    "    real_game_texts =  [_preprocess_game_text(human_game_texts[str(key)], game_text_size=game_text_size, remove_line_breaks=False) for key in real_game_keys]\n",
    "    model_game_texts = [_preprocess_game_text(human_cell_archive_game_texts[str(key)], game_text_size=game_text_size, remove_line_breaks=False) for key in real_game_keys]\n",
    "    for i, (real_game_text, model_game_text) in enumerate(zip(real_game_texts, model_game_texts)):\n",
    "        combined_game_texts = [r'\\textbf{Game Text (GPT-4 back-translated)}', real_game_text, model_game_text]\n",
    "        table_rows[i].append(' & '.join(combined_game_texts) + r' \\\\')\n",
    "\n",
    "    # real game program row\n",
    "    real_game_program_strings = [_preprocess_game_dsl(ast_printer.ast_to_string(typing.cast(tatsu.ast.AST, game_asts[idx]), '\\n')) for idx in real_game_indices]\n",
    "    model_program_strings = [_preprocess_game_dsl(ast_printer.ast_to_string(typing.cast(tatsu.ast.AST, model.population[key]), '\\n')) for key in real_game_keys]\n",
    "    for i, (real_game_program_string, model_program_string) in enumerate(zip(real_game_program_strings, model_program_strings)):\n",
    "        game_programs = [r'\\textbf{Game DSL Program}', real_game_program_string, model_program_string]\n",
    "        table_rows[i].append(' & '.join(game_programs) + r' \\\\')\n",
    "\n",
    "    for i in range(len(real_game_keys)):\n",
    "        table_rows[i].append(r'\\bottomrule')\n",
    "        table_rows[i].append(r'\\end{tabular}')\n",
    "        table_rows[i].append(r'\\end{table}')\n",
    "\n",
    "        print('\\n'.join(table_rows[i]))\n",
    "        print()\n",
    "        print('=' * 80)\n",
    "        print()\n",
    "\n",
    "    # return '\\n'.join(table_rows)\n",
    "\n",
    "\n",
    "make_comparison_tables_for_slides([0, 14, 31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_novel_game_tables_for_slides(novel_game_key_groups: typing.List[typing.List[KeyTypeAnnotation]], game_text_size: str = 'large'):\n",
    "    for group in novel_game_key_groups:\n",
    "        for key in group:\n",
    "            if str(key) not in novel_archive_cell_game_texts:\n",
    "                raise ValueError(f'Invalid novel game key: {key}')\n",
    "            \n",
    "    novel_game_key_groups = [\n",
    "        [literal_eval(key) if isinstance(key, str) else key for key in key_group]\n",
    "        for key_group in novel_game_key_groups\n",
    "    ]\n",
    "\n",
    "    for key_group in novel_game_key_groups:\n",
    "        table_rows = [\n",
    "            r'\\begin{table}[!h]', \n",
    "            r'\\begin{tabular}{|p{\\textcolumnwidth}|p{\\gamecolumnwidth}|p{\\gamecolumnwidth}|}',\n",
    "            r'\\toprule',\n",
    "        ] \n",
    "\n",
    "        # header row\n",
    "        headers = [r'\\textbf{Game ID}'] + [f'\\\\textbf{{Novel Model Sample}} (fitness {model.fitness_values[key]:.3f})' for key in key_group]\n",
    "        table_rows.append(' & '.join(headers) + r' \\\\')\n",
    "        table_rows.append(r'\\midrule')\n",
    "    \n",
    "        # game text row\n",
    "        game_texts = [r'\\textbf{Game Text (GPT-4 back-translated)}'] + [_preprocess_game_text(novel_archive_cell_game_texts[str(key)], game_text_size=game_text_size, remove_line_breaks=False) for key in key_group]\n",
    "        table_rows.append(' & '.join(game_texts) + r' \\\\')\n",
    "\n",
    "        # game program row\n",
    "        model_program_strings = [r'\\textbf{Game DSL Program}'] + [_preprocess_game_dsl(ast_printer.ast_to_string(typing.cast(tatsu.ast.AST, model.population[key]), '\\n')) for key in key_group]\n",
    "        table_rows.append(' & '.join(model_program_strings) + r' \\\\')\n",
    "\n",
    "        table_rows.append(r'\\bottomrule')\n",
    "        table_rows.append(r'\\end{tabular}')\n",
    "        table_rows.append(r'\\end{table}')\n",
    "\n",
    "        print('\\n'.join(table_rows))\n",
    "        print()\n",
    "        print('=' * 80)\n",
    "        print()\n",
    "\n",
    "\n",
    "all_novel_keys = list(novel_archive_cell_game_texts.keys())\n",
    "seed = 42\n",
    "n_novel_keys_to_sample = 1\n",
    "n_novel_key_groups = 1\n",
    "n_novel_keys_per_group = n_novel_keys_to_sample // n_novel_key_groups\n",
    "rng = np.random.default_rng(seed)\n",
    "key_indices = rng.choice(len(all_novel_keys), n_novel_keys_to_sample, replace=False)\n",
    "novel_game_key_groups = [\n",
    "    [all_novel_keys[key_indices[group_index * n_novel_keys_per_group] + i] for i in range(n_novel_keys_per_group)]\n",
    "    for group_index in range(n_novel_key_groups)\n",
    "]\n",
    "\n",
    "make_novel_game_tables_for_slides(novel_game_key_groups)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../llm_tests/translations/selected_preferences.json' ) as f:\n",
    "    exemplar_preference_description_dicts = json.load(f)\n",
    "\n",
    "PREF_DELIMITER = '-----Preference 1-----'\n",
    "\n",
    "exemplar_preference_descriptions = {}\n",
    "for key, desc_dict in exemplar_preference_description_dicts.items():\n",
    "    desc = desc_dict['stage_2']\n",
    "    desc = desc.split(PREF_DELIMITER)[1].strip()\n",
    "    exemplar_preference_descriptions[int(key)] = desc\n",
    "\n",
    "\n",
    "exemplar_preference_descriptions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplar_preference_features_to_latex(exemplar_preference_features: typing.Dict[str, int], text_size: str = 'small'):\n",
    "    feature_lines = []\n",
    "    for feature, value in exemplar_preference_features.items():\n",
    "        if value == 0:\n",
    "            continue\n",
    "\n",
    "        if feature == 'at_end_found':\n",
    "            feature_lines.append(r'Uses \\texttt{at\\_end}')\n",
    "            continue\n",
    "\n",
    "        feature_type = 'predicate'\n",
    "        if feature.startswith('predicate_used_'):\n",
    "            feature = feature.replace('predicate_used_', '')\n",
    "\n",
    "        elif feature.startswith('object_category_used_'):\n",
    "            feature = feature.replace('object_category_used_', '')\n",
    "            feature_type = 'object category'\n",
    "\n",
    "        if '|' in feature:\n",
    "            names = feature.split('|')\n",
    "        else:\n",
    "            names = [feature]    \n",
    "            \n",
    "        names = [r'\\texttt{' + name.replace('_', r'\\_') + '}' for name in names]\n",
    "        names_str = ' or '.join(names)\n",
    "        feature_lines.append(f'Uses {feature_type} {names_str}')\n",
    "\n",
    "    features = \"\\n\".join(feature_lines)\n",
    "    return f'{{ \\\\{text_size} {features} }}' \n",
    "\n",
    "\n",
    "def build_exemplar_preference_table(exemplar_preferences: typing.Dict[int, tatsu.ast.AST], backtranslations: typing.Optional[typing.Dict[int, str]] = None,\n",
    "                                    exemplar_preference_features: typing.Optional[typing.Dict[int, typing.Dict[str, int]]] = None, text_size: str  = 'small') :\n",
    "    table_rows = [\n",
    "        r'\\begin{table}[!h]', \n",
    "        r'\\caption{Exemplar preferences used as MAP-Elites behavioral characteristics.}',\n",
    "        r'\\label{tab:exemplar-preferences}',\n",
    "        r'\\begin{tabular}{|p{\\gamecolumnwidth}|p{\\gamecolumnwidth}|p{\\gamecolumnwidth}|}',\n",
    "        r'\\toprule',\n",
    "        r'\\textbf{Exemplar Preference} & \\textbf{Description (GPT-4 back-translated)} & \\textbf{Exemplar Features } \\\\',\n",
    "        r'\\midrule',\n",
    "    ] \n",
    "\n",
    "    for key, ast in exemplar_preferences.items():\n",
    "        row_components = [_preprocess_game_dsl(ast_printer.ast_section_to_string(ast, ast_parser.PREFERENCES, '\\n'))]\n",
    "        if backtranslations is not None:\n",
    "            row_components.append(_preprocess_game_text(backtranslations[key], game_text_size=text_size, remove_line_breaks=False).replace('\\n', '\\n\\n'))\n",
    "\n",
    "        if exemplar_preference_features is not None:\n",
    "            row_components.append(exemplar_preference_features_to_latex(exemplar_preference_features[key], text_size=text_size).replace('\\n', '\\n\\n'))\n",
    "\n",
    "        table_rows.append(' & '.join(row_components) + r' \\\\')\n",
    "\n",
    "    table_rows.append(r'\\bottomrule')\n",
    "    table_rows.append(r'\\end{tabular}')\n",
    "    table_rows.append(r'\\end{table}')\n",
    "\n",
    "    print('\\n'.join(table_rows))\n",
    "    print()\n",
    "    print('=' * 80)\n",
    "    print()\n",
    "\n",
    "\n",
    "# model.custom_featurizer.ast_file_path = '/Users/guydavidson/projects/game-generation-modeling/dsl/interactive-beta.pddl'\n",
    "# model.custom_featurizer._init_exemplars()\n",
    "\n",
    "build_exemplar_preference_table(model.custom_featurizer.exemplar_preference_asts, exemplar_preference_descriptions, model.custom_featurizer.exemplar_features, text_size='scriptsize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.custom_featurizer.exemplar_preference_asts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./games_for_human_experiment/exemplar_preferences.pkl', 'wb') as f:\n",
    "    pickle.dump(model.custom_featurizer.exemplar_preference_asts, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relate how many preferences there are to fitness scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_rows = []\n",
    "\n",
    "n_prefs_to_fitness = defaultdict(list)\n",
    "\n",
    "for key, fitness in model.fitness_values.items():\n",
    "    if key[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    n_prefs = key[2]\n",
    "    n_prefs_to_fitness[n_prefs].append(fitness)\n",
    "    dataframe_rows.append(['model', key, n_prefs, fitness])\n",
    "\n",
    "\n",
    "for n_prefs, fitnesses in n_prefs_to_fitness.items():\n",
    "    print(n_prefs, len(fitnesses), np.mean(fitnesses), np.std(fitnesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_n_prefs_to_fitness = defaultdict(list)\n",
    "\n",
    "for i, key in enumerate(ALL_REAL_GAME_KEYS):\n",
    "    if key is None:\n",
    "        continue\n",
    "    n_prefs = key[2]\n",
    "    ast = game_asts[i]\n",
    "    fitness = model._score_proposal(ast)\n",
    "    human_n_prefs_to_fitness[n_prefs].append(fitness)\n",
    "\n",
    "    dataframe_rows.append(['real', key, n_prefs, fitness])\n",
    "\n",
    "\n",
    "for n_prefs in sorted(human_n_prefs_to_fitness.keys()):\n",
    "    fitnesses = human_n_prefs_to_fitness[n_prefs]\n",
    "    print(n_prefs, len(fitnesses), np.mean(fitnesses), np.std(fitnesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_n_prefs_source(series: pd.Series) -> str:\n",
    "    return f'{series.n_prefs}_{series.source}'\n",
    "\n",
    "fitness_by_prefs_df = pd.DataFrame(dataframe_rows, columns=['source', 'key', 'n_prefs', 'fitness'])\n",
    "fitness_by_prefs_df = fitness_by_prefs_df.assign(n_prefs_source=fitness_by_prefs_df.apply(combine_n_prefs_source, axis=1))\n",
    "fitness_by_prefs_df.head()\n",
    "fitness_by_prefs_df.to_csv('./human_evals_data/fitness_by_prefs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATION_LINEPLOT_KWARGS = dict(lw=1.5, c='black')\n",
    "ANNOTATION_TEXT_KWARGS = dict(fontsize=12, ha='center', va='bottom', weight='bold')\n",
    "\n",
    "\n",
    "def annotate_significance(ax: plt.Axes, pair: typing.Tuple[int, int], data_df: pd.DataFrame, \n",
    "                          attribute: str = 'fitness', group_attribute: str = 'n_prefs',\n",
    "                          y_increment: float = 0, y_margin: float = 0.2, \n",
    "                          bar_y: float = 0.2, x_margin: float = 0.025,\n",
    "                          AST: str = '*', starts_only_y_dec: float = 0.1,\n",
    "                          plot_kwargs: dict = ANNOTATION_LINEPLOT_KWARGS,\n",
    "                          text_kwargs: dict = ANNOTATION_TEXT_KWARGS):\n",
    "    category_to_position = {int(t.get_text()): t._x for t in ax.get_xticklabels() if int(t.get_text()) in pair}\n",
    "\n",
    "    first_data = data_df[data_df[group_attribute] == pair[0]][attribute]\n",
    "    second_data = data_df[data_df[group_attribute] == pair[1]][attribute]\n",
    "    # result = stats.ttest_ind(first_data, second_data)\n",
    "    result = stats.ttest_ind(first_data, second_data)\n",
    "    p_value = result.pvalue\n",
    "    stars = AST * int(p_value < 0.05) + AST * int(p_value < 0.01) + AST * int(p_value < 0.001)\n",
    "    if not stars:\n",
    "        stars = 'n.s.'\n",
    "\n",
    "    y_max = max(first_data.max(), second_data.max())\n",
    "    y_bar_start = y_max + y_margin + y_increment\n",
    "    y_bar_end = y_bar_start + bar_y\n",
    "\n",
    "    points = [\n",
    "        (category_to_position[pair[0]] + x_margin, y_bar_start),\n",
    "        (category_to_position[pair[0]] + x_margin, y_bar_end),\n",
    "        (category_to_position[pair[1]] - x_margin, y_bar_end),\n",
    "        (category_to_position[pair[1]] - x_margin, y_bar_start),\n",
    "    ]\n",
    "    x, y = zip(*points)\n",
    "\n",
    "    ax.plot(list(x), list(y), **plot_kwargs)\n",
    "\n",
    "    middle = (category_to_position[pair[0]] + category_to_position[pair[1]]) / 2\n",
    "    text_height = y_bar_end - starts_only_y_dec if '*' in stars else y_bar_end\n",
    "    ax.text(middle, text_height, stars, **text_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ANNOTATION_INCREMENT = 0.3\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.violinplot(data=fitness_by_prefs_df, x='n_prefs', y='fitness', palette='tab10', hue='source', inner='quart', alpha=0.75, cut=1)\n",
    "# sns.swarmplot(data=fitness_by_prefs_df,  x='n_prefs', y='fitness', color='white', dodge=False, size=10, alpha=0.25)\n",
    "sns.pointplot(data=fitness_by_prefs_df,  x='n_prefs', y='fitness', errorbar=('ci', 95), linestyle='none', color='black', markers='d', markersize=15)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "pairs = [(1, 4), (1, 3), (2, 4), (1, 2), (2, 3), (3, 4)]\n",
    "\n",
    "for i, (low, high) in enumerate(pairs[::-1]):\n",
    "    annotate_significance(ax, (low, high), fitness_by_prefs_df, y_increment=i * ANNOTATION_INCREMENT + 0.1)\n",
    "\n",
    "\n",
    "plt.xlabel('Number of preferences', fontsize=16)\n",
    "plt.ylabel('Fitness Score', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.ylim(33, plt.ylim()[1])\n",
    "save_plot('fitness_by_n_prefs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    no_binarize=False, \n",
    "    no_merge=False, \n",
    "    use_specific_objects_ngram_model=False,\n",
    "    include_predicate_under_modal_terms=False,\n",
    "    include_arg_types_terms=False,\n",
    "    include_compositionality_terms=False,\n",
    ")\n",
    "# featurizer = build_fitness_featurizer(args)\n",
    "\n",
    "preprocessors = []\n",
    "\n",
    "# if not args.no_binarize:\n",
    "#     preprocessors.append(BinarizeFitnessFeatures())\n",
    "\n",
    "# if not args.no_merge and args.include_arg_types_terms:  # the merge is only used for the arg_types featuers\n",
    "#     preprocessors.append(MergeFitnessFeatures(COMMON_SENSE_PREDICATES_FUNCTIONS))\n",
    "\n",
    "featurizer = ASTFitnessFeaturizer(args, preprocessors=preprocessors)\n",
    "\n",
    "for section in ast_parser.SECTION_KEYS:\n",
    "    counter = SectionNodeCount(section)\n",
    "    featurizer.register(counter, section_rule=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_game_keys = list(human_game_texts.keys())\n",
    "index = 14 # 7, 12, 14\n",
    "\n",
    "key = real_game_keys[index]\n",
    "real_index = key_to_real_game_index[literal_eval(key)][0]\n",
    "print(key, real_index, f'{real_game_fitness_scores[real_index]:.3f}')\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "print(human_game_texts[key])\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "print(ast_printer.ast_to_string(game_asts[real_index], '\\n'))\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "print(model.fitness_values[literal_eval(key)])\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "print(human_cell_archive_game_texts[key])\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "print(ast_printer.ast_to_string(model.population[literal_eval(key)], '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_game_texts[str(real_game_index_to_key[83])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_nodes(ast):\n",
    "    return sum(v for k, v in featurizer.parse(ast, return_row=True).items() if k.startswith('node_count'))  # type: ignore\n",
    "\n",
    "node_count_rows = []\n",
    "\n",
    "for i, key in enumerate(ALL_REAL_GAME_KEYS):\n",
    "    if key is None:\n",
    "        continue\n",
    "\n",
    "    ast = game_asts[i]\n",
    "    total_node_count = _count_nodes(ast)\n",
    "    node_count_rows.append(['real', key, total_node_count])\n",
    "\n",
    "\n",
    "for key in human_cell_archive_game_texts.keys():\n",
    "    key = literal_eval(key)\n",
    "    ast = model.population[key]\n",
    "    total_node_count = _count_nodes(ast)\n",
    "    node_count_rows.append(['matched', key, total_node_count])\n",
    "\n",
    "\n",
    "for key in novel_archive_cell_game_texts.keys():\n",
    "    key = literal_eval(key)\n",
    "    ast = model.population[key]\n",
    "    total_node_count = _count_nodes(ast)\n",
    "    node_count_rows.append(['unmatched', key, total_node_count])\n",
    "\n",
    "\n",
    "def full_game_id_to_str(series: pd.Series) -> str:\n",
    "    return f\"{series.game_id}-{series.game_type}\"\n",
    "\n",
    "\n",
    "node_count_df = pd.DataFrame(node_count_rows, columns=['game_type', 'game_id', 'node_count'])\n",
    "node_count_df = node_count_df.assign(full_game_id=node_count_df.apply(full_game_id_to_str, axis=1))\n",
    "node_count_df.to_csv('./human_evals_data/node_counts.csv', index=False)\n",
    "node_count_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_keys = list(novel_archive_cell_game_texts.keys())\n",
    "for i, k in enumerate(novel_keys):\n",
    "    if k not in unmatched_game_key_to_scores_without_bottom_10:\n",
    "        print(i, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_keys = list(novel_archive_cell_game_texts.keys())\n",
    "index = 23\n",
    "key = novel_keys[index]\n",
    "\n",
    "print(key, model.fitness_values[literal_eval(key)])\n",
    "print(novel_archive_cell_game_texts[key])\n",
    "print('\\n\\n' + '=' * 100 + '\\n\\n')\n",
    "print(ast_printer.ast_to_string(model.population[literal_eval(key)], '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(novel_archive_cell_game_texts.items()):\n",
    "    if 'cube' in v:\n",
    "        print(i, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(human_game_texts['(1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in novel_archive_cell_game_texts.items():\n",
    "    if all(s in v for s in ['Stack blocks in specific configurations',]): \n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '(1, 1, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0)'\n",
    "\n",
    "print(key, model.fitness_values[literal_eval(key)])\n",
    "print(novel_archive_cell_game_texts[key])\n",
    "print('\\n\\n' + '=' * 100 + '\\n\\n')\n",
    "print(ast_printer.ast_to_string(model.population[literal_eval(key)], '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.intersection(*[set([1, 2]), set([2, 3])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
