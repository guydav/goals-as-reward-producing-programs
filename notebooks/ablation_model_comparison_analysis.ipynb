{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from argparse import Namespace\n",
    "from ast import literal_eval\n",
    "import copy\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import textwrap\n",
    "import typing\n",
    "\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "import duckdb\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.axes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from Levenshtein import distance as _edit_distance\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import tatsu\n",
    "import tatsu.ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('../reward-machine'))\n",
    "from src.ast_utils import _extract_game_id, deepcopy_ast, replace_child\n",
    "from src.ast_printer import ast_to_lines\n",
    "from src import fitness_energy_utils as utils\n",
    "from src.fitness_energy_utils import NON_FEATURE_COLUMNS\n",
    "from src.fitness_features import *\n",
    "from src.ast_counter_sampler import *\n",
    "from src.evolutionary_sampler import *\n",
    "from src import fitness_features_by_category, latest_model_paths, ast_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='..'))\n",
    "real_game_texts = [ast_printer.ast_to_string(ast, '\\n') for ast in game_asts]\n",
    "# regrown_game_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples.pddl'))\n",
    "# regrown_game_1024_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples-1024.pddl'))\n",
    "# print(len(real_game_texts), len(regrown_game_texts), len(regrown_game_texts) / 98, len(regrown_game_1024_texts), len(regrown_game_1024_texts) / 98)\n",
    "\n",
    "fitness_df = utils.load_fitness_data('../data/fitness_features_1024_regrowths.csv.gz')\n",
    "print(fitness_df.src_file.unique())\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBPLOTS_ADJUST_PARAMS = dict(top=0.925)\n",
    "DEFAULT_IGNORE_METRICS = ['Timestamp']\n",
    "\n",
    "\n",
    "FIGURE_TEMPLATE = r'''\\begin{{figure}}[!htb]\n",
    "% \\vspace{{-0.225in}}\n",
    "\\centering\n",
    "\\includegraphics[width=\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "% \\vspace{{-0.2in}}\n",
    "\\end{{figure}}\n",
    "'''\n",
    "WRAPFIGURE_TEMPLATE = r'''\\begin{{wrapfigure}}{{r}}{{0.5\\linewidth}}\n",
    "\\vspace{{-.3in}}\n",
    "\\begin{{spacing}}{{1.0}}\n",
    "\\centering\n",
    "\\includegraphics[width=0.95\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "\\end{{spacing}}\n",
    "% \\vspace{{-.25in}}\n",
    "\\end{{wrapfigure}}'''\n",
    "\n",
    "SAVE_PATH_PREFIX = './figures'\n",
    "\n",
    "\n",
    "def save_plot(save_path, bbox_inches='tight', should_print=False):\n",
    "    if save_path is not None:\n",
    "        save_path_no_ext = os.path.splitext(save_path)[0]\n",
    "        if should_print:\n",
    "            print('Figure:\\n')\n",
    "            print(FIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "            print('\\nWrapfigure:\\n')\n",
    "            print(WRAPFIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "            print('')\n",
    "        \n",
    "        if not save_path.startswith(SAVE_PATH_PREFIX):\n",
    "            save_path = os.path.join(SAVE_PATH_PREFIX, save_path)\n",
    "        \n",
    "        save_path = os.path.abspath(save_path)\n",
    "        folder, filename = os.path.split(save_path)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=bbox_inches, facecolor=plt.gcf().get_facecolor(), edgecolor='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace_filter_results_path = '../samples/trace_filter_results_max_exemplar_preferences_by_bcs_with_expected_values_2023_11_29_2023_12_05_1.pkl.gz'\n",
    "model_key = 'max_exemplar_preferences_by_bcs_with_expected_values'\n",
    "model_spec = latest_model_paths.MAP_ELITES_MODELS[model_key]\n",
    "baseline_model = typing.cast(MAPElitesSampler, model_spec.load())\n",
    "\n",
    "key_to_real_game_index = defaultdict(list)\n",
    "real_game_index_to_key = {}\n",
    "real_game_fitness_scores = []\n",
    "ALL_REAL_GAME_KEYS = []\n",
    "for i, ast in enumerate(game_asts):\n",
    "    fitness_score, features = baseline_model._score_proposal(ast, return_features=True)  # type: ignore\n",
    "    real_game_fitness_scores.append(fitness_score)\n",
    "    key = baseline_model._features_to_key(ast, features)\n",
    "    key_to_real_game_index[key].append(i)\n",
    "    real_game_index_to_key[i] = key\n",
    "    ALL_REAL_GAME_KEYS.append(key)\n",
    "\n",
    "trace_filter_results = model_spec.load_trace_filter_data()\n",
    "trace_filter_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVE_OCCUPANCY = 'archive_occupancy'\n",
    "\n",
    "\n",
    "def plot_sampler_fitness_trajectory(\n",
    "        evo: PopulationBasedSampler, title: typing.Optional[typing.List[str]] = None, \n",
    "        axsize: typing.Tuple[int, int] = (8, 6),\n",
    "        plot_metrics: typing.Optional[bool] = None, \n",
    "        ignore_metrics: typing.Optional[typing.List[str]] = DEFAULT_IGNORE_METRICS,\n",
    "        subplots_adjust_params: typing.Dict[str, float] = SUBPLOTS_ADJUST_PARAMS,\n",
    "        min_real_game_fitness: typing.Optional[float] = None, \n",
    "        max_real_game_fitness: typing.Optional[float] = None,\n",
    "        mean_real_game_fitness: typing.Optional[float] = None, \n",
    "        archive_occupancy_ignore_fake_bc: bool = True,\n",
    "        fitness_left: bool = True,\n",
    "        vertical: bool = False,\n",
    "        fontsize: int = 16,\n",
    "        save_path: typing.Optional[str] = None): \n",
    "    \n",
    "    if min_real_game_fitness is None or max_real_game_fitness is None:    \n",
    "        min_real_game_fitness =  -1 * evo.fitness_function.score_dict['max']\n",
    "        max_real_game_fitness = -1 * evo.fitness_function.score_dict['min']\n",
    "        mean_real_game_fitness = -1 * evo.fitness_function.score_dict['mean']\n",
    "\n",
    "    if plot_metrics is None:\n",
    "        plot_metrics = hasattr(evo, 'archive_metrics_history') and len(evo.archive_metrics_history) > 0  # type: ignore\n",
    "\n",
    "    if ignore_metrics is None:\n",
    "        ignore_metrics = []\n",
    "    \n",
    "    if not plot_metrics:\n",
    "        layout = (1, 1)\n",
    "    elif vertical:\n",
    "        layout = (2, 1)\n",
    "    else:\n",
    "        layout = (1, 2)\n",
    "\n",
    "    figsize = (axsize[0] * layout[1], axsize[1] * layout[0])\n",
    "    title_fontsize = fontsize + 4\n",
    "\n",
    "    fig, axes = plt.subplots(*layout, figsize=figsize)\n",
    "\n",
    "    mean, max_fit, std = [], [], []\n",
    "    for step_dict in evo.fitness_metrics_history:\n",
    "        mean.append(step_dict['mean'])\n",
    "        max_fit.append(step_dict['max'])\n",
    "        std.append(step_dict['std'])\n",
    "\n",
    "    mean = np.array(mean)\n",
    "    max_fit = np.array(max_fit)\n",
    "    std = np.array(std)\n",
    "    \n",
    "    fitness_ax_index = 0 if fitness_left else 1\n",
    "    fitness_ax = typing.cast(matplotlib.axes.Axes, axes[fitness_ax_index] if plot_metrics else axes)\n",
    "\n",
    "    fitness_ax.plot(mean, label='MAP-Elites fitness mean')\n",
    "    fitness_ax.fill_between(np.arange(len(mean)), mean - std, mean + std, alpha=0.2, label='MAP-Elites fitness std')  # type; ignore\n",
    "    fitness_ax.plot(max_fit, label='MAP-Elites fitness max')\n",
    "\n",
    "    fitness_ax.hlines(min_real_game_fitness, 0, len(mean), label='Real game fitness range', color='black', ls='--')\n",
    "    fitness_ax.hlines(max_real_game_fitness, 0, len(mean), color='black', ls='--')\n",
    "\n",
    "    if mean_real_game_fitness is not None:\n",
    "        fitness_ax.hlines(mean_real_game_fitness, 0, len(mean), label='Real game fitness mean', color='black', ls=':')\n",
    "\n",
    "    fitness_ax.set_xlabel('Generation (index)', fontsize=fontsize)\n",
    "    fitness_ax.set_ylabel('Fitness (arbitrary units)', fontsize=fontsize)\n",
    "\n",
    "    fitness_ax.legend(loc='best', fontsize=fontsize)\n",
    "    fitness_ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "    if title is not None:\n",
    "        if len(title) > 1 or not plot_metrics:\n",
    "            fitness_ax.set_title(title[0], fontsize=title_fontsize)\n",
    "        else:\n",
    "            plt.suptitle(title[0], fontsize=title_fontsize)\n",
    "    \n",
    "    if plot_metrics:\n",
    "        metrics_ax = typing.cast(matplotlib.axes.Axes, axes[1 - fitness_ax_index])\n",
    "\n",
    "        if plot_metrics == ARCHIVE_OCCUPANCY:\n",
    "            relevant_first_occupancies = evo.archive_cell_first_occupied  # type: ignore\n",
    "            if archive_occupancy_ignore_fake_bc:\n",
    "                relevant_first_occupancies = {k: v for k, v in relevant_first_occupancies.items() if k[0] == 1}\n",
    "\n",
    "            first_occupancy_counter = Counter(relevant_first_occupancies.values())\n",
    "            first_occupancy_arr = np.zeros(max(first_occupancy_counter.keys()) + 1)\n",
    "\n",
    "            for k, v in first_occupancy_counter.items():\n",
    "                first_occupancy_arr[k] = v\n",
    "\n",
    "            first_occupancy_cumsum = np.cumsum(first_occupancy_arr)\n",
    "            first_occupancy_cumsum /= first_occupancy_cumsum.max()\n",
    "\n",
    "            metrics_ax.plot(first_occupancy_cumsum)\n",
    "            metrics_ax.set_xlabel('Generation (index)', fontsize=fontsize)\n",
    "            metrics_ax.set_ylabel('Archive occupancy (%)', fontsize=fontsize)\n",
    "\n",
    "        else:\n",
    "            metrics = {key: [] for key in evo.archive_metrics_history[0].keys() if key not in ignore_metrics}  # type: ignore\n",
    "            for step_dict in evo.archive_metrics_history:  # type: ignore\n",
    "                for key, value in step_dict.items():\n",
    "                    if key in metrics:\n",
    "                        metrics[key].append(value)\n",
    "\n",
    "            \n",
    "            for key, values in metrics.items():\n",
    "                metrics_ax.plot(values, label=key.title())\n",
    "\n",
    "            metrics_ax.set_xlabel('Generation', fontsize=fontsize)\n",
    "            metrics_ax.set_ylabel('Number of games reaching threshold', fontsize=fontsize)\n",
    "\n",
    "            metrics_ax.legend(loc='best', fontsize=fontsize)\n",
    "        \n",
    "        metrics_ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "        if title is not None and len(title) > 1:\n",
    "            metrics_ax.set_title(title[1], fontsize=title_fontsize)\n",
    "\n",
    "        plt.subplots_adjust(**subplots_adjust_params)\n",
    "        \n",
    "\n",
    "    if save_path is not None:\n",
    "        save_plot(save_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_sampler_fitness_trajectory(baseline_model, ['Fitness', 'Occupancy'], axsize=(8, 4),\n",
    "                                plot_metrics=ARCHIVE_OCCUPANCY, fitness_left=False,\n",
    "                                save_path='baseline_quantitative_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATION_LINEPLOT_KWARGS = dict(lw=1.5, c='black')\n",
    "ANNOTATION_TEXT_KWARGS = dict(fontsize=12, ha='center', va='bottom', weight='bold')\n",
    "\n",
    "\n",
    "def annotate_significance(ax: plt.Axes, pair: typing.Tuple[int, int], data_df: pd.DataFrame, \n",
    "                          attribute: str = 'fitness', group_attribute: str = 'n_prefs',\n",
    "                          y_increment: float = 0, y_margin: float = 0.2, \n",
    "                          bar_y: float = 0.2, x_margin: float = 0.025,\n",
    "                          AST: str = '*', starts_only_y_dec: float = 0.1,\n",
    "                          plot_kwargs: dict = ANNOTATION_LINEPLOT_KWARGS,\n",
    "                          text_kwargs: dict = ANNOTATION_TEXT_KWARGS):\n",
    "    category_to_position = {int(t.get_text()): t._x for t in ax.get_xticklabels() if int(t.get_text()) in pair}\n",
    "\n",
    "    first_data = data_df[data_df[group_attribute] == pair[0]][attribute]\n",
    "    second_data = data_df[data_df[group_attribute] == pair[1]][attribute]\n",
    "    # result = stats.ttest_ind(first_data, second_data)\n",
    "    result = stats.ttest_ind(first_data, second_data)\n",
    "    p_value = result.pvalue\n",
    "    stars = AST * int(p_value < 0.05) + AST * int(p_value < 0.01) + AST * int(p_value < 0.001)\n",
    "    if not stars:\n",
    "        stars = 'n.s.'\n",
    "\n",
    "    y_max = max(first_data.max(), second_data.max())\n",
    "    y_bar_start = y_max + y_margin + y_increment\n",
    "    y_bar_end = y_bar_start + bar_y\n",
    "\n",
    "    points = [\n",
    "        (category_to_position[pair[0]] + x_margin, y_bar_start),\n",
    "        (category_to_position[pair[0]] + x_margin, y_bar_end),\n",
    "        (category_to_position[pair[1]] - x_margin, y_bar_end),\n",
    "        (category_to_position[pair[1]] - x_margin, y_bar_start),\n",
    "    ]\n",
    "    x, y = zip(*points)\n",
    "\n",
    "    ax.plot(list(x), list(y), **plot_kwargs)\n",
    "\n",
    "    middle = (category_to_position[pair[0]] + category_to_position[pair[1]]) / 2\n",
    "    text_height = y_bar_end - starts_only_y_dec if '*' in stars else y_bar_end  \n",
    "    ax.text(middle, text_height, stars, **text_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.fitness_metrics_history[-1]['mean'], -1 * baseline_model.fitness_function.score_dict['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossover ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_no_custom_ops_key = 'ablation_max_exemplar_preferences_by_bcs_with_expected_values_no_custom_ops'\n",
    "ablation_no_custom_ops_model_spec = latest_model_paths.MAP_ELITES_MODELS[ablation_no_custom_ops_key]\n",
    "ablation_no_custom_ops_model = typing.cast(MAPElitesSampler, ablation_no_custom_ops_model_spec.load())\n",
    "\n",
    "ablation_no_custom_ops_no_crossover_key = 'ablation_max_exemplar_preferences_by_bcs_with_expected_values_no_custom_ops_no_crossover'\n",
    "ablation_no_custom_ops_no_crossover_model_spec = latest_model_paths.MAP_ELITES_MODELS[ablation_no_custom_ops_no_crossover_key]\n",
    "ablation_no_custom_ops_no_crossover_model = typing.cast(MAPElitesSampler, ablation_no_custom_ops_no_crossover_model_spec.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_TO_MODEL = {\n",
    "    'baseline': baseline_model,\n",
    "    'no_custom_ops': ablation_no_custom_ops_model,\n",
    "    'no_custom_ops_no_crossover': ablation_no_custom_ops_no_crossover_model,\n",
    "}\n",
    "\n",
    "df_rows = []\n",
    "\n",
    "for name, model in NAME_TO_MODEL.items():\n",
    "    for key, fitness in model.fitness_values.items():\n",
    "        if key[0] == 1:\n",
    "            df_rows.append([name, key, fitness])\n",
    "\n",
    "\n",
    "ablation_fitness_df = pd.DataFrame(df_rows, columns=['model', 'key', 'fitness'])\n",
    "ablation_fitness_df = ablation_fitness_df.assign(standardized_fitness= (ablation_fitness_df.fitness - ablation_fitness_df.fitness.mean()) / ablation_fitness_df.fitness.std())\n",
    "print(ablation_fitness_df.shape)\n",
    "ablation_fitness_df.groupby('model').agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.violinplot(data=ablation_fitness_df, x='model', y='fitness', palette='tab10', hue='model', inner='quart', alpha=0.75, cut=1)\n",
    "sns.pointplot(data=ablation_fitness_df,  x='model', y='fitness', errorbar=('ci', 95), linestyle='none', color='black', markers='d', markersize=15)\n",
    "\n",
    "\n",
    "plt.xlabel('Ablation', fontsize=16)\n",
    "plt.ylabel('Fitness', fontsize=16)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['Full Model', 'No Custom Ops', 'No Custom Ops\\nNo Crossover'])\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "save_plot('ablation_fitness_violinplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for first_model_name, second_model_name in itertools.combinations(NAME_TO_MODEL.keys(), 2):\n",
    "    first_model_df = ablation_fitness_df[ablation_fitness_df.model == first_model_name]\n",
    "    second_model_df = ablation_fitness_df[ablation_fitness_df.model == second_model_name]\n",
    "\n",
    "    merged_df = first_model_df.merge(second_model_df, on='key', suffixes=(f'_{first_model_name}', f'_{second_model_name}'))\n",
    "\n",
    "    result = stats.ttest_rel(merged_df[f'standardized_fitness_{first_model_name}'], merged_df[f'standardized_fitness_{second_model_name}'])\n",
    "    stars = '*' * int(result.pvalue < 0.05) + '*' * int(result.pvalue < 0.01) + '*' * int(result.pvalue < 0.001)\n",
    "    print(f'{first_model_name} vs {second_model_name}:') \n",
    "    print(f'\\tt-statistic = {result.statistic}')\n",
    "    print(f'\\tp-value = {result.pvalue} {stars}')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_human_fitness = min(real_game_fitness_scores)\n",
    "median_human_fitness = np.median(real_game_fitness_scores)\n",
    "min_human_fitness, median_human_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 0\n",
    "median_count = 0\n",
    "\n",
    "for key, fitness in baseline_model.fitness_values.items():\n",
    "    if key[0] == 1:\n",
    "        if fitness >= min_human_fitness:\n",
    "            min_count += 1\n",
    "        \n",
    "        if fitness >= median_human_fitness:\n",
    "            median_count += 1\n",
    "\n",
    "\n",
    "min_count, median_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nonzero = 0\n",
    "count_any_nonzero = 0\n",
    "\n",
    "for key in baseline_model.population:\n",
    "    if key[0] == 1:\n",
    "        if trace_filter_results['summary'][key] > 0:\n",
    "            count_nonzero += 1\n",
    "        \n",
    "        per_component_count = {k : sum(v.values()) for k, v in trace_filter_results['full'][key].items()}\n",
    "        if any(v > 0 for v in per_component_count.values()):\n",
    "            count_any_nonzero += 1\n",
    "        else:\n",
    "            print(key)\n",
    "\n",
    "\n",
    "count_nonzero, count_any_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "2000 - 1515\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast_printer.ast_to_string(model.population[(1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0)], '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common sense ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_no_play_trace_features_key = 'ablation_max_exemplar_preferences_by_bcs_with_expected_values_no_play_trace_features'\n",
    "ablation_no_play_trace_features_model_spec = latest_model_paths.MAP_ELITES_MODELS[ablation_no_play_trace_features_key]\n",
    "ablation_no_play_trace_features_model = typing.cast(MAPElitesSampler, ablation_no_play_trace_features_model_spec.load())\n",
    "ablation_no_play_trace_features_trace_filter_data = ablation_no_play_trace_features_model_spec.load_trace_filter_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "no_play_trace_feature_games_full_fitness_model_scores = {}\n",
    "\n",
    "\n",
    "for key, game in tqdm(ablation_no_play_trace_features_model.population.items()):\n",
    "    if key[0] == 1:\n",
    "        no_play_trace_feature_games_full_fitness_model_scores[key] = baseline_model._score_proposal(game)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_baseline_full_fitness_model_scores = {k: v for k, v in baseline_model.fitness_values.items() if k[0] == 1}\n",
    "print(np.mean(list(relevant_baseline_full_fitness_model_scores.values())), np.mean(list(no_play_trace_feature_games_full_fitness_model_scores.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablated = []\n",
    "full = []\n",
    "\n",
    "for key in relevant_baseline_full_fitness_model_scores:\n",
    "    ablated.append(no_play_trace_feature_games_full_fitness_model_scores[key])\n",
    "    full.append(relevant_baseline_full_fitness_model_scores[key])\n",
    "\n",
    "\n",
    "print(np.mean(ablated), np.mean(full))\n",
    "\n",
    "stats.ttest_rel(ablated, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_with_trace_filter_difference = {}\n",
    "ablated_scores = []\n",
    "full_scores = []\n",
    "\n",
    "for key, baseline_trace_filter_score in trace_filter_results['summary'].items():\n",
    "    if key[0] == 1:\n",
    "        if key not in ablation_no_play_trace_features_trace_filter_data['summary']:\n",
    "            print(key)\n",
    "            continue\n",
    "\n",
    "        ablation_trace_filter_score = ablation_no_play_trace_features_trace_filter_data['summary'][key]\n",
    "        if baseline_trace_filter_score != ablation_trace_filter_score:\n",
    "            keys_with_trace_filter_difference[key] = (baseline_trace_filter_score, ablation_trace_filter_score)\n",
    "\n",
    "        ablated_scores.append(int(ablation_trace_filter_score > 0))\n",
    "        full_scores.append(int(baseline_trace_filter_score > 0))\n",
    "\n",
    "print(len(keys_with_trace_filter_difference))\n",
    "print(len([k for k, v in keys_with_trace_filter_difference.items() if v[0] > 0 and v[1] == 0]))\n",
    "print(len([k for k, v in keys_with_trace_filter_difference.items() if v[0] == 0 and v[1] > 0]))\n",
    "print(sum(ablated_scores), sum(full_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(ablated_scores, full_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_no_coherence_features_key = 'ablation_max_exemplar_preferences_by_bcs_with_expected_values_no_coherence_features'\n",
    "ablation_no_coherence_features_model_spec = latest_model_paths.MAP_ELITES_MODELS[ablation_no_coherence_features_key]\n",
    "ablation_no_coherence_features_model = typing.cast(MAPElitesSampler, ablation_no_coherence_features_model_spec.load())\n",
    "ablation_no_coherence_features_trace_filter_data = ablation_no_coherence_features_model_spec.load_trace_filter_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "no_coherence_feature_games_full_fitness_model_scores = {}\n",
    "\n",
    "\n",
    "for key, game in tqdm(ablation_no_coherence_features_model.population.items()):\n",
    "    if key[0] == 1:\n",
    "        no_coherence_feature_games_full_fitness_model_scores[key] = baseline_model._score_proposal(game)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_baseline_full_fitness_model_scores = {k: v for k, v in baseline_model.fitness_values.items() if k[0] == 1}\n",
    "print(np.mean(list(relevant_baseline_full_fitness_model_scores.values())), np.mean(list(no_coherence_feature_games_full_fitness_model_scores.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablated = []\n",
    "full = []\n",
    "\n",
    "for key in relevant_baseline_full_fitness_model_scores:\n",
    "    ablated.append(no_coherence_feature_games_full_fitness_model_scores[key])\n",
    "    full.append(relevant_baseline_full_fitness_model_scores[key])\n",
    "\n",
    "\n",
    "stats.ttest_rel(ablated, full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING_AS_ZERO = True\n",
    "\n",
    "keys_with_trace_filter_difference = {}\n",
    "ablated_scores = []\n",
    "full_scores = []\n",
    "missing_count = 0\n",
    "\n",
    "for key, baseline_trace_filter_score in trace_filter_results['summary'].items():\n",
    "    if key[0] == 1:\n",
    "        if key not in ablation_no_coherence_features_trace_filter_data['summary']:\n",
    "            missing_count += 1\n",
    "            if not MISSING_AS_ZERO:\n",
    "                continue\n",
    "        \n",
    "        ablation_trace_filter_score = ablation_no_coherence_features_trace_filter_data['summary'].get(key, 0)\n",
    "\n",
    "        if baseline_trace_filter_score != ablation_trace_filter_score:\n",
    "            keys_with_trace_filter_difference[key] = (baseline_trace_filter_score, ablation_trace_filter_score)\n",
    "\n",
    "        ablated_scores.append(int(ablation_trace_filter_score > 0))\n",
    "        full_scores.append(int(baseline_trace_filter_score > 0))\n",
    "\n",
    "print(len(keys_with_trace_filter_difference), missing_count)\n",
    "print(len([k for k, v in keys_with_trace_filter_difference.items() if v[0] > 0 and v[1] == 0]))\n",
    "print(len([k for k, v in keys_with_trace_filter_difference.items() if v[0] == 0 and v[1] > 0]))\n",
    "print(sum(ablated_scores), sum(full_scores))\n",
    "stats.ttest_rel(ablated_scores, full_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creativity/Complexity ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PREFERENCES_PREFIX = 'num_preferences_defined_'\n",
    "\n",
    "n_prefs_per_game = []\n",
    "\n",
    "for game in game_asts:\n",
    "    features = baseline_model._proposal_to_features(game)\n",
    "    n_prefs_key = [k for k, v in features.items() if k.startswith(NUM_PREFERENCES_PREFIX) and v][0]\n",
    "    n_prefs = int(n_prefs_key[len(NUM_PREFERENCES_PREFIX):])\n",
    "    n_prefs_per_game.append(n_prefs)\n",
    "\n",
    "np.mean(n_prefs_per_game), np.median(n_prefs_per_game), np.std(n_prefs_per_game)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(n_prefs_per_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_by_prefs_df = pd.read_csv('./human_evals_data/fitness_by_prefs.csv')\n",
    "\n",
    "ANNOTATION_INCREMENT = 0.3\n",
    "FONTSIZE = 16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 2])\n",
    "\n",
    "n_prefs_ax = plt.subplot(gs[0])\n",
    "n_prefs_ax.hist(n_prefs_per_game, bins=range(1, 8), align='left')\n",
    "n_prefs_ax.set_xlabel('Number of preferences defined', fontsize=FONTSIZE)\n",
    "n_prefs_ax.set_ylabel('Number of games', fontsize=FONTSIZE)\n",
    "n_prefs_ax.tick_params(axis='both', which='major', labelsize=FONTSIZE)\n",
    "n_prefs_ax.set_xticks(range(1, 7))\n",
    "n_prefs_ax.set_title('Number of preferences in real games', fontsize=FONTSIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fitness_ax = plt.subplot(gs[1])\n",
    "sns.violinplot(ax=fitness_ax, data=fitness_by_prefs_df, x='n_prefs', y='fitness', palette='tab10', hue='source', inner='quart', alpha=0.75, cut=1)\n",
    "# sns.swarmplot(data=fitness_by_prefs_df,  x='n_prefs', y='fitness', color='white', dodge=False, size=10, alpha=0.25)\n",
    "sns.pointplot(ax=fitness_ax, data=fitness_by_prefs_df,  x='n_prefs', y='fitness', errorbar=('ci', 95), linestyle='none', color='black', markers='d', markersize=15)\n",
    "\n",
    "\n",
    "pairs = [(1, 4), (1, 3), (2, 4), (1, 2), (2, 3), (3, 4)]\n",
    "\n",
    "for i, (low, high) in enumerate(pairs[::-1]):\n",
    "    annotate_significance(fitness_ax, (low, high), fitness_by_prefs_df, y_increment=i * ANNOTATION_INCREMENT + 0.1)\n",
    "\n",
    "\n",
    "fitness_ax.set_xlabel('Number of preferences defined', fontsize=FONTSIZE)\n",
    "fitness_ax.set_ylabel('Fitness Score (arbitrary units)', fontsize=FONTSIZE)\n",
    "fitness_ax.tick_params(axis='both', which='major', labelsize=FONTSIZE)\n",
    "fitness_ax.legend(fontsize=14)\n",
    "fitness_ax.set_ylim(33, fitness_ax.get_ylim()[1])\n",
    "fitness_ax.set_title('Fitness by number of preferences defined', fontsize=FONTSIZE)\n",
    "save_plot('complexity_n_prefs_combined.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcs_ablation_with_pref_count_key = 'bcs_ablation_predicate_and_object_groups_setup_at_end_pref_count_expected_values'\n",
    "bcs_ablation_with_pref_count_model_spec = latest_model_paths.MAP_ELITES_MODELS[bcs_ablation_with_pref_count_key]\n",
    "bcs_ablation_with_pref_count_model = typing.cast(MAPElitesSampler, bcs_ablation_with_pref_count_model_spec.load())\n",
    "bcs_ablation_with_pref_count_trace_filter_data = bcs_ablation_with_pref_count_model_spec.load_trace_filter_data()\n",
    "\n",
    "\n",
    "bcs_ablation_no_pref_count_key = 'bcs_ablation_latest_at_end_no_game_object_expected_values'\n",
    "bcs_ablation_no_pref_count_model_spec = latest_model_paths.MAP_ELITES_MODELS[bcs_ablation_no_pref_count_key]\n",
    "bcs_ablation_no_pref_count_model = typing.cast(MAPElitesSampler, bcs_ablation_no_pref_count_model_spec.load())\n",
    "bcs_ablation_no_pref_count_trace_filter_data = bcs_ablation_no_pref_count_model_spec.load_trace_filter_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast_printer.ast_to_string(bcs_ablation_no_pref_count_model.population[(0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0)], '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"(on ?v1 ?v8)\n",
    "            (on ?v4 ?v7)\n",
    "            (on ?v6 ?v9)\n",
    "            (on ?v9 ?v5)\n",
    "            (on ?v6 ?v7)\n",
    "            (on ?v4 ?v5)\n",
    "            (on ?v5 ?v8)\n",
    "            (on ?v2 ?v6)\n",
    "            (on ?v6 ?v4)\n",
    "            (on ?v2 ?v9)\n",
    "            (on ?v4 ?v1)\n",
    "            (on ?v7 ?v1)\n",
    "            (on ?v1 ?v6)\n",
    "            (on ?v1 ?v9)\n",
    "            (on ?v2 ?v8)\n",
    "            (on ?v6 ?v8)\n",
    "            (on ?v5 ?v7)\n",
    "            (on ?v3 ?v6)\n",
    "            (on ?v3 ?v8)\n",
    "            (on ?v7 ?v9)\n",
    "            (on ?v3 ?v4)\n",
    "            (on ?v2 ?v7)\n",
    "            (on ?v8 ?v7)\"\"\"\n",
    "\n",
    "\n",
    "edge_list = [l.replace('(on', '').replace(')', '').strip() for l in s.split('\\n')]\n",
    "G_directed = nx.parse_edgelist(edge_list, nodetype=str, create_using=nx.DiGraph())\n",
    "cycle_found = len(list(nx.simple_cycles(G_directed))) > 0\n",
    "G_undirected = G_directed.to_undirected()\n",
    "disconnected = not nx.is_connected(G_undirected)\n",
    "\n",
    "cycle_found, disconnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reward_machine_trace_filter\n",
    "\n",
    "\n",
    "def load_trace_filter_compute_keys_with_traces(map_elites_key: str):\n",
    "   args_str = f\"\"\"   --tqdm \n",
    "      --max-traces-per-game 400 \n",
    "      --n-workers 11 \n",
    "      --chunksize 1 \n",
    "      --save-interval 1 \n",
    "      --dont-sort-keys-by-traces \n",
    "      --use-only-database-nonconfirmed-traces \n",
    "      --worker-tqdm \n",
    "      --map-elites-model-name {map_elites_key}\n",
    "      --relative-path ..\n",
    "   \"\"\"\n",
    "\n",
    "   args = reward_machine_trace_filter.parser.parse_args(args_str.split())\n",
    "   trace_filter = reward_machine_trace_filter.build_trace_evaluator(args)\n",
    "\n",
    "   key_iter, population_size = trace_filter._build_key_iter()\n",
    "\n",
    "   keys_with_traces = []\n",
    "   for key in key_iter:\n",
    "      if key in trace_filter.result_summary_by_key:\n",
    "         continue\n",
    "\n",
    "      no_traces_retval = trace_filter.handle_single_game_cache_no_traces(key)\n",
    "      if not no_traces_retval[0]:\n",
    "         keys_with_traces.append(key)\n",
    "\n",
    "   return trace_filter, keys_with_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DEFAULT_CONTEXT = {\n",
    "    VARIABLES_CONTEXT_KEY: {\n",
    "        f'?v{i}': VariableDefinition([f'?v{i}'], ['block'], None)\n",
    "        for i in range(20)\n",
    "    },\n",
    "    SECTION_CONTEXT_KEY: PREFERENCES\n",
    "}\n",
    "\n",
    "\n",
    "KEY_TO_MISSING_PREF_KEYS = {}\n",
    "\n",
    "\n",
    "def find_game_preferences(game_ast):\n",
    "    preferences = None\n",
    "\n",
    "    for i in range(3, len(game_ast)):\n",
    "        if game_ast[i][0] == ast_parser.PREFERENCES:\n",
    "            preferences = game_ast[i][1].preferences\n",
    "            break\n",
    "\n",
    "    return preferences\n",
    "\n",
    "\n",
    "def find_pref_by_name(preferences, pref_name):\n",
    "    key_preferences = [p for p in preferences if p.definition.pref_name == pref_name]\n",
    "    if len(key_preferences) != 1:\n",
    "        raise ValueError(f'Found {len(key_preferences)} preferences for key {key}')\n",
    "\n",
    "    return key_preferences[0]\n",
    "\n",
    "\n",
    "def print_game_and_remaining_keys(model, key, trace_filter):\n",
    "    all_traces, non_database_confirmed_traces, counts_by_trace_and_key, stop_count_by_key, total_count_by_key = trace_filter._process_key_traces_databse_results(key)\n",
    "    _, _, _, expected_keys, database_keys_to_traces = trace_filter._find_key_traces(key)\n",
    "    ignore_keys = list(database_keys_to_traces.keys())\n",
    "    remaining_keys = [k for k in expected_keys if k not in ignore_keys]\n",
    "\n",
    "    game_ast = model.population[key]\n",
    "\n",
    "    if '(:setup' in remaining_keys:\n",
    "        print(f'Found remaining setup section for {key}:')\n",
    "        print(ast_printer.ast_section_to_string(game_ast[3], ast_parser.SETUP, '\\n'))\n",
    "        print()\n",
    "        remaining_keys.remove('(:setup')\n",
    "\n",
    "    if len(remaining_keys) == 0:\n",
    "        return 0, False, False\n",
    "\n",
    "    preferences = find_game_preferences(game_ast)\n",
    "    max_at_end_and_length = 0\n",
    "\n",
    "    if preferences is None:\n",
    "        print('Failed to find preferences section in game:')\n",
    "        print(ast_printer.ast_to_string(game_ast, '\\n'))\n",
    "        raise ValueError('No preferences section found')\n",
    "    \n",
    "    KEY_TO_MISSING_PREF_KEYS[key] = remaining_keys\n",
    "    cycle_found = False\n",
    "    disconnected = False\n",
    "    \n",
    "    for pref_key in remaining_keys:    \n",
    "        pref = find_pref_by_name(preferences, pref_key)\n",
    "        pref_str = ast_printer.ast_section_to_string(pref, ast_parser.PREFERENCES, '\\n')\n",
    "        if 'block' in pref_str and '(at-end' in pref_str and '(and' in pref_str:\n",
    "            at_end_and = pref.definition.pref_body.body.exists_args.at_end_pred.pred\n",
    "            at_end_and_length = len(at_end_and.and_args)\n",
    "            max_at_end_and_length = max(at_end_and_length, max_at_end_and_length)\n",
    "\n",
    "            logical_expr = BOOLEAN_PARSER(at_end_and, **simplified_context_deepcopy(DEFAULT_CONTEXT))\n",
    "            logical_evaluation = BOOLEAN_PARSER.evaluate_unnecessary_detailed_return_value(logical_expr)  # type: ignore\n",
    "\n",
    "            if logical_evaluation != 0:\n",
    "                print(f'Found non-zero logical evaluation for {key}-{pref_key}')\n",
    "\n",
    "\n",
    "            # edges = [l.strip() for l in ast_printer.ast_section_to_string(pred, PREFERENCES, '\\n').replace('(and', '').replace('(on', '').replace(')', '').strip().split('\\n')]\n",
    "            # print(edges)\n",
    "\n",
    "            # edges = defaultdict(list)\n",
    "            edge_list = []\n",
    "\n",
    "            # print(ast_printer.ast_section_to_string(at_end_and, ast_parser.PREFERENCES, '\\n'))\n",
    "            for p in at_end_and.and_args:\n",
    "                if p.pred.parseinfo.rule != 'predicate':\n",
    "                    continue\n",
    "                \n",
    "                pred_type = p.pred.pred.parseinfo.rule.replace('predicate_', '')\n",
    "                if pred_type == 'on':\n",
    "                    # edges[p.pred.pred.arg_1.term].append(p.pred.pred.arg_2.term)\n",
    "                    arg_1 = p.pred.pred.arg_1.term\n",
    "                    if isinstance(arg_1, tatsu.ast.AST):\n",
    "                        arg_1 = arg_1.terminal\n",
    "\n",
    "                    arg_2 = p.pred.pred.arg_2.term\n",
    "                    if isinstance(arg_2, tatsu.ast.AST):\n",
    "                        arg_2 = arg_2.terminal\n",
    "\n",
    "                    e = f'{arg_1} {arg_2}'\n",
    "                    edge_list.append(e)\n",
    "            \n",
    "            if edge_list:\n",
    "                G_d = nx.parse_edgelist(edge_list, nodetype=str, create_using=nx.DiGraph())\n",
    "                cycle_found = len(list(nx.simple_cycles(G_d))) > 0\n",
    "                G_ud = G_d.to_undirected()\n",
    "                disconnected = not nx.is_connected(G_ud) \n",
    "                \n",
    "            # ts = graphlib.TopologicalSorter(edges)\n",
    "            # try:\n",
    "            #     ts.prepare()\n",
    "            # except graphlib.CycleError:\n",
    "            #     cycle_found = True\n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(f'Found remaining preference of new form for {key}:')\n",
    "            print(pref_str)\n",
    "            print()\n",
    "\n",
    "    return max_at_end_and_length, cycle_found, disconnected\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_elites_key = bcs_ablation_with_pref_count_key\n",
    "model = bcs_ablation_with_pref_count_model\n",
    "trace_filter, keys_with_traces = load_trace_filter_compute_keys_with_traces(map_elites_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_by_max_length = defaultdict(list)\n",
    "keys_by_max_length_with_cycles = defaultdict(list)\n",
    "keys_by_max_length_with_disconnection = defaultdict(list)\n",
    "\n",
    "\n",
    "\n",
    "for key in keys_with_traces:\n",
    "    # ablation_no_coherence_features_model\n",
    "    max_length, cycle_found, disconnected = print_game_and_remaining_keys(model, key, trace_filter)\n",
    "    keys_by_max_length[max_length].append(key)\n",
    "    if cycle_found:\n",
    "        keys_by_max_length_with_cycles[max_length].append(key)\n",
    "    if disconnected:\n",
    "        keys_by_max_length_with_disconnection[max_length].append(key)\n",
    "\n",
    "\n",
    "print({k: len(v) for k, v in keys_by_max_length.items()})\n",
    "print({k: len(v) for k, v in keys_by_max_length_with_cycles.items()})\n",
    "print({k: len(v) for k, v in keys_by_max_length_with_disconnection.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game_str = \"\"\"\n",
    "(define (game test) (:domain few-objects-room-v1) \n",
    "(:constraints (and\n",
    "    (preference preference0\n",
    "    (exists (?v1 - cube_block ?v2 - block ?v3 - pyramid_block_red)\n",
    "        (then\n",
    "        (once (game_start))\n",
    "        (hold (and (not (same_object ?v1 ?v3)) (adjacent ?v1 ?v2)))\n",
    "        (once (and (not (same_object ?v2 ?v1)) (same_type ?v2 ?v3) (touch ?v1 ?v3) ))\n",
    "    )\n",
    "    )\n",
    "    )\n",
    "))\n",
    "(:scoring\n",
    "    (count-once-per-objects preference0)\n",
    "))\n",
    "\"\"\"\n",
    "config = grammar_parser.config.replace_config(None)\n",
    "ctx = NoParseinfoTokenizerModelContext(grammar_parser.rules, config=config)\n",
    "test_game_ast = grammar_parser.parse(test_game_str, config=config, ctx=ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = (0, 1, 3, 0, 1, 1, 0, 1, 0, 0, 0)\n",
    "# prefs = find_game_preferences(model.population[key])\n",
    "# pref = find_pref_by_name(prefs, KEY_TO_MISSING_PREF_KEYS[key][0])\n",
    "\n",
    "prefs = find_game_preferences(test_game_ast)\n",
    "pref = find_pref_by_name(prefs, 'preference0')\n",
    "\n",
    "\n",
    "pred = pref.definition.pref_body.body.exists_args.at_end_pred\n",
    "pred = ast_utils.deepcopy_ast(pred, ast_utils.ASTCopyType.NODE)\n",
    "# pred.pred.and_args.pop(-1)\n",
    "print(ast_printer.ast_section_to_string(pref, PREFERENCES, '\\n'))\n",
    "\n",
    "# mapping = {f'?v{i}': ['block'] for i in range(10)}\n",
    "mapping = {f'?v{i}': ['game_object'] for i in range(10)}\n",
    "\n",
    "trace_filter.trace_finder.predicate_data_estimator.max_child_args = 8\n",
    "trace_filter.trace_finder.predicate_data_estimator.query_timeout = 120\n",
    "\n",
    "# result = trace_filter.trace_finder.predicate_data_estimator.filter(pred, mapping, return_trace_ids=True)\n",
    "# result.head()\n",
    "\n",
    "local_context = dict(mapping={})\n",
    "\n",
    "trace_filter.trace_finder._handle_ast(pref, section=PREFERENCES, local_context=local_context)\n",
    "\n",
    "print({k: list(v)[0] if len(v) == 1 else len(v) for k, v in trace_filter.trace_finder.databse_confirmed_traces_by_preference_or_section.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trace_filter.trace_finder.databse_confirmed_traces_by_preference_or_section.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_child(setup.setup.forall_vars.variables[0].var_type.type, 'terminal', 'block')\n",
    "replace_child(setup.setup.forall_args.setup.statement.conserved_pred.pred.pred.arg_2.term, 'terminal', 'pyramid_block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = (0, 0, 2, 0, 1, 1, 0, 1, 0, 0, 0)\n",
    "setup = model.population[key][3][1]\n",
    "print(ast_printer.ast_section_to_string(setup, ast_parser.SETUP, '\\n'))\n",
    "\n",
    "# mapping = {f'?v{i}': ['block'] for i in range(10)}\n",
    "mapping = {f'?v{i}': ['game_object'] for i in range(10)}\n",
    "\n",
    "trace_filter.trace_finder.predicate_data_estimator.max_child_args = 8\n",
    "trace_filter.trace_finder.predicate_data_estimator.query_timeout = 120\n",
    "\n",
    "# result = trace_filter.trace_finder.predicate_data_estimator.filter(pred, mapping, return_trace_ids=True)\n",
    "# result.head()\n",
    "\n",
    "local_context = dict(mapping={})\n",
    "trace_filter.trace_finder.setup_partial_results = []\n",
    "retval = trace_filter.trace_finder._handle_ast(setup, section=ast_parser.SETUP, local_context=local_context)\n",
    "print(retval)\n",
    "print({k: len(v) for k, v in trace_filter.trace_finder.databse_confirmed_traces_by_preference_or_section.items()})\n",
    "print(trace_filter.trace_finder.setup_partial_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphlib\n",
    "\n",
    "edges = {\n",
    "    '?v2': ['?v5', '?v1'],\n",
    "    '?v5': ['?v0', '?v3'],\n",
    "    '?v3': ['?v2'],\n",
    "    '?v0': ['?v1', '?v4'],\n",
    "}\n",
    "\n",
    "ts = graphlib.TopologicalSorter(edges)\n",
    "ts.prepare()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
