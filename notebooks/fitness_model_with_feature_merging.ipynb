{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import gzip\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tatsu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src import fitness_energy_utils as utils\n",
    "from src.fitness_energy_utils import NON_FEATURE_COLUMNS\n",
    "from src.ast_counter_sampler import *\n",
    "from src.ast_utils import cached_load_and_parse_games_from_file, load_games_from_file, _extract_game_id\n",
    "from src.room_and_object_types import CATEGORIES_TO_TYPES, EMPTY_OBJECT\n",
    "from src.fitness_features import COMMON_SENSE_PREDICATES_FUNCTIONS, PREDICATE_FUNCTION_ARITY_MAP\n",
    "from src import ast_printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='..')\n",
    "real_game_texts = [ast_printer.ast_to_string(ast, '\\n') for ast in game_asts]\n",
    "regrown_game_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples.pddl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df = utils.load_fitness_data()\n",
    "# temporary hack\n",
    "# if 'text_ngram_score' in fitness_df.columns and fitness_df.text_ngram_score.min() >= 0:\n",
    "#     fitness_df.text_ngram_score = np.log(fitness_df.text_ngram_score)\n",
    "print(fitness_df.src_file.unique())\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARIZE_IGNORE_FEATURES = [\n",
    "    'setup_objects_used', 'starts_and_ends_once', 'correct_predicate_function_arity',\n",
    "    'section_without_pref_or_total_count_terminal', 'section_without_pref_or_total_count_scoring'\n",
    "]  \n",
    "\n",
    "BINARIZE_IGNORE_PATTERNS = [\n",
    "    re.compile(r'max_depth_[\\w\\d_]+'), \n",
    "    re.compile(r'mean_depth_[\\w\\d_]+'), \n",
    "    re.compile(r'node_count_[\\w\\d_]+')\n",
    "]  \n",
    "\n",
    "BINARIZE_NON_ONE = [\n",
    "    'all_variables_defined', 'all_variables_used', \n",
    "    'all_preferences_used', 'no_adjacent_once', 'variable_not_repeated',\n",
    "    'no_nested_logicals', 'no_identical_logical_children',     \n",
    "    'count_once_per_external_objects_used_correctly',         \n",
    "    'external_forall_used_correctly', 'pref_forall_used',        \n",
    "    'pref_forall_correct_arity', 'pref_forall_correct_types', 'no_two_number_operations',\n",
    "    'tautological_expression_found', 'redundant_expression_found',\n",
    "]  \n",
    "\n",
    "SCALE_ZERO_ONE_PATTERNS = [\n",
    "    re.compile(r'(ast|text)_ngram_n_\\d+_score'),\n",
    "]\n",
    "\n",
    "BINRARIZE_NONZERO_PATTERNS = [\n",
    "    re.compile(r'arg_types_[\\w_]+'), \n",
    "    re.compile(r'compositionality_structure_\\d+'),\n",
    "    re.compile(r'(ast|text)_ngram_n_\\d+_\\d+')\n",
    "]   \n",
    "\n",
    "\n",
    "def _update_single_series(series: pd.Series, ignore_columns: typing.Iterable[str] = NON_FEATURE_COLUMNS):\n",
    "    c = str(series.name)\n",
    "    if c in ignore_columns:\n",
    "        return series\n",
    "\n",
    "    if c in BINARIZE_IGNORE_FEATURES:\n",
    "        return series\n",
    "    \n",
    "    if any([p.match(c) for p in BINARIZE_IGNORE_PATTERNS]):\n",
    "        return series\n",
    "\n",
    "    if c in BINARIZE_NON_ONE:\n",
    "        return (series == 1).astype(int)\n",
    "    \n",
    "    if any([p.match(c) for p in SCALE_ZERO_ONE_PATTERNS]):\n",
    "        min_val, max_val = series.min(), series.max()\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "\n",
    "    if any([p.match(c) for p in BINRARIZE_NONZERO_PATTERNS]):\n",
    "        return (series != 0).astype(int)\n",
    "    \n",
    "    raise ValueError(f'No binarization rule for column {c}')\n",
    "\n",
    "\n",
    "def binarize_features(df: pd.DataFrame, ignore_columns: typing.Iterable[str] = NON_FEATURE_COLUMNS) -> pd.DataFrame:\n",
    "    binarized_df = df.apply(_update_single_series, axis=0, ignore_columns=ignore_columns)\n",
    "    return binarized_df\n",
    "\n",
    "\n",
    "binarized_df = binarize_features(fitness_df)\n",
    "binarized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_single_prefix(df: pd.DataFrame, feature_prefix: str, threshold: int = 10, \n",
    "    merge_function: typing.Callable = np.logical_or, merged_column_suffix: str = 'other', feature_suffix: str = '') -> None:\n",
    "    \n",
    "    index_feature_names = [c for c in df.columns if c.startswith(f'{feature_prefix}_ramps') and c.endswith(feature_suffix)]\n",
    "    if len(index_feature_names) == 0:\n",
    "        print(f'No index feature found for prefix {feature_prefix}')\n",
    "        return\n",
    "    \n",
    "    index_feature_name = index_feature_names[0]\n",
    "    insert_index = list(df.columns).index(index_feature_name)\n",
    "\n",
    "    counts = df[[c for c in df.columns if c.startswith(feature_prefix) and c.endswith(feature_suffix)]].sum()\n",
    "    keys_to_merge = counts.index[counts < threshold]  # type: ignore\n",
    "    if len(keys_to_merge) == 0:\n",
    "        print(feature_prefix)\n",
    "        return\n",
    "    new_series_values = reduce(merge_function, [df[k] for k in keys_to_merge[1:]], df[keys_to_merge[0]]).astype(int)\n",
    "    \n",
    "    merged_column_key = f'{feature_prefix}_{merged_column_suffix}{\"_\" + feature_suffix if feature_suffix else \"\"}'\n",
    "    df.insert(insert_index, merged_column_key, new_series_values)\n",
    "    df.drop(keys_to_merge, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "def merge_sparse_features(df: pd.DataFrame, threshold: int = 10, \n",
    "    merge_function: typing.Callable = np.logical_or, merged_column_suffix: str = 'other',\n",
    "    predicates: typing.Sequence[str] = COMMON_SENSE_PREDICATES_FUNCTIONS) -> pd.DataFrame:\n",
    "    df = df.copy(deep=True)\n",
    "\n",
    "    for feature_suffix in ('setup', 'constraints'):\n",
    "        for p in predicates:\n",
    "            feature_prefix = f'arg_types_{p}'\n",
    "            _merge_single_prefix(df, feature_prefix, threshold, merge_function, merged_column_suffix, feature_suffix)\n",
    "\n",
    "            # if p not in PREDICATE_FUNCTION_ARITY_MAP:\n",
    "            #     raise ValueError(f'Predicate {p} not in arity map')\n",
    "\n",
    "            # arity = PREDICATE_FUNCTION_ARITY_MAP[p]\n",
    "            # if arity == 1:\n",
    "            #     feature_prefix = f'arg_types_{p}'\n",
    "            #     _merge_single_prefix(df, feature_prefix, threshold, merge_function, merged_column_suffix, feature_suffix)\n",
    "\n",
    "            # else:  # arity = 2/3\n",
    "            #     for c in CATEGORIES_TO_TYPES.keys():\n",
    "            #         if c == EMPTY_OBJECT:\n",
    "            #             continue\n",
    "            #         feature_prefix = f'arg_types_{p}_{c}'\n",
    "            #         _merge_single_prefix(df, feature_prefix, threshold, merge_function, merged_column_suffix, feature_suffix)\n",
    "\n",
    "    return df\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_binarized_df = merge_sparse_features(binarized_df, threshold=10, predicates=COMMON_SENSE_PREDICATES_FUNCTIONS)\n",
    "print(binarized_df.shape, '=>', merged_binarized_df.shape)\n",
    "merged_binarized_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: theoretically we'd want to first train-test split and then merge features, but for a quick POC I'm doing it in the opposite order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_param_grid = [\n",
    "    {\n",
    "        'fitness__loss_function': [utils.fitness_square_square_loss], # [utils.fitness_hinge_loss_with_cross_example],\n",
    "        'fitness__weight_decay': [0.0, 0.25, 1, 2],  \n",
    "        'fitness__margin': [1, 2, 4, 8],\n",
    "        # 'fitness__beta': [0.25, 1, 2, 4],\n",
    "        'fitness__lr':  [1e-1, 3e-2, 1e-2, 3e-3],  # [1e-1, 3e-2, 1e-2, 3e-3],\n",
    "        'fitness__k': [4, 8, 16, 32, 64],\n",
    "        'fitness__batch_size': [2, 4, 8, 16],  # [1, 4, 8, 16],\n",
    "        # 'fitness__alpha': [0, 0.25, 0.5, 0.75, 1], # [0, 0.1, 0.2, 0.3],  #\n",
    "    },\n",
    "    # {\n",
    "    # #     'fitness__loss_function': [utils.fitness_log_loss],\n",
    "    # #     'fitness__weight_decay': [0.0, 0.125, 0.25, 0.5, 1],  \n",
    "    # #     'fitness__lr': [1e-2, 3e-3, 1e-3, 3e-4],\n",
    "    # #     'fitness__k': [16, 32, 64, 128],\n",
    "    # #     'fitness__batch_size': [1, 4, 8, 16],\n",
    "    # # },\n",
    "    # # {\n",
    "    # #     'fitness__loss_function': [utils.fitness_square_square_loss],\n",
    "    # #     'fitness__weight_decay': [0.0, 0.125, 0.25, 0.5, 1],  \n",
    "    # #     'fitness__margin': [1, 2, 4],\n",
    "    # #     'fitness__lr': [1e-2, 3e-3, 1e-3, 3e-4],\n",
    "    # #     'fitness__k': [16, 32, 64, 128],\n",
    "    # #     'fitness__batch_size': [1, 4, 8, 16],\n",
    "    # },   \n",
    "]\n",
    "\n",
    "scaler_kwargs = dict(passthrough=True)\n",
    "model_kwargs = dict(output_activation=nn.Identity())\n",
    "train_kwargs = dict(negative_score_reduction='none')\n",
    "cv_kwargs = dict(refit='single_game_rank')\n",
    "scoring = utils.build_multiple_scoring_function(\n",
    "    [utils.evaluate_fitness_overall_ecdf, utils.evaluate_fitness_single_game_rank],\n",
    "    ['overall_ecdf', 'single_game_rank'],\n",
    ")\n",
    "\n",
    "cv_merged_binarized_features_sq_sq, (train_tensor_merged_binarized_features_sq_sq, test_tensor_merged_binarized_features_sq_sq), test_results_merged_binarized_features_sq_sq = utils.model_fitting_experiment(\n",
    "    merged_binarized_df, test_param_grid, random_seed=42,\n",
    "    scoring_function=scoring, verbose=1, scaler_kwargs=scaler_kwargs,\n",
    "    model_kwargs=model_kwargs, train_kwargs=train_kwargs, cv_kwargs=cv_kwargs)\n",
    "\n",
    "\n",
    "utils.visualize_cv_outputs(cv_merged_binarized_features_sq_sq, train_tensor_merged_binarized_features_sq_sq, test_tensor_merged_binarized_features_sq_sq, test_results_merged_binarized_features_sq_sq, histogram_title_note='binarized + merged features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.arange(1, 100) / (98 * 129)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv.best_estimator_.fit(full_tensor)\n",
    "\n",
    "SAVE_MODEL = True\n",
    "if SAVE_MODEL:\n",
    "    output_path = f'../models/cv_fitness_model_binarized_merging_{datetime.now().strftime(\"%Y_%m_%d\")}.pkl.gz'\n",
    "    original_output_path = output_path[:]\n",
    "    i = 0\n",
    "    while os.path.exists(output_path):\n",
    "        output_path = original_output_path + f'_{i}'\n",
    "        i += 1\n",
    "\n",
    "    with gzip.open(output_path, 'wb') as f:\n",
    "        pickle.dump(cv_merged_binarized_features_sq_sq.best_estimator_, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [c for c in merged_binarized_df.columns if c not in NON_FEATURE_COLUMNS]\n",
    "full_merged_binarized_tensor = utils.df_to_tensor(merged_binarized_df, feature_columns)\n",
    "full_tensor_scores = cv_merged_binarized_features_sq_sq.best_estimator_.transform(full_merged_binarized_tensor).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_diffs = (full_tensor_scores[:, 1:] - full_tensor_scores[:, 0].unsqueeze(1)).ravel().numpy()\n",
    "energy_diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrown_game_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_regrowth_depth(game_text: str):\n",
    "    game_id_start = game_text.find('(game')\n",
    "    game_id_section = game_text[game_id_start:game_text.find(')', game_id_start)]\n",
    "    rightmost_dash = game_id_section.rfind('-')\n",
    "    regrowth_depth = game_id_section[game_id_section.rfind('-') + 3:]\n",
    "    penultimate_dash = game_id_section.rfind('-', 0, rightmost_dash)\n",
    "    node_depth = game_id_section[penultimate_dash + 3:rightmost_dash]\n",
    "    return int(node_depth), int(regrowth_depth)\n",
    "\n",
    "depths = [extract_regrowth_depth(g) for g in regrown_game_texts]\n",
    "node_depths, regrowth_depths = zip(*depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_by_tuple = defaultdict(list)\n",
    "\n",
    "for i, depth_tuple in enumerate(depths):\n",
    "    depths_by_tuple[depth_tuple].append(energy_diffs[i])\n",
    "\n",
    "visit_counts = np.zeros((max(node_depths) + 1, max(regrowth_depths) + 1))\n",
    "mean_energies = np.zeros((max(node_depths) + 1, max(regrowth_depths) + 1))\n",
    "for n_d, r_d in depths_by_tuple.keys():\n",
    "    mean_energies[n_d, r_d] = np.mean(depths_by_tuple[(n_d, r_d)])\n",
    "    visit_counts[n_d, r_d] = len(depths_by_tuple[(n_d, r_d)])\n",
    "\n",
    "mean_energies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mean_energies, cmap='hot', origin='lower')\n",
    "plt.xlim(min(regrowth_depths) - 0.5, max(regrowth_depths) + 0.5)\n",
    "plt.ylim(min(node_depths) - 0.5, max(node_depths) + 0.5)\n",
    "\n",
    "plt.xlabel('Depth of regrown sub-tree')\n",
    "plt.ylabel('Depth in original tree of mutation node')\n",
    "\n",
    "plt.colorbar(label='Mean energy difference (regrown - original)')\n",
    "plt.title('Effect of node and regrowth depth on energy difference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(visit_counts, cmap='bone', origin='lower')\n",
    "plt.xlim(min(regrowth_depths) - 0.5, max(regrowth_depths) + 0.5)\n",
    "plt.ylim(min(node_depths) - 0.5, max(node_depths) + 0.5)\n",
    "\n",
    "plt.xlabel('Depth of regrown sub-tree')\n",
    "plt.ylabel('Depth in original tree of mutation node')\n",
    "\n",
    "plt.colorbar(label='# of regrown games at this cell')\n",
    "plt.title('Number of regrown games at each node and regrowth depth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_energies[113 // mean_energies.shape[1], 113 % mean_energies.shape[1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8b8c8d5c21b7aee7bd0053f69e9c255c013bbea7031fbef7e66d58dd46c0fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
