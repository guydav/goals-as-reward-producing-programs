{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from argparse import Namespace\n",
    "from ast import literal_eval\n",
    "import copy\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import textwrap\n",
    "import typing\n",
    "\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "import duckdb\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.axes\n",
    "import matplotlib.pyplot as plt\n",
    "from Levenshtein import distance as _edit_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import tatsu\n",
    "import tatsu.ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "from scipy.special import comb\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src.ast_utils import _extract_game_id, deepcopy_ast, replace_child\n",
    "from src.ast_printer import ast_to_lines\n",
    "from src import fitness_energy_utils as utils\n",
    "from src.fitness_energy_utils import NON_FEATURE_COLUMNS\n",
    "from src.fitness_features import *\n",
    "from src.ast_counter_sampler import *\n",
    "from src.evolutionary_sampler import *\n",
    "from src import fitness_features_by_category, latest_model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "real_game_texts = [ast_printer.ast_to_string(ast, '\\n') for ast in game_asts]\n",
    "\n",
    "\n",
    "# regrown_game_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples.pddl'))\n",
    "# regrown_game_1024_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples-1024.pddl'))\n",
    "# print(len(real_game_texts), len(regrown_game_texts), len(regrown_game_texts) / 98, len(regrown_game_1024_texts), len(regrown_game_1024_texts) / 98)\n",
    "\n",
    "fitness_df = utils.load_fitness_data('../data/fitness_features_1024_regrowths.csv.gz')\n",
    "print(fitness_df.src_file.unique())\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace_filter_results_path = '../samples/trace_filter_results_max_exemplar_preferences_by_bcs_with_expected_values_2023_11_29_2023_12_05_1.pkl.gz'\n",
    "model_key = 'max_exemplar_preferences_by_bcs_with_expected_values'\n",
    "model_spec = latest_model_paths.MAP_ELITES_MODELS[model_key]\n",
    "model = typing.cast(MAPElitesSampler, model_spec.load())\n",
    "\n",
    "key_to_real_game_index = defaultdict(list)\n",
    "real_game_index_to_key = {}\n",
    "real_game_fitness_scores = []\n",
    "ALL_REAL_GAME_KEYS = []\n",
    "for i, ast in enumerate(game_asts):\n",
    "    fitness_score, features = model._score_proposal(ast, return_features=True)  # type: ignore\n",
    "    real_game_fitness_scores.append(fitness_score)\n",
    "    key = model._features_to_key(ast, features)\n",
    "    key_to_real_game_index[key].append(i)\n",
    "    real_game_index_to_key[i] = key\n",
    "    ALL_REAL_GAME_KEYS.append(key)\n",
    "\n",
    "trace_filter_results = model_spec.load_trace_filter_data()\n",
    "trace_filter_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_GAME_INDICES_TO_INCLUDE = [\n",
    "    0, 4, 6, 7, 11,\n",
    "    14, 17, 23, 26, 28,\n",
    "    31, 32, 35, 37, 40,\n",
    "    41, 42, 45, 49, 51,\n",
    "    52, 55, 58, 59, 64,\n",
    "    74, 88, 90, 94, 96,\n",
    "]\n",
    "\n",
    "REAL_GAME_KEY_LIST = [real_game_index_to_key[i] for i in REAL_GAME_INDICES_TO_INCLUDE]\n",
    "REAL_GAME_KEY_DICT = {key: i for i, key in enumerate(REAL_GAME_KEY_LIST)}\n",
    "REAL_GAME_KEYS = set(REAL_GAME_KEY_LIST)\n",
    "print(len(REAL_GAME_KEYS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNMATCHED_TOP_30_KEYS = [\n",
    "    (1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0),\n",
    "    (1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1),\n",
    "    (1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "    (1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0),\n",
    "    (1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0),\n",
    "    (1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0),\n",
    "    (1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "    (1, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0),\n",
    "    (1, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0),\n",
    "    (1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1),\n",
    "    (1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0),\n",
    "    (1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0),\n",
    "    (1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0),\n",
    "    (1, 1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0),\n",
    "    (1, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0),\n",
    "    (1, 1, 3, 1, 0, 0, 1, 0, 0, 0, 1, 0),\n",
    "    (1, 1, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0),\n",
    "    (1, 1, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0),\n",
    "    (1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0),\n",
    "    (1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "    (1, 1, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0),\n",
    "    (1, 1, 3, 0, 0, 1, 0, 0, 1, 0, 0, 0),\n",
    "    (1, 0, 4, 0, 1, 1, 0, 1, 0, 1, 0, 0),\n",
    "    (1, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 0),\n",
    "    (1, 1, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "    (1, 1, 4, 0, 0, 1, 1, 1, 0, 1, 0, 0),\n",
    "    (1, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0, 1),\n",
    "    (1, 1, 4, 0, 2, 0, 0, 0, 1, 0, 0, 0),\n",
    "    (1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "    (1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATIONS_DIR = '../llm_tests/translations'\n",
    "TRANSLATION_DATE = '2024_01_12'\n",
    "UNMATCHED_ONLY_TOP_30 = True\n",
    "\n",
    "with open(f'{TRANSLATIONS_DIR}/human_games_translations_split_{TRANSLATION_DATE}.json') as f:\n",
    "    human_game_texts = json.load(f)\n",
    "    human_game_texts = {literal_eval(k): v for k, v in human_game_texts.items()}\n",
    "\n",
    "with open(f'{TRANSLATIONS_DIR}/human_cell_archive_games_translations_split_{TRANSLATION_DATE}.json') as f:\n",
    "    human_cell_archive_game_texts = json.load(f)\n",
    "    human_cell_archive_game_texts = {literal_eval(k): v for k, v in human_cell_archive_game_texts.items()}\n",
    "\n",
    "with open(f'{TRANSLATIONS_DIR}/novel_archive_cell_games_translations_split_{TRANSLATION_DATE}.json') as f:\n",
    "    novel_archive_cell_game_texts = json.load(f)\n",
    "    novel_archive_cell_game_texts = {literal_eval(k): v for k, v in novel_archive_cell_game_texts.items()}\n",
    "    if UNMATCHED_ONLY_TOP_30:\n",
    "        novel_archive_cell_game_texts = {k: v for k, v in novel_archive_cell_game_texts.items() if k in UNMATCHED_TOP_30_KEYS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map each archive cell key to the relevant AST\n",
    "\n",
    "@gdrtodd -- we could also pull in more human games, etc., but these are the ones from the human eval dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_game_key_to_ast = {key: game_asts[i] for key, i in REAL_GAME_KEY_DICT.items()}\n",
    "matched_game_key_to_ast = {key: model.population[key] for key in human_cell_archive_game_texts.keys()}\n",
    "unmatched_game_key_to_ast = {key: model.population[key] for key in novel_archive_cell_game_texts.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract fitness features for a game\n",
    "\n",
    "This returns a dict where each key is a fitness feature name and each value is the value of that feature\n",
    "\n",
    "If for some reason you want all features, rather than the ones that the model used, set `only_used=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fitness_features(ast: tatsu.ast.AST, only_used: bool = True):\n",
    "    features = model._proposal_to_features(ast)\n",
    "    if only_used:\n",
    "        features = {k: v for k, v in features.items() if k in model.feature_names}\n",
    "    return features\n",
    "\n",
    "extract_fitness_features(real_game_key_to_ast[REAL_GAME_KEY_LIST[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know why you'd ever want a game's features as a Tensor, but just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fitness_tensor(ast: tatsu.ast.AST):\n",
    "    features = extract_fitness_features(ast, False)\n",
    "    return model._features_to_tensor(features)\n",
    "\n",
    "\n",
    "def fitness_score(ast: tatsu.ast.AST):\n",
    "    return model._score_proposal(ast, return_features=False)\n",
    "\n",
    "fitness_score(real_game_key_to_ast[REAL_GAME_KEY_LIST[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below explicitly maps a game to its BCs, mostly useful in case you want to know which BC index is which feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_to_behavioral_feature_dict(ast: tatsu.ast.AST):\n",
    "    features = extract_fitness_features(ast, False)\n",
    "    return model.custom_featurizer.get_game_features(ast, features)\n",
    "\n",
    "\n",
    "game_to_behavioral_feature_dict(real_game_key_to_ast[REAL_GAME_KEY_LIST[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons Between Real and Matched Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Cosine Similarity in Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_game_fitness_tensors = torch.stack([extract_fitness_tensor(real_game_key_to_ast[key]) for key in REAL_GAME_KEY_LIST])\n",
    "matched_game_fitness_tensors = torch.stack([extract_fitness_tensor(matched_game_key_to_ast[key]) for key in human_cell_archive_game_texts.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average cosine similarity between real games\n",
    "real_game_similarities = []\n",
    "for i, j in itertools.combinations(range(len(REAL_GAME_KEY_LIST)), 2):\n",
    "    real_game_similarities.append(\n",
    "        F.cosine_similarity(real_game_fitness_tensors[i], real_game_fitness_tensors[j], dim=0).item()\n",
    "    )\n",
    "\n",
    "# Compute average cosine similarity between matched games\n",
    "matched_game_similarities = []\n",
    "for i, j in itertools.combinations(range(len(human_cell_archive_game_texts)), 2):\n",
    "    matched_game_similarities.append(\n",
    "        F.cosine_similarity(matched_game_fitness_tensors[i], matched_game_fitness_tensors[j], dim=0).item()\n",
    "    )\n",
    "\n",
    "# Compute average cosine similarity between real and matched games\n",
    "real_matched_game_similarities = []\n",
    "for i, j in itertools.product(range(len(REAL_GAME_KEY_LIST)), range(len(human_cell_archive_game_texts))):\n",
    "    real_matched_game_similarities.append(\n",
    "        F.cosine_similarity(real_game_fitness_tensors[i], matched_game_fitness_tensors[j], dim=0).item()\n",
    "    )\n",
    "\n",
    "real_game_similarities = np.array(real_game_similarities)\n",
    "matched_game_similarities = np.array(matched_game_similarities)\n",
    "real_matched_game_similarities = np.array(real_matched_game_similarities)\n",
    "\n",
    "# Print the results\n",
    "print(f'Average cosine similarity between real games: {real_game_similarities.mean()} +/- {real_game_similarities.std()}')\n",
    "print(f'Average cosine similarity between matched games: {matched_game_similarities.mean()} +/- {matched_game_similarities.std()}')\n",
    "print(f'Average cosine similarity between real and matched games: {real_matched_game_similarities.mean()} +/- {real_matched_game_similarities.std()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Differing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = []\n",
    "for key in human_cell_archive_game_texts.keys():\n",
    "    real_game = real_game_key_to_ast[key]\n",
    "    matched_game = matched_game_key_to_ast[key]\n",
    "\n",
    "    real_game_features = extract_fitness_features(real_game)\n",
    "    matched_game_features = extract_fitness_features(matched_game)\n",
    "\n",
    "    n_diffs = 0\n",
    "    for k in model.feature_names:\n",
    "        if real_game_features[k] != matched_game_features[k] and 'ngram' not in k:\n",
    "            n_diffs += 1\n",
    "\n",
    "    differences.append(n_diffs)\n",
    "\n",
    "differences = np.array(differences)\n",
    "print(f'Average number of differing positions excluding ngram features: {differences.mean()} +/- {differences.std()} (of {len(model.feature_names)} total)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [+] Which Features Differ the Most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Discriminator Between Real and Corresponding Matched Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [+] Edit Distance (both on anonymized ASTs and on Stage 1 Translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [+] Reward Machine: Are the Same Preferences Activated from the Same Traces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitness_energy_utils import load_data\n",
    "\n",
    "human_games_trace_filter_data = load_data('', 'samples', f'/trace_filter_results_interactive-beta.pddl_2024_03_19', relative_path='..')\n",
    "human_games_trace_filter_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activating_traces(filter_info, key, exclude_setup=False):\n",
    "    sub_ast_to_trace_activations = filter_info['full'][key]\n",
    "    \n",
    "    sub_ast_to_activating_traces = {}\n",
    "    for sub_ast, trace_activations in sub_ast_to_trace_activations.items():\n",
    "        activating_traces = [trace for trace, activation in trace_activations.items() if activation > 0]\n",
    "        sub_ast_to_activating_traces[sub_ast] = set(activating_traces)\n",
    "\n",
    "    if exclude_setup:\n",
    "        sub_ast_to_activating_traces = {sub_ast: traces for sub_ast, traces in sub_ast_to_activating_traces.items() if 'setup' not in sub_ast}\n",
    "\n",
    "    sub_ast_to_activating_traces['all'] = set.intersection(*[sub_ast_to_activating_traces[sub_ast] for sub_ast in sub_ast_to_activating_traces.keys()])\n",
    "    sub_ast_to_activating_traces['any'] = set.union(*[sub_ast_to_activating_traces[sub_ast] for sub_ast in sub_ast_to_activating_traces.keys()])\n",
    "\n",
    "\n",
    "    return sub_ast_to_activating_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap the human_games_trace_filter_data according to the key instead of the index\n",
    "remapped_human_games_trace_filter_data = {\"full\": {}}\n",
    "\n",
    "for real_game_idx in human_games_trace_filter_data['full'].keys():\n",
    "    if real_game_idx not in REAL_GAME_INDICES_TO_INCLUDE:\n",
    "        continue\n",
    "    \n",
    "    real_game_key = real_game_index_to_key[real_game_idx]\n",
    "    remapped_human_games_trace_filter_data['full'][real_game_key] = human_games_trace_filter_data['full'][real_game_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remapped_human_games_trace_filter_data['full'][(1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0)]['(:setup'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_comparison_results = {}\n",
    "for key in human_cell_archive_game_texts.keys():\n",
    "    activating_traces = get_activating_traces(trace_filter_results, key)\n",
    "\n",
    "    if key not in remapped_human_games_trace_filter_data['full']:\n",
    "        print(f'Key {key} not in remapped_human_games_trace')\n",
    "        continue\n",
    "\n",
    "    corresponding_human_activating_traces = get_activating_traces(remapped_human_games_trace_filter_data, key)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for ast_type in ['any', 'all']:\n",
    "        results[ast_type] = {\n",
    "            'intersection': activating_traces[ast_type].intersection(corresponding_human_activating_traces[ast_type]),\n",
    "            'union': activating_traces[ast_type].union(corresponding_human_activating_traces[ast_type]),\n",
    "            'generated_minus_human': activating_traces[ast_type] - corresponding_human_activating_traces[ast_type],\n",
    "            'human_minus_generated': corresponding_human_activating_traces[ast_type] - activating_traces[ast_type],\n",
    "        }\n",
    "\n",
    "    trace_comparison_results[key] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the proportion of traces that activate at least one AST that are unique to generated / human games\n",
    "prop_unique_generated_any = []\n",
    "prop_unique_human_any = []\n",
    "prop_unique_either_any = []\n",
    "prop_not_unique_any = []\n",
    "jaccard_similarities = []\n",
    "\n",
    "for key in trace_comparison_results.keys():\n",
    "    results = trace_comparison_results[key]['any']\n",
    "\n",
    "    all_trace_count = len(results['union'])\n",
    "    prop_unique_generated_any.append(len(results['generated_minus_human']) / all_trace_count)\n",
    "    prop_unique_human_any.append(len(results['human_minus_generated']) / all_trace_count)\n",
    "    prop_unique_either_any.append(len(results['generated_minus_human'].union(results['human_minus_generated'])) / all_trace_count)\n",
    "    prop_not_unique_any.append(len(results['intersection']) / all_trace_count)\n",
    "\n",
    "    jaccard_similarities.append(len(results['intersection']) / len(results['union']))\n",
    "\n",
    "# Plot histograms of the proportion of unique traces in two separate plots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 4))\n",
    "ax[0].hist(prop_unique_generated_any, bins=10)\n",
    "ax[0].set_xlabel('Proportion of activated traces unique to generated game')\n",
    "ax[0].set_ylabel('Number of cells')\n",
    "\n",
    "ax[1].hist(prop_unique_human_any, bins=10, color='g')\n",
    "ax[1].set_xlabel('Proportion activating traces unique to human game')\n",
    "ax[1].set_ylabel('Number of cells')\n",
    "\n",
    "ax[2].hist(prop_unique_either_any, bins=10, color='r')\n",
    "ax[2].set_xlabel('Proportion activating traces unique to *either* game')\n",
    "ax[2].set_ylabel('Number of cells')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Then do the same thing as a bar plot with the proportions in descending order\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 4))\n",
    "prop_unique_generated_any = np.array(prop_unique_generated_any)\n",
    "prop_unique_human_any = np.array(prop_unique_human_any)\n",
    "prop_unique_either_any = np.array(prop_unique_either_any)\n",
    "prop_not_unique_any = np.array(prop_not_unique_any)\n",
    "\n",
    "sorted_indices = np.argsort(prop_unique_generated_any)\n",
    "ax[0].bar(np.arange(len(prop_unique_generated_any)), prop_unique_generated_any[sorted_indices], color='b')\n",
    "\n",
    "sorted_indices = np.argsort(prop_unique_human_any)\n",
    "ax[1].bar(np.arange(len(prop_unique_human_any)), prop_unique_human_any[sorted_indices], color='g')\n",
    "\n",
    "sorted_indices = np.argsort(prop_unique_either_any)\n",
    "ax[2].bar(np.arange(len(prop_unique_either_any)), prop_unique_either_any[sorted_indices], color='r')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked bar chart showing the proportion of traces unique to generated / human / neither\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "indices_sorted_by_not_unique = np.argsort(prop_not_unique_any)\n",
    "\n",
    "ax.bar(np.arange(len(prop_unique_generated_any)), \n",
    "       prop_unique_generated_any[indices_sorted_by_not_unique],\n",
    "       color='dodgerblue',\n",
    "       label='Unique to generated game')\n",
    "\n",
    "ax.bar(np.arange(len(prop_unique_human_any)),\n",
    "       prop_unique_human_any[indices_sorted_by_not_unique],\n",
    "       bottom=prop_unique_generated_any[indices_sorted_by_not_unique],\n",
    "       color='lightgreen',\n",
    "       label='Unique to human game')\n",
    "\n",
    "ax.bar(np.arange(len(prop_not_unique_any)),\n",
    "       prop_not_unique_any[indices_sorted_by_not_unique],\n",
    "       bottom=prop_unique_generated_any[indices_sorted_by_not_unique] + prop_unique_human_any[indices_sorted_by_not_unique],\n",
    "       color='slateblue',\n",
    "       label='Shared')\n",
    "\n",
    "# remove x ticks\n",
    "ax.set_xticks([])\n",
    "\n",
    "ax.set_xlabel('Archive cells')\n",
    "ax.set_ylabel('Proportion of activating traces')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of Jaccard similarities\n",
    "plt.hist(jaccard_similarities, bins=10)\n",
    "\n",
    "# Vertical line for the average Jaccard similarity\n",
    "plt.axvline(np.mean(jaccard_similarities), color='r', linestyle='--')\n",
    "\n",
    "plt.xlabel('Jaccard similarity')\n",
    "plt.ylabel('Number of cells')\n",
    "plt.title('Jaccard Similarity of Activating Traces Between Human and Generated Games')\n",
    "\n",
    "print(f\"Average and median Jaccard similarity: {np.mean(jaccard_similarities)}, {np.median(jaccard_similarities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the proportion of traces that activate at least one AST that are unique to generated / human games\n",
    "prop_unique_generated_all = []\n",
    "prop_unique_human_all = []\n",
    "prop_unique_either_all = []\n",
    "\n",
    "for key in trace_comparison_results.keys():\n",
    "    results = trace_comparison_results[key]['all']\n",
    "\n",
    "    all_trace_count = len(results['union'])\n",
    "    if all_trace_count == 0:\n",
    "        print(f'Games at {key} have no traces that activate setup and all preferences in either game')\n",
    "        continue\n",
    "\n",
    "    prop_unique_generated_all.append(len(results['generated_minus_human']) / all_trace_count)\n",
    "    prop_unique_human_all.append(len(results['human_minus_generated']) / all_trace_count)\n",
    "    prop_unique_either_all.append(len(results['generated_minus_human'].union(results['human_minus_generated'])) / all_trace_count)\n",
    "\n",
    "# Plot histograms of the proportion of unique traces in two separate plots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 4))\n",
    "ax[0].hist(prop_unique_generated_all, bins=10)\n",
    "ax[0].set_xlabel('Proportion of activated traces unique to generated game')\n",
    "ax[0].set_ylabel('Number of cells')\n",
    "\n",
    "ax[1].hist(prop_unique_human_all, bins=10, color='g')\n",
    "ax[1].set_xlabel('Proportion activating traces unique to human game')\n",
    "ax[1].set_ylabel('Number of cells')\n",
    "\n",
    "ax[2].hist(prop_unique_either_all, bins=10, color='r')\n",
    "ax[2].set_xlabel('Proportion activating traces unique to *either* game')\n",
    "ax[2].set_ylabel('Number of cells')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [+] Most Similar Generated Game to Specific Real Games Using Reward Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generated_game_keys = list(matched_game_key_to_ast.keys()) + list(unmatched_game_key_to_ast.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_human_game_keys = real_game_index_to_key.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The keys of the real games for which we want to know the most similar generated game\n",
    "TARGET_KEYS = [\n",
    "    (1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0), # matched 14\n",
    "    (1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0), # matched 31\n",
    "    (1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0), # matched 40\n",
    "\n",
    "    (1, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0), # unmatched (place the bin near the north wall...)\n",
    "    (1, 1, 3, 1, 0, 0, 1, 0, 0, 0, 1, 0), # unmatched (credit cards and CDs)\n",
    "    (1, 1, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0), # unmatched (block stacking)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in get_activating_traces(trace_filter_results, (1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0)).items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_SETUP = True\n",
    "AGGREGATION = \"all\"\n",
    "\n",
    "paired_keys = []\n",
    "print(f\"CLOSEST MATCH RESULTS (aggregation = {AGGREGATION}, exclude setup = {EXCLUDE_SETUP})\")\n",
    "for target in TARGET_KEYS:\n",
    "    target_activating_traces = get_activating_traces(trace_filter_results, target, EXCLUDE_SETUP)[AGGREGATION]\n",
    "\n",
    "    best_key, best_jaccard_similarity = None, 0\n",
    "    best_union, best_intersection = None, None\n",
    "    for key in all_human_game_keys:\n",
    "        if key not in remapped_human_games_trace_filter_data['full']:\n",
    "            continue\n",
    "\n",
    "        activating_traces = get_activating_traces(remapped_human_games_trace_filter_data, key, EXCLUDE_SETUP)[AGGREGATION]\n",
    "\n",
    "        intersection = target_activating_traces.intersection(activating_traces)\n",
    "        union = target_activating_traces.union(activating_traces)\n",
    "        if len(union) > 0:\n",
    "            jaccard_similarity = len(intersection) / len(union)\n",
    "        else:\n",
    "            jaccard_similarity = 0\n",
    "\n",
    "        if jaccard_similarity > best_jaccard_similarity:\n",
    "            best_key, best_jaccard_similarity = key, jaccard_similarity\n",
    "            best_union, best_intersection = len(union), len(intersection)\n",
    "    \n",
    "    print(f\"Gen. key {target} --> real key {best_key} (sim = {best_jaccard_similarity:.2f}, {best_intersection} / {best_union})\")\n",
    "    paired_keys.append((target, best_key))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for generated_key, real_game_key in paired_keys:\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(f\"[GEN] Preference breakdown for {generated_key}\")\n",
    "    for key, value in get_activating_traces(trace_filter_results, generated_key).items():\n",
    "        print(key, len(value))\n",
    "\n",
    "    print(f\"[REAL] Preference breakdown for {real_game_key}\")\n",
    "    for key, value in get_activating_traces(remapped_human_games_trace_filter_data, real_game_key).items():\n",
    "        print(key, len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [+] Archive Diversity Computation Using Reward Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_no_custom_ops_key = 'ablation_max_exemplar_preferences_by_bcs_with_expected_values_no_custom_ops'\n",
    "ablation_no_custom_ops_model_spec = latest_model_paths.MAP_ELITES_MODELS[ablation_no_custom_ops_key]\n",
    "ablation_no_custom_ops_model = typing.cast(MAPElitesSampler, ablation_no_custom_ops_model_spec.load())\n",
    "\n",
    "ablation_no_custom_ops_no_crossover_key = 'ablation_max_exemplar_preferences_by_bcs_with_expected_values_no_custom_ops_no_crossover'\n",
    "ablation_no_custom_ops_no_crossover_model_spec = latest_model_paths.MAP_ELITES_MODELS[ablation_no_custom_ops_no_crossover_key]\n",
    "ablation_no_custom_ops_no_crossover_model = typing.cast(MAPElitesSampler, ablation_no_custom_ops_no_crossover_model_spec.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_no_custom_ops_trace_filter_data = ablation_no_custom_ops_model_spec.load_trace_filter_data()\n",
    "ablation_no_custom_ops_no_crossover_model_trace_filter_data = ablation_no_custom_ops_no_crossover_model_spec.load_trace_filter_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossover_ablation_trace_filter_mapping = {\n",
    "    \"Full Model\": trace_filter_results,\n",
    "    \"No Custom Ops\": ablation_no_custom_ops_trace_filter_data,\n",
    "    \"No Custom Ops No Crossover\": ablation_no_custom_ops_no_crossover_model_trace_filter_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossover_ablation_pairwise_similarities_mapping = {}\n",
    "\n",
    "for ablation_key in crossover_ablation_trace_filter_mapping.keys():\n",
    "    ablation_trace_filter_data = crossover_ablation_trace_filter_mapping[ablation_key]\n",
    "    ablation_activating_traces = {}\n",
    "\n",
    "    valid_keys = [key for key in ablation_trace_filter_data['full'].keys() if key[0] == 1] # filter out keys with the invalid BC active\n",
    "\n",
    "\n",
    "    for key in valid_keys:\n",
    "        activating_traces = get_activating_traces(ablation_trace_filter_data, key)\n",
    "        ablation_activating_traces[key] = activating_traces\n",
    "\n",
    "    pairwise_jaccard_similarities = []\n",
    "    total = comb(len(valid_keys), 2)\n",
    "\n",
    "    for key1, key2 in tqdm(itertools.combinations(valid_keys, 2), desc=\"Computing pairwise similarities\", total=total):\n",
    "        traces1 = ablation_activating_traces[key1]['any']\n",
    "        traces2 = ablation_activating_traces[key2]['any']\n",
    "\n",
    "        intersection = traces1.intersection(traces2)\n",
    "        union = traces1.union(traces2)\n",
    "\n",
    "        # TODO: how to handle the case where the union is empty?\n",
    "        if len(union) < 1:\n",
    "            continue\n",
    "\n",
    "        pairwise_jaccard_similarities.append(len(intersection) / len(union))\n",
    "\n",
    "    crossover_ablation_pairwise_similarities_mapping[ablation_key] = pairwise_jaccard_similarities\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in crossover_ablation_pairwise_similarities_mapping.keys():\n",
    "    similarities = crossover_ablation_pairwise_similarities_mapping[key]\n",
    "    print(f\"{key}: {len(set(similarities))} unique, {len(similarities)} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_labels = ['Full Model', 'No Custom Ops', 'No Custom Ops\\nNo Crossover']\n",
    "\n",
    "# Plot each of the pairwise similarities as a violin plot, labeled with the key\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.violinplot(data=list(crossover_ablation_pairwise_similarities_mapping.values()), ax=ax, inner='quart', alpha=0.75)\n",
    "sns.pointplot(data=list(crossover_ablation_pairwise_similarities_mapping.values()), errorbar=('ci', 95), linestyle='none', color='black', markers='d', markersize=15, ax=ax)\n",
    "ax.set_xticklabels(tick_labels)\n",
    "ax.set_ylabel('Jaccard similarity')\n",
    "ax.set_xlabel('Ablation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Activating Traces for Games Under Each Ablation\")\n",
    "num_traces_per_cell = {}\n",
    "for ablation_key in crossover_ablation_trace_filter_mapping.keys():\n",
    "    ablation_trace_filter_data = crossover_ablation_trace_filter_mapping[ablation_key]\n",
    "    ablation_activating_traces = {}\n",
    "\n",
    "    valid_keys = [key for key in ablation_trace_filter_data['full'].keys() if key[0] == 1] # filter out keys with the invalid BC active\n",
    "\n",
    "    for key in valid_keys:\n",
    "        activating_traces = get_activating_traces(ablation_trace_filter_data, key)\n",
    "        ablation_activating_traces[key] = activating_traces\n",
    "\n",
    "    num_traces = [len(ablation_activating_traces[key]['any']) for key in valid_keys]\n",
    "\n",
    "    average_num_traces = np.mean(num_traces)\n",
    "    std_num_traces = np.std(num_traces)\n",
    "    median_num_traces = np.median(num_traces)\n",
    "\n",
    "    num_traces_per_cell[ablation_key] = num_traces\n",
    "\n",
    "    print(f\"{ablation_key}: {average_num_traces} +/- {std_num_traces}, median {median_num_traces}\")\n",
    "\n",
    "# T-Tests between each pair of ablations\n",
    "# from scipy.stats import ttest_rel\n",
    "# for key1, key2 in itertools.combinations(crossover_ablation_pairwise_similarities_mapping.keys(), 2):\n",
    "#     nums1 = num_traces_per_cell[key1]\n",
    "#     nums2 = num_traces_per_cell[key2]\n",
    "\n",
    "#     t_stat, p_val = ttest_rel(nums1, nums2)\n",
    "#     print(f\"T-Test between {key1} and {key2}: t={t_stat}, p={p_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_labels = ['Full Model', 'No Custom Ops', 'No Custom Ops\\nNo Crossover']\n",
    "\n",
    "# Plot each of the pairwise similarities as a violin plot, labeled with the key\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.violinplot(data=list(num_traces_per_cell.values()), ax=ax, inner='quart', alpha=0.75)\n",
    "sns.pointplot(data=list(num_traces_per_cell.values()), errorbar=('ci', 95), linestyle='none', color='black', markers='d', markersize=15, ax=ax)\n",
    "ax.set_xticklabels(tick_labels)\n",
    "ax.set_ylabel('Number of activating traces')\n",
    "ax.set_xlabel('Ablation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of *Unique* Activating Traces for Games Under Each Ablation\")\n",
    "for ablation_key in crossover_ablation_trace_filter_mapping.keys():\n",
    "    ablation_trace_filter_data = crossover_ablation_trace_filter_mapping[ablation_key]\n",
    "    ablation_activating_traces = {}\n",
    "\n",
    "    valid_keys = [key for key in ablation_trace_filter_data['full'].keys() if key[0] == 1] # filter out keys with the invalid BC active\n",
    "\n",
    "\n",
    "    for key in valid_keys:\n",
    "        activating_traces = get_activating_traces(ablation_trace_filter_data, key)\n",
    "        ablation_activating_traces[key] = activating_traces\n",
    "\n",
    "    num_unique = []\n",
    "    for key in tqdm(valid_keys, desc=\"Computing unique traces\"):\n",
    "        traces = ablation_activating_traces[key]['all']\n",
    "        all_other_traces = set.union(*[ablation_activating_traces[k]['all'] for k in valid_keys if k != key])\n",
    "\n",
    "        num_unique.append(len(traces - all_other_traces))\n",
    "\n",
    "    average_num_unique = np.mean(num_unique)\n",
    "    std_num_unique = np.std(num_unique)\n",
    "    median_num_unique = np.median(num_unique)\n",
    "\n",
    "    print(f\"{ablation_key}: {average_num_unique} +/- {std_num_unique}, median {median_num_unique}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
