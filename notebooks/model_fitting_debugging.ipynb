{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import gzip\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tatsu\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src import fitness_energy_utils as utils\n",
    "from src.fitness_energy_utils import NON_FEATURE_COLUMNS\n",
    "from src.ast_counter_sampler import *\n",
    "from src.ast_utils import cached_load_and_parse_games_from_file, load_games_from_file, _extract_game_id\n",
    "from src import ast_printer\n",
    "from src.fitness_features_preprocessing import NGRAM_SCORE_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='..'))\n",
    "# real_game_texts = [ast_printer.ast_to_string(ast, '\\n') for ast in game_asts]\n",
    "# regrown_game_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples.pddl'))\n",
    "\n",
    "# regrown_game_asts = list(cached_load_and_parse_games_from_file('../dsl/ast-real-regrowth-samples-1024.pddl', grammar_parser, True, relative_path='..'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df = utils.load_fitness_data('../data/fitness_features_1024_regrowths.csv.gz')\n",
    "print(fitness_df.src_file.unique())\n",
    "print(fitness_df.shape)\n",
    "original_game_counts = fitness_df.groupby('original_game_name').src_file.count().value_counts()\n",
    "if len(original_game_counts) == 1:\n",
    "    print(f'All original games have {original_game_counts.index[0] - 1} regrowths')  # type: ignore\n",
    "else:\n",
    "    print('Some original games have different numbers of regrowths: {original_game_counts}')\n",
    "fitness_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'train on more features' experiment\n",
    "* Train model to convergence on some # of features\n",
    "* Add more features (and increase model size)\n",
    "* Continue training\n",
    "* Evaluate\n",
    "\n",
    "**TODO:** is this something about measuring statistics on the un-shuffled dataset at the end of training vs. training on the shuffled?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_by_abs_diff_threshold(diffs: pd.Series, score_threshold: float):\n",
    "    feature_columns = list(diffs[diffs >= score_threshold].index)\n",
    "\n",
    "    remove_all_ngram_scores = []  \n",
    "    for score_type in ('full', 'setup', 'constraints', 'terminal', 'scoring'):\n",
    "        col_names = sorted([c for c in feature_columns if c.startswith(f'ast_ngram_{score_type}') and c.endswith('_score')])\n",
    "\n",
    "        if score_type not in remove_all_ngram_scores:\n",
    "            col_names = col_names[:-1]\n",
    "\n",
    "        for col in col_names:\n",
    "            feature_columns.remove(col)\n",
    "\n",
    "    return feature_columns\n",
    "\n",
    "\n",
    "mean_features_by_real = fitness_df[['real'] + [c for c in fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()\n",
    "sorted_abs_diffs = abs_diffs.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_feature_set_columns = get_features_by_abs_diff_threshold(sorted_abs_diffs, 0.01)\n",
    "small_feature_set_columns = get_features_by_abs_diff_threshold(sorted_abs_diffs, 0.04)\n",
    "print(large_feature_set_columns[:len(small_feature_set_columns)] == small_feature_set_columns)\n",
    "\n",
    "large_feature_set_tensor = utils.df_to_tensor(fitness_df, large_feature_set_columns)\n",
    "small_feature_set_tensor = utils.df_to_tensor(fitness_df, small_feature_set_columns)\n",
    "print(torch.all(large_feature_set_tensor[:, :, :len(small_feature_set_columns)] == small_feature_set_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = large_feature_set_tensor.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 1.0\n",
    "\n",
    "scaler_kwargs = dict(passthrough=True)\n",
    "model_kwargs = dict()\n",
    "train_kwargs = dict(\n",
    "    loss_function=utils.fitness_softmin_loss,\n",
    "    k=1024,\n",
    "    lr=1e-2,\n",
    "    beta=BETA, \n",
    "    negative_score_reduction='none', \n",
    "    n_epochs=3000, \n",
    "    shuffle_negatives=True, \n",
    "    bias_init_margin_ratio=0.01,\n",
    "    # device=torch.device('cuda:0'), \n",
    "    # regularizer=regularizer,\n",
    "    split_validation_from_train=True,\n",
    "    patience_epochs=20,\n",
    "    use_lr_scheduler=False,\n",
    "    batch_size=4,\n",
    "    )\n",
    "\n",
    "scoring = utils.build_multiple_scoring_function(\n",
    "    [utils.wrap_loss_function_to_metric(utils.fitness_sofmin_loss_positive_negative_split, dict(beta=BETA), True),  # type: ignore\n",
    "     utils.evaluate_fitness_overall_ecdf, utils.evaluate_fitness_single_game_rank, utils.evaluate_fitness_single_game_min_rank, \n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.01), True),  # type: ignore\n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.05), True),  # type: ignore\n",
    "     ],\n",
    "    ['loss', 'overall_ecdf', 'single_game_rank', 'single_game_min_rank', 'energy_of_negative@1%', 'energy_of_negative@5%'],\n",
    ")\n",
    "\n",
    "model, (small_train_tensor, small_test_tensor), small_results = utils.initialize_and_fit_model(\n",
    "    small_feature_set_tensor, split_test_set=True, \n",
    "    random_seed=DEFAULT_RANDOM_SEED,\n",
    "    scaler_kwargs=scaler_kwargs, model_kwargs=model_kwargs, train_kwargs=train_kwargs,\n",
    "    scoring_function=scoring, \n",
    ")\n",
    "utils.print_results_dict(small_results)  # type: ignore\n",
    "utils.plot_loss_curves(model.named_steps['fitness'].losses, 'Initial training loss curves')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = model.named_steps['fitness'].model\n",
    "new_model = utils.FitnessEnergyModel(len(large_feature_set_columns))\n",
    "\n",
    "init_weights = utils.make_init_weight_function(0.01)\n",
    "new_model.apply(init_weights)\n",
    "\n",
    "new_model.fc1.weight.data[:, :len(small_feature_set_columns)] = old_model.fc1.weight.data\n",
    "new_model.fc1.bias.data = old_model.fc1.bias.data\n",
    "\n",
    "print(torch.all(new_model.fc1.weight.data[:, :len(small_feature_set_columns)] == old_model.fc1.weight.data), torch.all(new_model.fc1.bias.data == old_model.fc1.bias.data))\n",
    "model.named_steps['fitness'].model = new_model\n",
    "model.named_steps['fitness'].init_model = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (large_train_tensor, large_test_tensor), large_results = utils.initialize_and_fit_model(\n",
    "    large_feature_set_tensor, split_test_set=True, \n",
    "    random_seed=DEFAULT_RANDOM_SEED,\n",
    "    scaler_kwargs=scaler_kwargs, model_kwargs=model_kwargs, train_kwargs=train_kwargs,\n",
    "    scoring_function=scoring, \n",
    "    pipeline=model,\n",
    ")\n",
    "\n",
    "utils.print_results_dict(large_results)  # type: ignore\n",
    "utils.plot_loss_curves(model.named_steps['fitness'].losses, 'Post-large loss curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_only_model, (large_only_train_tensor, large_only_test_tensor), large_only_results = utils.initialize_and_fit_model(\n",
    "    large_feature_set_tensor, split_test_set=True, \n",
    "    random_seed=DEFAULT_RANDOM_SEED,\n",
    "    scaler_kwargs=scaler_kwargs, model_kwargs=model_kwargs, train_kwargs=train_kwargs,\n",
    "    scoring_function=scoring, \n",
    ")\n",
    "\n",
    "utils.print_results_dict(large_only_results)  # type: ignore\n",
    "utils.plot_loss_curves(large_only_model.named_steps['fitness'].losses, 'Post-large loss curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_losses = {f'{k} (small then large)': v for k, v in model.named_steps['fitness'].losses.items()}\n",
    "combined_losses.update({f'{k} (large only)': v for k, v in large_only_model.named_steps['fitness'].losses.items()})\n",
    "\n",
    "utils.plot_loss_curves(combined_losses, 'Loss curves from both models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same as above but for a learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 1.0\n",
    "\n",
    "scaler_kwargs = dict(passthrough=True)\n",
    "model_kwargs = dict()\n",
    "train_kwargs = dict(\n",
    "    loss_function=utils.fitness_softmin_loss,\n",
    "    k=1024,\n",
    "    lr=1e-2,\n",
    "    beta=BETA, \n",
    "    negative_score_reduction='none', \n",
    "    n_epochs=3000, \n",
    "    shuffle_negatives=True, \n",
    "    bias_init_margin_ratio=0.01,\n",
    "    # device=torch.device('cuda:0'), \n",
    "    # regularizer=regularizer,\n",
    "    split_validation_from_train=True,\n",
    "    patience_epochs=10,\n",
    "    use_lr_scheduler=False,\n",
    "    lr_scheduler_verbose=True,\n",
    "    batch_size=4,\n",
    "    )\n",
    "\n",
    "scoring = utils.build_multiple_scoring_function(\n",
    "    [utils.wrap_loss_function_to_metric(utils.fitness_sofmin_loss_positive_negative_split, dict(beta=BETA), True),  # type: ignore\n",
    "     utils.evaluate_fitness_overall_ecdf, utils.evaluate_fitness_single_game_rank, utils.evaluate_fitness_single_game_min_rank, \n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.01), True),  # type: ignore\n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.05), True),  # type: ignore\n",
    "     ],\n",
    "    ['loss', 'overall_ecdf', 'single_game_rank', 'single_game_min_rank', 'energy_of_negative@1%', 'energy_of_negative@5%'],\n",
    ")\n",
    "\n",
    "model_no_scheduler, (no_scheduler_train_tensor, no_scheduler_test_tensor), no_scheduler_results = utils.initialize_and_fit_model(\n",
    "    small_feature_set_tensor, split_test_set=True, \n",
    "    random_seed=DEFAULT_RANDOM_SEED,\n",
    "    scaler_kwargs=scaler_kwargs, model_kwargs=model_kwargs, train_kwargs=train_kwargs,\n",
    "    scoring_function=scoring, \n",
    ")\n",
    "utils.print_results_dict(no_scheduler_results)  # type: ignore\n",
    "utils.plot_loss_curves(model_no_scheduler.named_steps['fitness'].losses, 'No scheduler loss curves')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs['use_lr_scheduler'] = True\n",
    "# train_kwargs['patience_epochs'] = 20\n",
    "\n",
    "\n",
    "model_scheduler, (scheduler_train_tensor, scheduler_test_tensor), scheduler_results = utils.initialize_and_fit_model(\n",
    "    small_feature_set_tensor, split_test_set=True, \n",
    "    random_seed=DEFAULT_RANDOM_SEED,\n",
    "    scaler_kwargs=scaler_kwargs, model_kwargs=model_kwargs, train_kwargs=train_kwargs,\n",
    "    scoring_function=scoring, \n",
    ")\n",
    "utils.print_results_dict(scheduler_results)  # type: ignore\n",
    "utils.plot_loss_curves(model_scheduler.named_steps['fitness'].losses, 'Scheduler loss curves')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_losses = {f'{k} (no scheduler)': v for k, v in model_no_scheduler.named_steps['fitness'].losses.items()}\n",
    "combined_losses.update({f'{k} (scheduler)': v for k, v in model_scheduler.named_steps['fitness'].losses.items()})\n",
    "\n",
    "utils.plot_loss_curves(combined_losses, 'Loss curves from both models')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicitly trying different numbers of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 1.0\n",
    "\n",
    "scaler_kwargs = dict(passthrough=True)\n",
    "model_kwargs = dict()\n",
    "train_kwargs = dict(\n",
    "    loss_function=utils.fitness_softmin_loss,\n",
    "    k=1024,\n",
    "    lr=3e-4,\n",
    "    beta=BETA, \n",
    "    negative_score_reduction='none', \n",
    "    n_epochs=5000, \n",
    "    shuffle_negatives=True, \n",
    "    bias_init_margin_ratio=0.01,\n",
    "    # device=torch.device('cuda:0'), \n",
    "    # regularizer=regularizer,\n",
    "    split_validation_from_train=True,\n",
    "    use_lr_scheduler=False,\n",
    "    lr_scheduler_verbose=True,\n",
    "    batch_size=4,\n",
    "    patience_epochs=5000,\n",
    "    random_seed=3333,\n",
    "    )\n",
    "\n",
    "scoring = utils.build_multiple_scoring_function(\n",
    "    [utils.wrap_loss_function_to_metric(utils.fitness_sofmin_loss_positive_negative_split, dict(beta=BETA), True),  # type: ignore\n",
    "     utils.evaluate_fitness_overall_ecdf, utils.evaluate_fitness_single_game_rank, utils.evaluate_fitness_single_game_min_rank, \n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.01), True),  # type: ignore\n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.05), True),  # type: ignore\n",
    "     ],\n",
    "    ['loss', 'overall_ecdf', 'single_game_rank', 'single_game_min_rank', 'energy_of_negative@1%', 'energy_of_negative@5%'],\n",
    ")\n",
    "\n",
    "n_features_losses = {}\n",
    "n_features_results = {}\n",
    "\n",
    "for score_thresold in tqdm.tqdm(reversed([0, 0.005, 0.01, 0.02, 0.03, 0.04])):\n",
    "    features = get_features_by_abs_diff_threshold(abs_diffs, score_thresold)\n",
    "    features_model, (features_train_tensor, features_test_tensor), features_results = utils.initialize_and_fit_model(\n",
    "        fitness_df, feature_columns=features,\n",
    "        split_test_set=True, \n",
    "        random_seed=DEFAULT_RANDOM_SEED,\n",
    "        scaler_kwargs=scaler_kwargs, model_kwargs=model_kwargs, train_kwargs=train_kwargs,\n",
    "        scoring_function=scoring,)\n",
    "    \n",
    "    display(Markdown(f'## {len(features)} features'))\n",
    "    utils.print_results_dict(features_results, ['test'])  # type: ignore\n",
    "    n = len(features)\n",
    "    n_features_losses[n] = features_model.named_steps['fitness'].losses\n",
    "    n_features_results[n] = features_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ('train', 'val',)\n",
    "n_values = list(n_features_losses.keys())\n",
    "feature_losses = {f'{k} ({n})': n_features_losses[n][k] for n in n_values for k in keys}\n",
    "\n",
    "utils.plot_loss_curves(feature_losses, 'Loss curves with various # features', cmap='tab20',\n",
    "                       legend_loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    [f'**{n}**'] + [abs(n_features_results[n][key][metric])\n",
    "                    for metric in ('loss', 'shuffled_loss', 'overall_ecdf')\n",
    "                    for key in ('train', 'test') ]\n",
    "    for n in n_values\n",
    "]\n",
    "\n",
    "display(Markdown(tabulate.tabulate(rows, headers=['n', 'train loss', 'test loss', 'train shuffled loss', 'test shuffled loss',  'train ecdf', 'test ecdf'], tablefmt='github')))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 1.0\n",
    "N_WORKERS = 4\n",
    "CHUNKSIZE = 10\n",
    "\n",
    "scaler_kwargs = dict(passthrough=True)\n",
    "model_kwargs = dict()\n",
    "train_kwargs = dict(\n",
    "    loss_function=utils.fitness_softmin_loss,\n",
    "    k=1024,\n",
    "    lr=4e-3,\n",
    "    beta=BETA, \n",
    "    negative_score_reduction='none', \n",
    "    n_epochs=10000, \n",
    "    shuffle_negatives=True, \n",
    "    bias_init_margin_ratio=0.01,\n",
    "    device=torch.device('cuda:0'), \n",
    "    # regularizer=regularizer,\n",
    "    split_validation_from_train=True,\n",
    "    )\n",
    "\n",
    "sweep_param_grid = dict(\n",
    "    patience_epochs=range(50, 300, 50),\n",
    "    use_lr_scheduler=[False, True],\n",
    "    batch_size=[1, 2, 4, 8, 16], # batch_size=[1, 2, 4, 8, 16],\n",
    "    score_threshold=[0, 0.005, 0.01, 0.02, 0.03, 0.04], # score_threshold=[0, 0.005, 0.01, 0.02, 0.03, 0.04],\n",
    ")\n",
    "\n",
    "scoring = utils.build_multiple_scoring_function(\n",
    "    [utils.wrap_loss_function_to_metric(utils.fitness_sofmin_loss_positive_negative_split, dict(beta=BETA), True),  # type: ignore\n",
    "     utils.evaluate_fitness_overall_ecdf, utils.evaluate_fitness_single_game_rank, utils.evaluate_fitness_single_game_min_rank, \n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.01), True),  # type: ignore\n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.05), True),  # type: ignore\n",
    "     ],\n",
    "    ['loss', 'overall_ecdf', 'single_game_rank', 'single_game_min_rank', 'energy_of_negative@1%', 'energy_of_negative@5%'],\n",
    ")\n",
    "\n",
    "mean_features_by_real = fitness_df[['real'] + [c for c in fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()  # .sort_values(ascending=False)\n",
    "\n",
    "sweep_models = {}\n",
    "sweep_results = {}\n",
    "sweep_losses = {}\n",
    "\n",
    "def fit_configuration(setting_kwargs):\n",
    "    setting_key = list(setting_kwargs.values()) \n",
    "\n",
    "    setting_train_kwargs = train_kwargs.copy()\n",
    "    score_threshold = setting_kwargs.pop('score_threshold')\n",
    "    setting_train_kwargs.update(setting_kwargs)\n",
    "\n",
    "    feature_columns = get_features_by_abs_diff_threshold(abs_diffs, score_threshold)\n",
    "\n",
    "    model, _, results = utils.initialize_and_fit_model(\n",
    "        fitness_df, split_test_set=True, feature_columns=feature_columns,\n",
    "        random_seed=DEFAULT_RANDOM_SEED,\n",
    "        scaler_kwargs=scaler_kwargs, model_kwargs=model_kwargs, train_kwargs=setting_train_kwargs,\n",
    "        scoring_function=scoring, \n",
    "    )\n",
    "    \n",
    "    setting_key.append(len(feature_columns))\n",
    "    setting_key = tuple(setting_key)\n",
    "    sweep_models[setting_key] = model\n",
    "    sweep_results[setting_key] = results\n",
    "    sweep_losses[setting_key] = model.named_steps['fitness'].losses\n",
    "\n",
    "\n",
    "def param_combination_iterator():\n",
    "    for combination in itertools.product(*sweep_param_grid.values()):  # type: ignore\n",
    "        yield dict(zip(sweep_param_grid.keys(), combination))\n",
    "\n",
    "\n",
    "for setting_kwargs in tqdm.tqdm(param_combination_iterator(), total=np.product([len(v) for v in sweep_param_grid.values()])):  # type: ignore\n",
    "    fit_configuration(setting_kwargs)\n",
    "\n",
    "\n",
    "# with multiprocessing.Pool(N_WORKERS) as p:\n",
    "#     for row in tqdm.tqdm(p.imap_unordered(fit_configuration, param_combination_iterator(), chunksize=CHUNKSIZE), total=np.product([len(v) for v in sweep_param_grid.values()])):  # type: ignore\n",
    "#         continue\n",
    "    \n",
    "\n",
    "KEY_HEADERS = ['patience_epochs', 'use_lr_scheduler', 'batch_size', 'score_threshold', 'n_features']\n",
    "example_values = next(iter(sweep_results.values()))\n",
    "VALUE_HEADERS = [f'{outer_key}_{inner_key}' for outer_key in example_values for inner_key in example_values[outer_key]]\n",
    "\n",
    "rows = [list(key) + [results[outer_key][inner_key] for outer_key in results for inner_key in results[outer_key]]\n",
    "        for key, results in sweep_results.items()]\n",
    "\n",
    "sweep_results_df = pd.DataFrame(rows, columns=KEY_HEADERS + VALUE_HEADERS)\n",
    "sweep_results_df = sweep_results_df.assign(**{c: sweep_results_df[c].abs() for c in sweep_results_df.columns if 'ecdf' in c or 'loss' in c or 'energy_of_negative' in c}, \n",
    "                                           use_lr_scheduler=sweep_results_df.use_lr_scheduler.astype(int))\n",
    "sweep_results_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../tmp/sweep_results_large_55_n_epochs.pkl.gz', 'rb') as f:\n",
    "    full_sweep_results = pickle.load(f)\n",
    "\n",
    "sweep_results_df = full_sweep_results['sweep_results_df']\n",
    "sweep_models = full_sweep_results['sweep_models']\n",
    "sweep_losses = full_sweep_results['sweep_losses']\n",
    "\n",
    "sweep_results_df = sweep_results_df.assign(train_end_train_loss=0, train_end_val_loss=0)\n",
    "for k in sweep_losses.keys():\n",
    "    sweep_results_df.loc[(sweep_results_df.n_epochs == k[0]) & (sweep_results_df.use_lr_scheduler == int(k[1])) & (sweep_results_df.batch_size == k[2]) & (sweep_results_df.n_features == k[4]), 'train_end_train_loss'] = sweep_losses[k]['train'][-1]\n",
    "    sweep_results_df.loc[(sweep_results_df.n_epochs == k[0]) & (sweep_results_df.use_lr_scheduler == int(k[1])) & (sweep_results_df.batch_size == k[2]) & (sweep_results_df.n_features == k[4]), 'train_end_val_loss'] = sweep_losses[k]['val'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MAPPINGS = {\n",
    "    'patience_epochs': 'Patience Epochs',\n",
    "    'n_features': '# of Features Used',\n",
    "    'n_epochs': '# of Epochs Trained',\n",
    "    'use_lr_scheduler': 'Use LR Scheduler',\n",
    "    'batch_size': 'Batch Size',\n",
    "    'train_ecdf': 'Train ECDF',\n",
    "    'test_ecdf': 'Test ECDF',\n",
    "    'train_game_rank': 'Train Game Rank',\n",
    "    'test_game_rank': 'Test Game Rank',\n",
    "}\n",
    "\n",
    "\n",
    "def plot_sweep_results(\n",
    "    results_df: pd.DataFrame, \n",
    "    x_key: str, \n",
    "    color_by_key: str,\n",
    "    column_by_key: typing.Optional[str] = None,\n",
    "    row_by_key: typing.Optional[str] = None,\n",
    "    filter_conditions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "    legend_ax_index: int = 0,\n",
    "    name_mappings: typing.Dict[str, str] = NAME_MAPPINGS,\n",
    "    metrics: typing.List[str] = ['train_overall_ecdf', 'test_overall_ecdf'],\n",
    "    cmap_name: str = 'tab20',\n",
    "    ylabel: typing.Optional[str] = None,\n",
    "    show_ax_titles: bool = True,\n",
    "    subplot_adjust_params: typing.Optional[typing.Dict[str, float]] = None,\n",
    "    suptitle: typing.Optional[str] = None,\n",
    "    ):\n",
    "\n",
    "    color_values = list(sorted(results_df[color_by_key].unique()))\n",
    "    x_values = list(sorted(results_df[x_key].unique()))\n",
    "\n",
    "    column_values = []\n",
    "    if column_by_key is not None:\n",
    "        column_values = list(sorted(results_df[column_by_key].unique()))\n",
    "\n",
    "    row_values = []\n",
    "    if row_by_key is not None:\n",
    "        row_values = list(sorted(results_df[row_by_key].unique()))\n",
    "\n",
    "    if filter_conditions is not None:\n",
    "        row_filter = np.ones(len(results_df), dtype=bool)\n",
    "        for col, val in filter_conditions.items():\n",
    "            row_filter &= (results_df[col] == val)\n",
    "\n",
    "        df = results_df[row_filter]\n",
    "    else:\n",
    "        df = results_df\n",
    "\n",
    "\n",
    "    groupby_fields = []\n",
    "    n_rows = n_columns = 1\n",
    "\n",
    "    if row_by_key is not None:\n",
    "        groupby_fields.append(row_by_key)\n",
    "        n_rows = len(row_values)\n",
    "\n",
    "    if column_by_key is not None:\n",
    "        groupby_fields.append(column_by_key)\n",
    "        n_columns = len(column_values)\n",
    "        \n",
    "    groupby_fields.append(color_by_key)\n",
    "    groupby_fields.append(x_key)    \n",
    "    results_groupby = df.groupby(groupby_fields)[metrics].mean()\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_columns, figsize=(6 * n_columns, 4 * n_rows), squeeze=False)\n",
    "    cmap = plt.get_cmap(cmap_name)  # type: ignore\n",
    "\n",
    "    for row_index, row_axes in enumerate(axes):\n",
    "        row_value = None if row_by_key is None else row_values[row_index]\n",
    "        for col_index, ax in enumerate(row_axes):\n",
    "            col_value = None if column_by_key is None else column_values[col_index]\n",
    "            \n",
    "            for color_index, color_value in enumerate(color_values):\n",
    "                key = []\n",
    "                if row_value is not None: key.append(row_value)\n",
    "                if col_value is not None: key.append(col_value)\n",
    "                key.append(color_value)\n",
    "\n",
    "                for metric_index, metric in enumerate(metrics):\n",
    "                    y_values = [results_groupby.loc[tuple(key + [x])][metric] for x in x_values]\n",
    "                    ax.plot(x_values, y_values, marker='o', linestyle='--', linewidth=2, \n",
    "                            color=cmap(color_index * len(metrics) + metric_index), \n",
    "                            label=name_mappings.get(color_value, color_value) if metric_index == 0 else None)\n",
    "\n",
    "            ax.set_xlabel(name_mappings.get(x_key, x_key))\n",
    "            if col_index == 0: ax.set_ylabel(ylabel if ylabel is not None else name_mappings.get(metrics[0], metrics[0]))\n",
    "            ax.set_xticks(x_values)\n",
    "            ax.set_xticklabels(x_values)\n",
    "            if (row_index * n_columns) + col_index  == legend_ax_index: ax.legend()\n",
    "            if show_ax_titles and column_by_key is not None: ax.set_title(f'{name_mappings.get(column_by_key, column_by_key)}={col_value}')\n",
    "\n",
    "    ylim_min = min(ax.get_ylim()[0] for ax in itertools.chain.from_iterable(axes))\n",
    "    ylim_max = max(ax.get_ylim()[1] for ax in itertools.chain.from_iterable(axes))\n",
    "    for ax in itertools.chain.from_iterable(axes):\n",
    "        ax.set_ylim(ylim_min, ylim_max)\n",
    "\n",
    "    if subplot_adjust_params is not None:\n",
    "        plt.subplots_adjust(**subplot_adjust_params)\n",
    "\n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle, fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'n_epochs', \n",
    "    column_by_key='use_lr_scheduler',\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Scheduler')\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'n_epochs', \n",
    "    column_by_key='use_lr_scheduler',\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Scheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='batch_size',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Batch Size')\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='batch_size',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Batch Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'batch_size', \n",
    "    column_by_key='n_epochs',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Patience Epochs')\n",
    "\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'batch_size', \n",
    "    column_by_key='n_epochs',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Patience Epochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'patience_epochs', 'n_features', \n",
    "    column_by_key='batch_size',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Batch Size')\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'patience_epochs',  'n_features', \n",
    "    column_by_key='batch_size',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Batch Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'batch_size', \n",
    "    column_by_key='n_epochs',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Best Model Loss vs. # of Features Used')\n",
    "\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'batch_size', \n",
    "    column_by_key='n_epochs',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_overall_ecdf', 'test_overall_ecdf'],\n",
    "    ylabel='ECDF',\n",
    "    suptitle='Best Model ECDF vs. # of Features Used')\n",
    "\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'batch_size', \n",
    "    column_by_key='n_epochs',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_end_train_loss', 'train_end_val_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Train End Loss vs. # of Features Used')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_epochs', 'batch_size', \n",
    "    column_by_key='n_features',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Best Model Loss vs. # of Epochs Trained')\n",
    "\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_epochs', 'batch_size', \n",
    "    column_by_key='n_features',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_overall_ecdf', 'test_overall_ecdf'],\n",
    "    ylabel='ECDF',\n",
    "    suptitle='Best Model ECDF vs. # of Epochs Trained')\n",
    "\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_epochs', 'batch_size', \n",
    "    column_by_key='n_features',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_end_train_loss', 'train_end_val_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Train End Loss vs. # of Epochs Trained')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ../tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../tmp/sweep_results_large_22.pkl.gz', 'rb') as f:\n",
    "    full_sweep_results_22 = pickle.load(f)\n",
    "\n",
    "sweep_results_df_22 = full_sweep_results_22['sweep_results_df']\n",
    "sweep_models_22 = full_sweep_results_22['sweep_models']\n",
    "sweep_losses_22 = full_sweep_results_22['sweep_losses']\n",
    "\n",
    "sweep_results_df_22 = sweep_results_df_22.assign(train_end_train_loss=0, train_end_val_loss=0)\n",
    "for k in sweep_losses_22.keys():\n",
    "    sweep_results_df_22.loc[(sweep_results_df_22.patience_epochs == k[0]) & (sweep_results_df_22.use_lr_scheduler == int(k[1])) & (sweep_results_df_22.batch_size == k[2]) & (sweep_results_df_22.n_features == k[4]), 'train_end_train_loss'] = sweep_losses_22[k]['train'][-1]\n",
    "    sweep_results_df_22.loc[(sweep_results_df_22.patience_epochs == k[0]) & (sweep_results_df_22.use_lr_scheduler == int(k[1])) & (sweep_results_df_22.batch_size == k[2]) & (sweep_results_df_22.n_features == k[4]), 'train_end_val_loss'] = sweep_losses_22[k]['val'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df_22, 'patience_epochs', 'batch_size', \n",
    "    column_by_key='n_features',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Best Model Loss vs. Patience Epochs')\n",
    "\n",
    "\n",
    "plot_sweep_results(sweep_results_df_22, 'patience_epochs', 'batch_size', \n",
    "    column_by_key='n_features',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_overall_ecdf', 'test_overall_ecdf'],\n",
    "    ylabel='ECDF',\n",
    "    suptitle='Best Model ECDF vs. Patience Epochs')\n",
    "\n",
    "\n",
    "plot_sweep_results(sweep_results_df_22, 'patience_epochs', 'batch_size', \n",
    "    column_by_key='n_features',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_end_train_loss', 'train_end_val_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Train End Loss vs. Patience Epochs')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
