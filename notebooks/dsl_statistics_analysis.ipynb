{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.read_csv('../data/dsl_statistics_interactive.csv', index_col='Index')\n",
    "manual_df = pd.read_csv('../data/manual_dsl_statistics.csv')\n",
    "\n",
    "stats_df = stats_df.merge(manual_df, on='game_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.loc[stats_df.terminal_exists.isna(), 'terminal_exists'] = False\n",
    "\n",
    "room = np.zeros((len(stats_df),), dtype=int)\n",
    "room[['medium' in d for d in stats_df.domain_name]] = 1\n",
    "room[['many' in d for d in stats_df.domain_name]] = 2\n",
    "\n",
    "room_name = ['Few'] * len(stats_df)\n",
    "stats_df = stats_df.assign(room=room, room_name=room_name)\n",
    "\n",
    "stats_df.loc[['medium' in d for d in stats_df.domain_name], 'room_name'] = 'Medium'\n",
    "stats_df.loc[['many' in d for d in stats_df.domain_name], 'room_name'] = 'Many'\n",
    "\n",
    "stats_df.src_file = stats_df.src_file.apply(lambda s: s.replace('problems-', '').replace('.pddl', ''))\n",
    "room[['interactive' in s for s in stats_df.src_file]] = 3\n",
    "stats_df = stats_df.assign(src=room)\n",
    "\n",
    "def average_list_series(df, name):\n",
    "    avg = np.empty_like(df[name])\n",
    "    avg[:] = np.NaN\n",
    "    for i, entry in df[name].items():\n",
    "        if isinstance(entry, str):\n",
    "            avg[i] = np.fromstring(entry[1:-1], sep=',', dtype=int).mean()\n",
    "\n",
    "    return df.assign(**{f'average_{name}': avg})\n",
    "\n",
    "stats_df = average_list_series(stats_df, 'length_of_then')\n",
    "stats_df = average_list_series(stats_df, 'setup_objects_quantified')\n",
    "stats_df = average_list_series(stats_df, 'preference_objects_quantified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REORDERED_COLUMNS = ['src', 'src_file', 'game_name', 'domain_name', 'room', 'room_name',\n",
    "    'num_preferences',\n",
    "    'length_of_then', 'average_length_of_then',\n",
    "    'setup_objects_quantified', 'average_setup_objects_quantified',\n",
    "    'preference_objects_quantified', 'average_preference_objects_quantified',\n",
    "    'terminal_exists', 'object_types_quantified', 'is_throwing',\n",
    "    'is_construction', 'difficulty', 'first_time_points', 'max_depth', 'ast_nodes',\n",
    "]\n",
    "stats_df = stats_df.reindex(columns=REORDERED_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFFICULTIES = ('Very Easy', 'Easy', 'Medium', 'Hard', 'Very Hard')\n",
    "ROOM_NAMES = ('Few', 'Medium', 'Many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme:\n",
    "* `game_name`: room-\\[row number in my spreadsheet\\].\n",
    "* `domain_name`: the room identifier as it's iun the games (domain is a PDDL thing)\n",
    "* `length_of_then`: average length of all `then` clauses in this game.\n",
    "* `num_preferences`: how many preferences I used to represent the game.\n",
    "* `setup/preference_objects_quantified`: average number of objects quantified over in quantifiers (`exists`/`forall`) in the game representation, split by which section of the game it's in.\n",
    "* `terminal_exists`: whether the game uses a `terminal` clause.\n",
    "* `object_types_quantified`: how many times each object type was quantified in each game, combined between the setup and preferences. \n",
    "* `is_throwing/construction`: a manual labeling I provided by reviewing the game. Most games are either, some are both, some are neither. I decided that construction games are only those involving construction in the actual gameplay (\"build a tower to max height\"), and not those where there's some element of construction in the setup (\"stack blocks and then throw a ball to knock them over\"). We could discuss/change this classification.\n",
    "* `difficuly/first_time_points`: data we collected from the participants about their games. The `DIFFICULTIES` variables above decodes, but 1 is \"very easy\", 5 is \"very hard\", and the rest follow.\n",
    "* `max_depth`: what's the deepest the game's AST goes\n",
    "* `ast_nodes`: how many total nodes of type AST (so not strings, lists, etc.) exist in the AST.\n",
    "* `room/room_name`: simplifications of the room designation from the game/domain names to ease working with the data.\n",
    "* `src/src_file`: which file the data came from (survey by specific room or interactive beta)\n",
    "\n",
    "## Participant rating vs. ther difficulty listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = stats_df.plot(x='first_time_points', y='difficulty', kind='scatter', c='src', colormap='Dark2', colorbar=False, logx='sym', yticks=np.arange(1, 6))\n",
    "ax.set_yticklabels(DIFFICULTIES)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total nodes by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.pivot(columns='src_file').ast_nodes.plot(kind='hist', stacked=True, title='AST Nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Depth by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.pivot(columns='src_file').max_depth.plot(kind='hist', stacked=True, title='Max Depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of preferences histogram by room type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.pivot(columns='src_file').num_preferences.plot(kind='hist', stacked=True, title='Number of Preferences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average number of objects quantified in the setup/preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_list_strings(df, name):\n",
    "    data = [[] for _ in range(len(df.src.unique()))]\n",
    "    for _, (entry, src_num) in stats_df.loc[:, (name, 'src')].iterrows():\n",
    "        if isinstance(entry, str):\n",
    "            data[src_num].extend(np.fromstring(entry[1:-1], sep=',', dtype=np.int))\n",
    "\n",
    "    return data\n",
    "\n",
    "def stacked_bar_chart_from_lists(df, name):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    data = data_from_list_strings(df, name)\n",
    "    counters = [Counter(d) for d in data]\n",
    "\n",
    "    x_min = min([min(c.keys()) for c in counters])\n",
    "    x_max = max([max(c.keys()) for c in counters])\n",
    "    start = Counter({x:0 for x in range(x_min, x_max + 1)})\n",
    "\n",
    "    for i, counter in enumerate(counters):\n",
    "        x_locs = list(sorted(counter.keys()))\n",
    "        data = [counter[x] for x in x_locs]\n",
    "        ax.bar(x_locs, data, bottom=[start[x] for x in x_locs], label=stats_df.src_file.unique()[i])\n",
    "        start += counter\n",
    "    \n",
    "    ax.set_xlabel(name)\n",
    "    ax.set_ylabel('Count')\n",
    "    plt.legend(loc='upper right')\n",
    "    ticks = np.arange(x_min, x_max + 2)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_title(name.replace('_', ' ').title())\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_bar_chart_from_lists(stats_df, 'setup_objects_quantified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_bar_chart_from_lists(stats_df, 'preference_objects_quantified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average length of 'then' clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_bar_chart_from_lists(stats_df, 'length_of_then')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parsing the types/counts of objects quantified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "coocurrence_results = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "\n",
    "for i, (object_types, src_name) in stats_df.loc[:, ('object_types_quantified', 'src_file')].iterrows():\n",
    "    object_types = json.loads(object_types.replace(\"'\", '\"'))\n",
    "    for obj_type, count in object_types.items():\n",
    "        results[obj_type][src_name]['games'] += 1\n",
    "        results[obj_type][src_name]['references'] += count\n",
    "\n",
    "    types = list(object_types.keys())\n",
    "    for first_type, second_type in combinations(types, 2):\n",
    "        coocurrence_results[first_type][second_type] += 1\n",
    "        coocurrence_results[second_type][first_type] += 1\n",
    "\n",
    "result_rows = []\n",
    "for obj_type, obj_results in results.items():\n",
    "    for src_name, src_results in obj_results.items():\n",
    "        result_rows.append((obj_type, src_name, src_results['games'], src_results['references']))\n",
    "\n",
    "object_counts_df = pd.DataFrame.from_records(result_rows, \n",
    "    columns=('object_type', 'src_file', 'games', 'references'))\n",
    "\n",
    "\n",
    "all_object_types = object_counts_df.object_type.unique()\n",
    "n_object_types = len(all_object_types)\n",
    "object_type_cocurrence = np.zeros((n_object_types, n_object_types), dtype=np.int)\n",
    "object_type_to_id = {obj_type:i for i, obj_type in enumerate(all_object_types)}\n",
    "\n",
    "for first_obj_type, first_obj_results in coocurrence_results.items():\n",
    "    for second_obj_type, count in first_obj_results.items():\n",
    "        object_type_cocurrence[object_type_to_id[first_obj_type]][object_type_to_id[second_obj_type]] = count\n",
    "\n",
    "\n",
    "object_src_counts = object_counts_df.groupby('object_type').src_file.count()\n",
    "\n",
    "object_counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme\n",
    "* `object_type`: the type of the object\n",
    "* `src_file`: as above, the name of the source file (room survet or interactive) by the number of items\n",
    "* `games`: how many games (for this room) refer to this object at least once, as part of quantifiers (`exists`/`forall`).\n",
    "* `references`: how many total references appear to this object type in quantifiers in games of this type. \n",
    "* **Important**: I realize now that this undercounts -- since it only looks in references, and doesn't look in places where we directly refer to a non-quantified object (such as the desk or bed). I should think about how to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "cumulative_start = defaultdict(lambda: 0)\n",
    "\n",
    "for src_file in object_counts_df.src_file.unique():\n",
    "    src_data = object_counts_df.loc[object_counts_df.src_file == src_file, ('object_type', 'games')]\n",
    "    current_start = [cumulative_start[obj] for obj in src_data.object_type]\n",
    "    ax.barh(src_data.object_type, src_data.games, left=current_start, label=src_file)\n",
    "\n",
    "    for i, (obj, count) in src_data.iterrows():\n",
    "        cumulative_start[obj] += count\n",
    "\n",
    "ax.set_xlabel('Count of games referring to the object')\n",
    "ax.set_ylabel('Object')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Would the above be more interesting as...?**\n",
    "* Fraction of the games that referred to the item?\n",
    "    * ... from the games that could (that is, from the ones that had the type of object in the room)\n",
    "* Counting the total quantifications, instead of only the number of games that refer to a particular type?\n",
    "    * Graph with the total quantifications below:\n",
    "* Makes some sort of aggregation? Balls, blocks, etc.?\n",
    "* Separated between setup and preferences?\n",
    "* **Important**: I realize now that this undercounts -- since it only looks in references, and doesn't look in places where we directly refer to a non-quantified object (such as the desk or bed). I should think about how to fix that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "cumulative_start = defaultdict(lambda: 0)\n",
    "\n",
    "for src_name in object_counts_df.src_file.unique():\n",
    "    src_data = object_counts_df.loc[object_counts_df.src_file == src_file, ('object_type', 'references')]\n",
    "    current_start = [cumulative_start[obj] for obj in src_data.object_type]\n",
    "    ax.barh(src_data.object_type, src_data.references, left=current_start, label=src_name)\n",
    "\n",
    "    for i, (obj, count) in src_data.iterrows():\n",
    "        cumulative_start[obj] += count\n",
    "\n",
    "ax.set_xlabel('Count of total quantified references to the object')\n",
    "ax.set_ylabel('Object')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-ocurrence between different objects (within a game)\n",
    "\n",
    "* Would this be more interesting as within a particular quantification (exists/forall)? \n",
    "* Or split by setup and preferences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.gca()\n",
    "plt.imshow(object_type_cocurrence)\n",
    "plt.xticks(np.arange(n_object_types), all_object_types, rotation='vertical')\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "plt.yticks(np.arange(n_object_types), all_object_types)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same as above, but sorted by count\n",
    "\n",
    "Loses some of the block structure, but gains a better sense of the important and recurring objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocurrence_counts = object_type_cocurrence.sum(axis=0)\n",
    "desc_order = np.argsort(ocurrence_counts)[::-1]\n",
    "sorted_object_type_cocurrence = np.zeros_like(object_type_cocurrence)\n",
    "\n",
    "for i in range(sorted_object_type_cocurrence.shape[0]):\n",
    "    sorted_object_type_cocurrence[i, :] = object_type_cocurrence[desc_order[i], desc_order]\n",
    "\n",
    "sorted_object_types = all_object_types[desc_order]\n",
    "\n",
    "temp = np.copy(object_type_cocurrence).astype(np.float)\n",
    "for i, i_obj_type in enumerate(sorted_object_types):\n",
    "    for j, j_obj_type in enumerate(sorted_object_types):\n",
    "        temp[i,j] /= min(object_room_counts[i_obj_type], object_room_counts[j_obj_type])\n",
    "\n",
    "normed_sorted_object_type_cocurrence = np.zeros_like(temp)\n",
    "normed_desc_order = np.argsort(temp.sum(axis=0))[::-1]\n",
    "for i in range(normed_sorted_object_type_cocurrence.shape[0]):\n",
    "    normed_sorted_object_type_cocurrence[i, :] = temp[normed_desc_order[i], normed_desc_order]\n",
    "\n",
    "normed_sorted_object_types = all_object_types[normed_desc_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.gca()\n",
    "plt.imshow(normed_sorted_object_type_cocurrence)\n",
    "plt.xticks(np.arange(n_object_types), normed_sorted_object_types, rotation='vertical')\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "plt.yticks(np.arange(n_object_types), normed_sorted_object_types)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation table and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS = [\n",
    "    'src', 'average_length_of_then', 'average_setup_objects_quantified', 'average_preference_objects_quantified',\n",
    "    'terminal_exists', 'is_throwing', 'is_construction', 'difficulty', 'first_time_points'\n",
    "]\n",
    "\n",
    "numeric_df = stats_df[NUMERIC_COLUMNS]\n",
    "numeric_df.loc[:, 'terminal_exists'] = numeric_df.terminal_exists.astype(np.float)\n",
    "\n",
    "arr = numeric_df.to_numpy(dtype=np.float)\n",
    "nan_masked_arr = np.ma.masked_invalid(arr)\n",
    "corr = np.ma.corrcoef(nan_masked_arr, rowvar=False)\n",
    "corr_df = pd.DataFrame(corr, columns=numeric_df.columns, index=numeric_df.columns)\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = nan_masked_arr.shape[1]\n",
    "n = np.zeros((n_rows, n_rows), dtype=np.int)\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(i, n_rows):\n",
    "        n[i, j] = n[j, i] = np.invert(nan_masked_arr.mask[:,[i, j]].any(axis=1)).sum()\n",
    "\n",
    "n\n",
    "\n",
    "t_stat = corr.data * np.sqrt(n - 2) / np.sqrt(1 - (corr.data ** 2))\n",
    "t_stat[np.diag_indices(t_stat.shape[0])] = 0\n",
    "p_values = stats.t(n - 2).sf(np.abs(t_stat)) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_p_values = []\n",
    "for i, row in enumerate(p_values):\n",
    "    output_p_values.append([corr_df.columns[i]] + [f'{p:1.4f}{\"*\" if p < 0.05 else \"\"}{\"*\" if p < 0.01 else \"\"}{\"*\" if p < 0.001 else \"\"}' for p in row])\n",
    "\n",
    "headers = [\"\"] + corr_df.columns\n",
    "tabulate(output_p_values, headers, tablefmt='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* < 0.05, \\** < 0.01, *** < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7f8e00f851a7185e5345342178c14041451eaa6562c62790473e641b6de40ed"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
