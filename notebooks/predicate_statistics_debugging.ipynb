{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 17:38:05 - ast_utils - DEBUG    - Using cache folder: /Users/guydavidson/tmp/game_generation_cache\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import duckdb\n",
    "from functools import reduce\n",
    "import glob\n",
    "import gzip\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import tatsu, tatsu.ast\n",
    "import tqdm.notebook as tqdmn\n",
    "import sqlparse\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('../reward-machine'))\n",
    "\n",
    "import compile_predicate_statistics\n",
    "import compile_predicate_statistics_split_args\n",
    "from compile_predicate_statistics_split_args import *\n",
    "from config import SPECIFIC_NAMED_OBJECTS_BY_ROOM\n",
    "import config\n",
    "from utils import get_object_assignments, extract_variables\n",
    "\n",
    "import ast_printer\n",
    "import ast_parser\n",
    "import ast_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2231745, 13)\n"
     ]
    }
   ],
   "source": [
    "cache_dir = compile_predicate_statistics.get_project_dir() + '/reward-machine/caches'\n",
    "\n",
    "bitstings_df_path = os.path.join(cache_dir, 'predicate_statistics_bitstring_intervals_028b3733.pkl.gz')\n",
    "bitstrings_df = pd.read_pickle(bitstings_df_path)\n",
    "print(bitstrings_df.shape)\n",
    "with gzip.open(os.path.join(cache_dir, 'trace_lengths_028b3733.pkl'), 'rb') as f:\n",
    "    trace_lengths_and_domains = pickle.load(f)\n",
    "\n",
    "\n",
    "# regular_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics.pkl'))\n",
    "split_args_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics_028b3733.pkl.gz'))\n",
    "# split_args_df = split_args_df[split_args_df['predicate'] != 'same_type']\n",
    "# print(split_args_df.shape)\n",
    "# # split_args_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics_4d5dd602.pkl.gz'))\n",
    "\n",
    "# # stats = compile_predicate_statistics.CommonSensePredicateStatistics(cache_dir)\n",
    "# split_args_stats = compile_predicate_statistics_split_args.CommonSensePredicateStatisticsSplitArgs(\n",
    "#     # cache_dir, compile_predicate_statistics_split_args.CURRENT_TEST_TRACE_NAMES, overwrite=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = 855\n",
    "# end = 856\n",
    "# def find_index_ranges(string):\n",
    "#     ranges = []\n",
    "#     start = None\n",
    "#     for i, char in enumerate(string):\n",
    "#         if char == '1':\n",
    "#             if start is None:\n",
    "#                 start = i\n",
    "#         else:\n",
    "#             if start is not None:\n",
    "#                 ranges.append((start, i))\n",
    "#                 start = None\n",
    "#     if start is not None:\n",
    "#         ranges.append((start, len(string)))\n",
    "#     return ranges\n",
    "\n",
    "# relevant_rows = (bitstrings_df.trace_id == 'G2chsLHjitbS4fASq4vq-preCreateGame-rerecorded') & (bitstrings_df.predicate == 'on') & (bitstrings_df.arg_1_id == 'Drawer|-01.52|+00.14|+00.35') & (bitstrings_df.arg_2_id == 'Golfball|+00.96|+01.04|-02.70')\n",
    "# d = bitstrings_df[relevant_rows]\n",
    "# print(d.shape)\n",
    "# intervals = d.iloc[0].intervals\n",
    "# print(find_index_ranges(intervals))\n",
    "# intervals = intervals[:start] + '1' * (end - start) + intervals[end:]\n",
    "# print(find_index_ranges(intervals))\n",
    "# bitstrings_df.loc[relevant_rows, 'intervals'] = intervals\n",
    "\n",
    "# bitstrings_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed = False\n",
    "\n",
    "# for i in (1, 2):\n",
    "#     rows = (bitstrings_df[f'arg_{i}_id'] == 'SmallSlide|-01.31|+00.14|-03.10') & (bitstrings_df[f'arg_{i}_type'] == 'triangle_block_tan')\n",
    "#     unique_types = bitstrings_df.loc[rows][f'arg_{i}_type'].unique()\n",
    "#     unique_is_blocks = bitstrings_df.loc[rows][f'arg_{i}_is_block'].unique()\n",
    "#     print(bitstrings_df.loc[rows, [f'arg_{i}_type', f'arg_{i}_is_block']].shape, unique_types, unique_is_blocks)\n",
    "#     if len(unique_types) == 1 and unique_types[0] == 'triangle_block_tan':\n",
    "#         print('Editing type to triangular_ramp_tan')\n",
    "#         bitstrings_df.loc[rows, f'arg_{i}_type'] = 'triangular_ramp_tan'\n",
    "\n",
    "#     if len(unique_is_blocks) == 1 and unique_is_blocks[0] == True:\n",
    "#         print('Editing is_block to False')\n",
    "#         bitstrings_df.loc[rows, f'arg_{i}_is_block'] = False\n",
    "#         changed = True\n",
    "\n",
    "#         unique_types = bitstrings_df.loc[rows][f'arg_{i}_type'].unique()\n",
    "#         unique_is_blocks = bitstrings_df.loc[rows][f'arg_{i}_is_block'].unique()\n",
    "#         print(bitstrings_df.loc[rows, [f'arg_{i}_type', f'arg_{i}_is_block']].shape, unique_types, unique_is_blocks)\n",
    "#         print()\n",
    "\n",
    "# for i in (1, 2):\n",
    "#     rows = (bitstrings_df[f'arg_{i}_id'] == 'TriangleBlock|-02.94|+01.23|-02.46')\n",
    "#     unique_types = bitstrings_df.loc[rows][f'arg_{i}_type'].unique()\n",
    "#     print(bitstrings_df.loc[rows].shape, unique_types)\n",
    "\n",
    "#     if len(unique_types) == 1:\n",
    "#         partial_copy = bitstrings_df.loc[rows].copy()\n",
    "#         partial_copy[f'arg_{i}_type'] = 'triangle_block_tan'\n",
    "#         bitstrings_df = pd.concat([bitstrings_df, partial_copy])  # type: ignore\n",
    "#         changed = True\n",
    "\n",
    "#         rows = bitstrings_df[f'arg_{i}_id'] == 'TriangleBlock|-02.94|+01.23|-02.46'\n",
    "#         type_value_counts = bitstrings_df.loc[rows][f'arg_{i}_type'].value_counts()\n",
    "#         print(bitstrings_df.loc[rows].shape, type_value_counts)\n",
    "#         print()\n",
    "\n",
    "# if changed:\n",
    "#     bitstrings_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_index = bitstrings_df[bitstrings_df.arg_1_type.isin(config.ON_EXCLUDED_OBJECT_TYPES) & (bitstrings_df.predicate == 'on')].index\n",
    "# filtered_bitstrings_df = bitstrings_df.drop(drop_index)\n",
    "# filtered_bitstrings_df = filtered_bitstrings_df.reset_index(drop=True)\n",
    "\n",
    "# print(filtered_bitstrings_df.shape, bitstrings_df.shape, \n",
    "#       bitstrings_df.shape[0] - filtered_bitstrings_df.shape[0], \n",
    "#       filtered_bitstrings_df.index.max())\n",
    "\n",
    "# filtered_bitstrings_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_bitstrings_df = bitstrings_df.sort_values(by=['predicate', 'arg_1_type', 'arg_2_type', 'trace_id'])\n",
    "# sorted_bitstrings_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_object_excluded_types = set(config.GAME_OBJECT_EXCLUDED_TYPES)\n",
    "# bitstrings_df = bitstrings_df.assign(arg_1_is_game_object=~bitstrings_df.arg_1_type.isin(game_object_excluded_types), arg_2_is_game_object=~bitstrings_df.arg_2_type.isin(game_object_excluded_types))\n",
    "# bitstrings_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitstrings_df = bitstrings_df.assign(arg_1_is_block=bitstrings_df.arg_1_type.str.contains('block'), arg_2_is_block=bitstrings_df.arg_2_type.str.contains('block'))\n",
    "# bitstrings_df.to_pickle(bitstings_df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bytes([0, 1])\n",
    "BYTE_MAPPING = {b: str(i) for i, b in enumerate(b)}\n",
    "\n",
    "\n",
    "def row_to_string_intervals(row):\n",
    "    value = np.zeros(row['trace_length'], dtype=np.uint8)\n",
    "    for interval in row['intervals']:\n",
    "        value[interval[0]:interval[1]] = 1\n",
    "\n",
    "    return ''.join(map(lambda b: BYTE_MAPPING[b], value.tobytes()))\n",
    "    \n",
    "\n",
    "def create_bitstings_df(df, trace_lengths_and_domains_dict, output_path):\n",
    "    trace_lengths_and_domains_rows = [(key, *value) for key, value in trace_lengths_and_domains_dict.items()]\n",
    "    trace_lengths_and_domains_df = pd.DataFrame(trace_lengths_and_domains_rows, columns=['trace_id', 'trace_length', 'domain'])\n",
    "\n",
    "    split_args_with_trace_length_df = df.join(trace_lengths_and_domains_df.drop(columns=['domain']).set_index('trace_id'), on='trace_id')\n",
    "    split_args_with_string_intervals_df = split_args_with_trace_length_df.assign(intervals=split_args_with_trace_length_df.apply(row_to_string_intervals, axis=1))\n",
    "    split_args_with_string_intervals_df.drop(columns=['trace_length']).to_pickle(output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_lengths_and_domains_rows = [(key, *value) for key, value in trace_lengths_and_domains.items()]\n",
    "trace_lengths_and_domains_df = pd.DataFrame(trace_lengths_and_domains_rows, columns=['trace_id', 'trace_length', 'domain'])\n",
    "\n",
    "split_args_with_trace_length_df = split_args_df.join(trace_lengths_and_domains_df.drop(columns=['domain']).set_index('trace_id'), on='trace_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bytes([0, 1])\n",
    "BYTE_MAPPING = {b: str(i) for i, b in enumerate(b)}\n",
    "\n",
    "\n",
    "def row_to_string_intervals(row):\n",
    "    value = np.zeros(row['trace_length'], dtype=np.uint8)\n",
    "    for interval in row['intervals']:\n",
    "        value[interval[0]:interval[1]] = 1\n",
    "\n",
    "    return ''.join(map(lambda b: BYTE_MAPPING[b], value.tobytes()))\n",
    "    # return np.array2string(value, separator='', threshold=max_length + 10)[1:-1].replace('\\n ', '')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_with_string_intervals_df = split_args_with_trace_length_df.assign(intervals=split_args_with_trace_length_df.apply(row_to_string_intervals, axis=1))\n",
    "split_args_with_string_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_with_string_intervals_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_with_string_intervals_df.loc[\n",
    "    (split_args_with_string_intervals_df.arg_1_type == 'ball'), \"arg_1_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arg_id, arg_type in compile_predicate_statistics_split_args.OBJECT_ID_TYPE_REMAP.items():\n",
    "    original_type = None\n",
    "    if 'window' in arg_id.lower():\n",
    "        original_type = 'window'\n",
    "    if 'shelf' in arg_id.lower():\n",
    "        original_type = 'shelf'\n",
    "    \n",
    "    if original_type is None:\n",
    "        raise ValueError(f'Could not find original type for {arg_id}')\n",
    "    \n",
    "    split_args_with_string_intervals_df.loc[\n",
    "    (split_args_with_string_intervals_df.arg_1_id == arg_id) & (split_args_with_string_intervals_df.arg_1_type == original_type), \"arg_1_type\"] = arg_type\n",
    "\n",
    "    split_args_with_string_intervals_df.loc[\n",
    "    (split_args_with_string_intervals_df.arg_2_id == arg_id) & (split_args_with_string_intervals_df.arg_2_type == original_type), \"arg_2_type\"] = arg_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from config import OBJECTS_BY_ROOM_AND_TYPE, SPECIFIC_NAMED_OBJECTS_BY_ROOM, META_TYPES, GAME_OBJECT, BUILDING\n",
    "\n",
    "all_df_types = set(split_args_with_string_intervals_df.arg_1_type.unique()) | set(split_args_with_string_intervals_df.arg_2_type.unique())\n",
    "computed_types = set(reduce(lambda x, y: x + y, [list(x.keys()) for x in chain(OBJECTS_BY_ROOM_AND_TYPE.values(), SPECIFIC_NAMED_OBJECTS_BY_ROOM.values())]))\n",
    "computed_types.difference_update(META_TYPES.keys())\n",
    "computed_types.remove(GAME_OBJECT)\n",
    "computed_types.add(BUILDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_types - computed_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_types - all_df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAMED_TYPES = \"\"\"blue_cube_block\n",
    "tan_cube_block\n",
    "yellow_cube_block\n",
    "blue_pyramid_block\n",
    "red_pyramid_block\n",
    "yellow_pyramid_block\n",
    "blue_dodgeball\n",
    "red_dodgeball\n",
    "pink_dodgeball\n",
    "green_golfball\n",
    "green_triangular_ramp\"\"\".split('\\n')\n",
    "\n",
    "class DefaultValueDict(dict):\n",
    "    def __init__(self, *args, **kawags):\n",
    "        super().__init__(*args, **kawags)\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        return key\n",
    "    \n",
    "arg_type_mapping = DefaultValueDict()\n",
    "for renamed_type in RENAMED_TYPES:\n",
    "    sp = renamed_type.split('_')\n",
    "    new_name = '_'.join(sp[1:] + sp[:1])\n",
    "    arg_type_mapping[renamed_type] =  new_name\n",
    "\n",
    "\n",
    "\n",
    "bitstrings_df.assign(arg_1_type=bitstrings_df.arg_1_type.map(arg_type_mapping), \n",
    "                     arg_2_type=bitstrings_df.arg_2_type.map(arg_type_mapping),).to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = split_args_stats.trace_lengths_and_domains_df.select(pl.col('trace_length').max()).item()\n",
    "print(MAX_LENGTH)\n",
    "b = bytes([0, 1])\n",
    "BYTE_MAPPING = {b: str(i) for i, b in enumerate(b)}\n",
    "\n",
    "\n",
    "\n",
    "def intervals_to_string(intervals, max_length: int = MAX_LENGTH):\n",
    "    value = np.zeros(max_length, dtype=np.uint8)\n",
    "    for interval in intervals:\n",
    "        value[interval[0]:interval[1]] = 1\n",
    "\n",
    "    return ''.join(map(lambda b: BYTE_MAPPING[b], value.tobytes()))\n",
    "    # return np.array2string(value, separator='', threshold=max_length + 10)[1:-1].replace('\\n ', '')\n",
    "\n",
    "\n",
    "intervals = split_args_df.intervals.apply(intervals_to_string)\n",
    "\n",
    "# small_split_args_df = small_split_args_df.assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ast_utils import cached_load_and_parse_games_from_file\n",
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitness_features import *\n",
    "\n",
    "class SmallLogicalsRefactorTestFeaturizer(PredicateFoundInDataSmallLogicals):\n",
    "    def __init__(self, use_trace_ids: bool = True):\n",
    "        super().__init__()\n",
    "        self.use_trace_ids = use_trace_ids\n",
    "        self.current_implementation_more = defaultdict(dict)\n",
    "        self.refactored_implementation_more = defaultdict(dict)\n",
    "\n",
    "    def _query_predicate_data_estimator(self, pred: tatsu.ast.AST, \n",
    "                                        mapping: typing.Dict[str, typing.Union[str, typing.List[str]]],\n",
    "                                        context: ContextDict) -> typing.Optional[int]:\n",
    "\n",
    "        current_results = self.predicate_data_estimator.filter(pred, mapping, return_trace_ids=self.use_trace_ids)\n",
    "\n",
    "        # refactored_method = self.predicate_data_estimator._handle_and_refactored if 'and_args' in pred else self.predicate_data_estimator._handle_or_refactored\n",
    "        # refactored_query = refactored_method(pred, mapping, return_trace_ids=self.use_trace_ids)[0]\n",
    "        # refactored_results = self.predicate_data_estimator.con.execute(refactored_query).fetchall()\n",
    "        refactored_results = self.predicate_data_estimator.filter(pred, mapping, return_trace_ids=self.use_trace_ids, use_refactored_impl=True)\n",
    "\n",
    "        if self.use_trace_ids:\n",
    "            current_results = set(current_results)\n",
    "            refactored_results = set(itertools.chain.from_iterable(refactored_results))\n",
    "\n",
    "            if current_results.symmetric_difference(refactored_results):\n",
    "                pred_str = ast_printer.ast_section_to_string(pred, context[SECTION_CONTEXT_KEY]) + '_' + str(mapping) # type: ignore\n",
    "                only_current_trace_ids = current_results - refactored_results\n",
    "                only_refactored_trace_ids = refactored_results - current_results\n",
    "\n",
    "                rule = pred.parseinfo.rule  # type: ignore\n",
    "\n",
    "                if only_current_trace_ids and pred_str not in self.current_implementation_more:\n",
    "                    self.current_implementation_more[rule][pred_str] = only_current_trace_ids\n",
    "\n",
    "                if only_refactored_trace_ids and rule not in self.refactored_implementation_more:\n",
    "                    self.refactored_implementation_more[rule][pred_str] = only_refactored_trace_ids\n",
    "                    \n",
    "        else:\n",
    "            if current_results != refactored_results:\n",
    "                pred_str = ast_printer.ast_section_to_string(pred, context[SECTION_CONTEXT_KEY]) + '_' + str(mapping)\n",
    "                rule = pred.parseinfo.rule  # type: ignore\n",
    "                result_set = self.current_implementation_more if current_results > refactored_results else self.refactored_implementation_more\n",
    "                result_set[rule][pred_str] = current_results - refactored_results\n",
    "\n",
    "args = Namespace(\n",
    "    no_binarize=False, \n",
    "    no_merge=False, \n",
    "    use_specific_objects_ngram_model=False,\n",
    "    include_predicate_under_modal_terms=False,\n",
    "    include_arg_types_terms=False,\n",
    "    include_compositionality_terms=False,\n",
    ")\n",
    "preprocessors = []\n",
    "\n",
    "if not args.no_binarize:\n",
    "    preprocessors.append(BinarizeFitnessFeatures())\n",
    "\n",
    "if not args.no_merge and args.include_arg_types_terms:  # the merge is only used for the arg_types featuers\n",
    "    preprocessors.append(MergeFitnessFeatures(COMMON_SENSE_PREDICATES_FUNCTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness = ASTFitnessFeaturizer(args, preprocessors=preprocessors)\n",
    "refactor_test = SmallLogicalsRefactorTestFeaturizer(False)\n",
    "fitness.register(refactor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [fitness.parse(game_asts[i], 'interactive-beta.pddl', return_row=False) for i in range(len(game_asts))]\n",
    "\n",
    "\n",
    "print('Current implementation more:')\n",
    "for rule, rule_dict in refactor_test.current_implementation_more.items():\n",
    "    print(f'Under rule {rule}:')\n",
    "    for pred_str, diff in rule_dict.items():\n",
    "        if isinstance(diff, set):\n",
    "            diff = len(diff)\n",
    "        print(f'{pred_str} => {diff}')\n",
    "\n",
    "print('Refactored implementation more:')\n",
    "for rule, rule_dict in refactor_test.refactored_implementation_more.items():\n",
    "    print(f'Under rule {rule}:')\n",
    "    for pred_str, diff in rule_dict.items():\n",
    "        if isinstance(diff, set):\n",
    "            diff = len(diff)\n",
    "        print(f'{pred_str} => {diff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 17:38:36 - compile_predicate_statistics_full_database - INFO     - Loading data from files with query timeout 15\n",
      "2024-05-07 17:38:58 - compile_predicate_statistics_full_database - INFO     - Creating DuckDB table...\n",
      "2024-05-07 17:38:59 - compile_predicate_statistics_full_database - INFO     - Creating data table indices...\n",
      "2024-05-07 17:39:21 - compile_predicate_statistics_full_database - INFO     - Loaded data, found 843825 rows\n"
     ]
    }
   ],
   "source": [
    "import compile_predicate_statistics_full_database\n",
    "\n",
    "stats = compile_predicate_statistics_full_database.CommonSensePredicateStatisticsFullDatabase.get_instance(force_trace_names_hash='028b3733', log_queries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 17:41:48 - src.ast_utils - INFO     - Loading from cache file: /Users/guydavidson/tmp/game_generation_cache/interactive-beta-cache.pkl.gz\n",
      "2024-05-07 17:41:48 - src.ast_utils - INFO     - Finished loading cache file: /Users/guydavidson/tmp/game_generation_cache/interactive-beta-cache.pkl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(or  (and    (adjacent ?g agent)    (adjacent door agent)    (agent_holds ?d) )  (near door ?g)  (and    (not      (agent_holds ?v0)   )    (in_motion ?v0) )  (and    (not      (in_motion ?v3)   )    (in ?v2 ?v3)    (on ?v2 ?v3) ))\n"
     ]
    }
   ],
   "source": [
    "from src.ast_utils import cached_load_and_parse_games_from_file\n",
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='.'))\n",
    "\n",
    "test_game_str = \"\"\"\n",
    "(define (game test-game) (:domain many-objects-room-v1)\n",
    "(:constraints (and\n",
    "    (preference testPred\n",
    "        (exists (?v1 - building ?v2 - block)\n",
    "            (at-end (or  \n",
    "                (and (adjacent ?g agent) (adjacent door agent) (agent_holds ?d))\n",
    "                (near door ?g)\n",
    "                (and (not (agent_holds ?v0)) (in_motion ?v0))\n",
    "                (and (not (in_motion ?v3)) (in ?v2 ?v3) (on ?v2 ?v3)) \n",
    "                \n",
    "                ; (agent_holds ?v1)\n",
    "                ; (and (not (agent_holds ?v1)) (in_motion ?v1))\n",
    "                ; (and (not (in_motion ?v1)) (in ?v0 ?v1))\n",
    "\n",
    "                ; (adjacent bed agent)\n",
    "                ; (on bed ?v2)\n",
    "                ; (on bed agent)  \n",
    "                ; (in_motion ?r)  \n",
    "                ; (not    (agent_holds ?r) )  \n",
    "                ; (not    (touch floor ?r) )\n",
    "                ; (not    (in_motion ?d) )  \n",
    "                ; (not    (in_motion ?b) )  \n",
    "                ; (not    (object_orientation ?t upright) )\n",
    "                ; (in_motion ?d)\n",
    "                ; (not (agent_holds ?d))\n",
    "                ; (not (agent_holds ?t))\n",
    "                ; (adjacent agent bed)\n",
    "                ; (not (agent_holds ?v1))\n",
    "                ; (not (on bed ?v2))\n",
    "            ))\n",
    "        )\n",
    "    )\n",
    "))\n",
    "(:scoring (+\n",
    "    (* (count testPred) 1)\n",
    ")))\n",
    "\"\"\".strip()\n",
    "# test_game_str = \"\"\"\n",
    "# (define (game test-game) (:domain many-objects-room-v1)\n",
    "# (:constraints (and\n",
    "#     (preference testPred\n",
    "#         (exists (?v0 - block ?v1 - wall)\n",
    "#             (at-end (and \n",
    "#                 (on ?v1 ?v0)\n",
    "#                 (in ?v0 ?v1)\n",
    "#             ))\n",
    "#         )\n",
    "#     )\n",
    "# ))\n",
    "# (:scoring (+\n",
    "#     (* (count testPred) 1)\n",
    "# )))\n",
    "# \"\"\".strip()\n",
    "\n",
    "test_ast = grammar_parser.parse(test_game_str)\n",
    "test_pred = test_ast[3][1].preferences[0].definition.pref_body.body.exists_args.at_end_pred.pred\n",
    "print(ast_printer.ast_section_to_string(test_pred, ast_parser.PREFERENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(and  (adjacent ?g agent)  (adjacent door agent)  (agent_holds ?d))\n"
     ]
    }
   ],
   "source": [
    "print(ast_printer.ast_section_to_string(test_pred.or_args[0], ast_parser.PREFERENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>intervals</th>\n",
       "      <th>door</th>\n",
       "      <th>?g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5217Qbj8V5farCF3bBJT-createGame-rerecorded</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x07\\xfe\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FyGQn1qJCLTLU1hfQfZ2-freePlay-rerecorded</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x05\\xf8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FyGQn1qJCLTLU1hfQfZ2-gameplay-attempt-1-rereco...</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x01\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ShyRlIFgTWW6vqNrd3K5-gameplay-attempt-1-rereco...</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x06\\xfc\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q6a8AbiIdcLA9tJzAu14-gameplay-attempt-1-rereco...</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x01\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5217Qbj8V5farCF3bBJT-gameplay-attempt-1-rereco...</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x05\\xf8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IvoZWi01FO2uiNpNHyci-gameplay-attempt-2-rereco...</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SFpazSkgQ7MFSwDEa3c9-gameplay-attempt-1-rereco...</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x06\\xfc\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YB9HWpd4rMWwS3P2mCVk-freePlay-rerecorded</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x02\\xc0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gPJmGRgPdHSlBI7IMc3z-gameplay-attempt-2-rereco...</td>\n",
       "      <td>many</td>\n",
       "      <td>b'\\x04\\xf0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>door</td>\n",
       "      <td>Golfball|+01.05|+01.04|-02.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            trace_id domain  \\\n",
       "0         5217Qbj8V5farCF3bBJT-createGame-rerecorded   many   \n",
       "1           FyGQn1qJCLTLU1hfQfZ2-freePlay-rerecorded   many   \n",
       "2  FyGQn1qJCLTLU1hfQfZ2-gameplay-attempt-1-rereco...   many   \n",
       "3  ShyRlIFgTWW6vqNrd3K5-gameplay-attempt-1-rereco...   many   \n",
       "4  Q6a8AbiIdcLA9tJzAu14-gameplay-attempt-1-rereco...   many   \n",
       "5  5217Qbj8V5farCF3bBJT-gameplay-attempt-1-rereco...   many   \n",
       "6  IvoZWi01FO2uiNpNHyci-gameplay-attempt-2-rereco...   many   \n",
       "7  SFpazSkgQ7MFSwDEa3c9-gameplay-attempt-1-rereco...   many   \n",
       "8           YB9HWpd4rMWwS3P2mCVk-freePlay-rerecorded   many   \n",
       "9  gPJmGRgPdHSlBI7IMc3z-gameplay-attempt-2-rereco...   many   \n",
       "\n",
       "                                           intervals  door  \\\n",
       "0  b'\\x07\\xfe\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "1  b'\\x05\\xf8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "2  b'\\x01\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "3  b'\\x06\\xfc\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "4  b'\\x01\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "5  b'\\x05\\xf8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "6  b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "7  b'\\x06\\xfc\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "8  b'\\x02\\xc0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "9  b'\\x04\\xf0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...  door   \n",
       "\n",
       "                              ?g  \n",
       "0  Golfball|+01.05|+01.04|-02.70  \n",
       "1  Golfball|+01.05|+01.04|-02.70  \n",
       "2  Golfball|+01.05|+01.04|-02.70  \n",
       "3  Golfball|+01.05|+01.04|-02.70  \n",
       "4  Golfball|+01.05|+01.04|-02.70  \n",
       "5  Golfball|+01.05|+01.04|-02.70  \n",
       "6  Golfball|+01.05|+01.04|-02.70  \n",
       "7  Golfball|+01.05|+01.04|-02.70  \n",
       "8  Golfball|+01.05|+01.04|-02.70  \n",
       "9  Golfball|+01.05|+01.04|-02.70  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {'?g': ['golfball_green'], '?d':  ['dodgeball']}\n",
    "stats.filter(test_pred.or_args[1], mapping, return_full_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_machine_trace_filter import PreferenceStateMachineLogicEvaluator, _query_trace_id_to_length, MERGE_IGNORE_COLUMNS\n",
    "logic_evaluator = PreferenceStateMachineLogicEvaluator(_query_trace_id_to_length(stats.con))\n",
    "\n",
    "\n",
    "mapping = {'?v0': ['ball'], '?v1': ['cylindrical_block'], '?v2': ['cylindrical_block']}\n",
    "# trace_id = 'G2chsLHjitbS4fASq4vq-preCreateGame-rerecorded'\n",
    "dfs = []\n",
    "for pred in test_pred.or_args:\n",
    "    df = stats.filter(pred, mapping, return_full_result=True)\n",
    "    # df = df[df.trace_id == trace_id]\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "dfs = [logic_evaluator._preprocess_results(df) for df in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = None\n",
    "wildcard_indices = []\n",
    "merged_indices = []\n",
    "\n",
    "initial_intervals_index = None\n",
    "for i, result in enumerate(dfs):\n",
    "    if isinstance(result, pd.DataFrame):\n",
    "        if merged_df is None:\n",
    "            merged_df = result\n",
    "            initial_intervals_index = i\n",
    "        else:\n",
    "            merge_columns = ['trace_id']\n",
    "            merge_columns.extend(set(merged_df.columns) & set(result.columns) - MERGE_IGNORE_COLUMNS)\n",
    "            merged_df = merged_df.merge(result, on=merge_columns, how='inner', suffixes=('', f'_{i + 1}'))\n",
    "            merged_indices.append(i)\n",
    "    else:\n",
    "        wildcard_indices.append(i)\n",
    "\n",
    "merged_df.rename(columns={'intervals': f'intervals_{initial_intervals_index + 1}'}, inplace=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _state_machine_logic_apply(\n",
    "        row: pd.Series, modals: typing.List[str], trace_ids_found: typing.Set[str],\n",
    "        check_all_unique_objects_per_trace: bool = False):\n",
    "\n",
    "    if row.trace_id in trace_ids_found and not check_all_unique_objects_per_trace:\n",
    "        return True\n",
    "\n",
    "    index = 0\n",
    "    state = 0\n",
    "    interval_keys = sorted([key for key in row.keys() if key.startswith('intervals')])\n",
    "    final_state = len(interval_keys)\n",
    "    intervals = [row[key] for key in interval_keys]\n",
    "\n",
    "    try:\n",
    "        while index < logic_evaluator.trace_id_to_length[row.trace_id] and state < final_state:\n",
    "            next_valid = intervals[state][index] == '1'\n",
    "            if next_valid:\n",
    "                print(f'At index {index} of trace {row.trace_id}, moved to state {state + 1}')\n",
    "                state += 1\n",
    "\n",
    "            elif state > 0 and (state == 1 or modals[state - 1] == 'hold'):\n",
    "                current_valid = intervals[state - 1][index] == '1'\n",
    "                if not current_valid:\n",
    "                    print(f'At index {index} of trace {row.trace_id}, reset state to 0')\n",
    "                    state = 0\n",
    "\n",
    "            index += 1\n",
    "\n",
    "    except IndexError:\n",
    "        print(f'Trace id {row.trace_id} has length {logic_evaluator.trace_id_to_length[row.trace_id]}, but intervals {[len(i) for i in intervals]} for keys {interval_keys} are too short')\n",
    "        raise\n",
    "\n",
    "    result = state == final_state\n",
    "    if result and not check_all_unique_objects_per_trace:\n",
    "        trace_ids_found.add(row.trace_id)\n",
    "\n",
    "    return result\n",
    "\n",
    "for i, t in merged_df.iterrows():\n",
    "    rv = _state_machine_logic_apply(t, ['once', 'hold', 'once'], set())\n",
    "    if rv:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_ranges(string):\n",
    "    ranges = []\n",
    "    start = None\n",
    "    for i, char in enumerate(string):\n",
    "        if char == '1':\n",
    "            if start is None:\n",
    "                start = i\n",
    "        else:\n",
    "            if start is not None:\n",
    "                ranges.append((start, i))\n",
    "                start = None\n",
    "    if start is not None:\n",
    "        ranges.append((start, len(string)))\n",
    "    return ranges\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print(find_index_ranges(merged_df.iloc[0][f'intervals_{i + 1}']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "SELECT distinct(trace_id) FROM data\n",
    "WHERE predicate='in'\n",
    "AND arg_1_type='doggie_bed'\n",
    "AND arg_2_type='dodgeball'\n",
    "\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'?v0': ['cylindrical_block_green']}\n",
    "df1 = stats.filter(test_pred.or_args[0], mapping, return_full_result=True)\n",
    "len(df1[df1.trace_id == 'GLPtcvJUaHkUYK7iEPRq-gameplay-attempt-1-rerecorded'].intervals.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'?v0': ['cylindrical_block_green']}\n",
    "df2 = stats.filter(test_pred.or_args[1], mapping, return_full_result=True)\n",
    "len(df2[df2.trace_id == 'GLPtcvJUaHkUYK7iEPRq-gameplay-attempt-1-rerecorded'].intervals.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'?v0': ['cylindrical_block_green']}\n",
    "df3 = stats.filter(test_pred.or_args[2], mapping, return_full_result=True)\n",
    "len(df3[df3.trace_id == 'GLPtcvJUaHkUYK7iEPRq-gameplay-attempt-1-rerecorded'].intervals.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.trace_id == 'GLPtcvJUaHkUYK7iEPRq-gameplay-attempt-1-rerecorded'].intervals.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id_query = \"\"\"\n",
    "SELECT trace_id, length from trace_length_and_domains tld\n",
    "\"\"\".strip()\n",
    "\n",
    "trace_id_to_length = {trace_id: length for trace_id, length in stats.con.execute(trace_id_query).fetchall()}\n",
    "\n",
    "def intervals_to_strings_apply(row):\n",
    "    length = trace_id_to_length[row.trace_id]\n",
    "    return bin(int.from_bytes(row.intervals, 'big'))[-length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'?v2': ['cube_block']}\n",
    "\n",
    "first_df = stats.filter(test_pred.or_args[0], mapping, return_full_result=True).drop(columns=['domain'])\n",
    "first_df.intervals = first_df.apply(intervals_to_strings_apply, axis=1)\n",
    "\n",
    "second_df = stats.filter(test_pred.or_args[1], mapping, return_full_result=True).drop(columns=['domain'])\n",
    "second_df.intervals = second_df.apply(intervals_to_strings_apply, axis=1)\n",
    "\n",
    "ignore_columns = set(['trace_id', 'domain', 'intervals'])\n",
    "\n",
    "shared_variable_columns = set(first_df.columns) & set(second_df.columns) - ignore_columns\n",
    "shared_variable_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_columns = ['trace_id']\n",
    "join_columns.extend(shared_variable_columns)\n",
    "merged_df = first_df.merge(second_df, on=join_columns, how='inner', suffixes=('_1', '_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_machine(row, found):\n",
    "    if row.trace_id in found:\n",
    "        return True\n",
    "    \n",
    "    index = 0\n",
    "    state = 0\n",
    "    interval_keys = sorted([key for key in row.keys() if key.startswith('intervals')])\n",
    "    final_state = len(interval_keys)\n",
    "    intervals = [row[key] for key in interval_keys]\n",
    "\n",
    "    while index < trace_id_to_length[row.trace_id] and state != final_state:\n",
    "        next_valid = intervals[state][index] == '1'\n",
    "        if next_valid:\n",
    "            state += 1\n",
    "\n",
    "        elif state > 0:\n",
    "            # TODO: add a check here that the current modal is a hold or that the current state is 1\n",
    "            current_valid = intervals[state - 1][index] == '1'\n",
    "            if not current_valid:\n",
    "                state = 0\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    result = state == final_state\n",
    "    if result:\n",
    "        found.add(row.trace_id)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = set()\n",
    "merged_df[merged_df.apply(state_machine, axis=1, found=found)].trace_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_state_machine_logic(predicate_results: typing.List[typing.Union[s]]):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b'23897234802374092187492364329784632789'\n",
    "\n",
    "b2 = b'238972348023740921874923643297846327'\n",
    "\n",
    "int(bin(int.from_bytes(b, 'big'))[-40:], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id_query = \"\"\"\n",
    "SELECT trace_id, length from trace_length_and_domains tld\n",
    "\"\"\".strip()\n",
    "\n",
    "trace_id_to_length = {trace_id: length for trace_id, length in stats.con.execute(trace_id_query).fetchall()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id_to_length['YB9HWpd4rMWwS3P2mCVk-freePlay-rerecorded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ast = grammar_parser.parse(test_game_str)\n",
    "test_pred = test_ast[3][1].preferences[0].definition.pref_body.body.exists_args.at_end_pred.pred\n",
    "\n",
    "mapping = {'?r': ['dodgeball_red']}\n",
    "\n",
    "current_more_traces_permutations = []\n",
    "refactored_more_traces_permutations = []\n",
    "equal_permutations = []\n",
    "current_count_by_permutation = {}\n",
    "refactored_count_by_permutation = {}\n",
    "\n",
    "children_key = 'and_args' if 'and_args' in test_pred else 'or_args'\n",
    "original_children =  copy.deepcopy(test_pred[children_key])\n",
    "\n",
    "for n_children in range(1, len(original_children) + 1):\n",
    "    for combination in itertools.combinations(list(range(len(original_children))), n_children):\n",
    "        new_children = []\n",
    "        for i in combination:\n",
    "            new_children.append(original_children[i])\n",
    "\n",
    "        ast_utils.replace_child(test_pred, children_key, new_children)    \n",
    "\n",
    "        current_results = set(stats.filter(test_pred, mapping, return_trace_ids=True))\n",
    "\n",
    "        refactored_method = stats._handle_and_refactored if 'and_args' in test_pred else stats._handle_or_refactored\n",
    "        refactored_query = refactored_method(test_pred, mapping, return_trace_ids=True)[0]\n",
    "\n",
    "        try:\n",
    "            refactored_results = stats.con.execute(refactored_query).fetchall()\n",
    "            refactored_results = set(itertools.chain.from_iterable(refactored_results))\n",
    "        \n",
    "        except:\n",
    "            print(sqlparse.format(refactored_query, reindent=True, keyword_case='upper'))\n",
    "            raise\n",
    "\n",
    "        current_count_by_permutation[combination] = len(current_results)\n",
    "        refactored_count_by_permutation[combination] = len(refactored_results)\n",
    "\n",
    "        sym_diff = refactored_results.symmetric_difference(current_results)\n",
    "        if len(sym_diff) > 0:\n",
    "            if len(current_results - refactored_results) > 0:\n",
    "                current_more_traces_permutations.append(combination)\n",
    "            if len(refactored_results - current_results) > 0:\n",
    "                refactored_more_traces_permutations.append(combination)\n",
    "\n",
    "        else:\n",
    "            equal_permutations.append(combination)\n",
    "\n",
    "if equal_permutations:\n",
    "    print('equal combinations')\n",
    "    for permutation in equal_permutations:\n",
    "        print(permutation, current_count_by_permutation[permutation], refactored_count_by_permutation[permutation])\n",
    "\n",
    "    print()\n",
    "\n",
    "if current_more_traces_permutations:\n",
    "    print('current more combinations')\n",
    "    for permutation in current_more_traces_permutations:\n",
    "        print(permutation, current_count_by_permutation[permutation], refactored_count_by_permutation[permutation])\n",
    "\n",
    "    print()\n",
    "\n",
    "if refactored_more_traces_permutations:\n",
    "    print('refactored more combinations')\n",
    "    for permutation in refactored_more_traces_permutations:\n",
    "        print(permutation, current_count_by_permutation[permutation], refactored_count_by_permutation[permutation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_trace_ids = False\n",
    "\n",
    "mapping = {'?r': ['dodgeball_red']}\n",
    "current_results = stats.filter(test_pred, mapping, return_trace_ids=return_trace_ids)\n",
    "# df = stats.filter(test_pred, {'?v1': ['pillow'], '?v2': ['hexagonal_bin']}, last_interval_bit_set=True)\n",
    "# df = stats.filter(test_pred, {'?v1': ['building'], '?v2': ['flat_block'], '?v3': ['block']}, last_interval_bit_set=True)\n",
    "# df\n",
    "if isinstance(current_results, int):\n",
    "    print(current_results)\n",
    "else:\n",
    "    current_results = set(current_results)\n",
    "    print(len(current_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping = {'?d': ['dodgeball'], '?b': ['beachball'], '?t': ['teddy_bear']}\n",
    "refactored_method = stats._handle_and_refactored if 'and_args' in test_pred else stats._handle_or_refactored\n",
    "refactored_query = refactored_method(test_pred, mapping, return_trace_ids=return_trace_ids)[0]\n",
    "\n",
    "if not return_trace_ids:\n",
    "    refactored_query = f\"SELECT COUNT(*) FROM ({refactored_query} LIMIT 1);\"\n",
    "\n",
    "print(sqlparse.format(refactored_query, reindent=True, keyword_case='upper'))\n",
    "\n",
    "refactored_results = stats.con.execute(refactored_query).fetchall()\n",
    "\n",
    "if return_trace_ids:\n",
    "    refactored_results = set(itertools.chain.from_iterable(refactored_results))\n",
    "    print(len(refactored_results))\n",
    "    print(refactored_results.symmetric_difference(current_results))\n",
    "\n",
    "else:\n",
    "    refactored_results = refactored_results[0][0]\n",
    "    print(refactored_results, current_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "WITH predicate AS\n",
    "  (SELECT d0.trace_id,\n",
    "          d0.domain,\n",
    "          d0.intervals,\n",
    "          d0.arg_1_id AS \"?r\"\n",
    "   FROM DATA d0\n",
    "   WHERE d0.predicate='in_motion'\n",
    "     AND (d0.arg_1_type='dodgeball_red')),\n",
    "     negated_predicate AS\n",
    "  (SELECT predicates.*,\n",
    "          (COALESCE(intervals_d1, tld.intervals) | COALESCE(intervals_d2, tld.intervals)) AS intervals\n",
    "   FROM\n",
    "     (SELECT COALESCE(d1.trace_id, d2.trace_id) AS \"trace_id\",\n",
    "             COALESCE(d1.domain, d2.domain) AS \"domain\",\n",
    "             COALESCE(d1.arg_1_id, d2.arg_2_id) AS \"?r\",\n",
    "             d2.arg_1_id AS \"floor\",\n",
    "             d1.intervals AS intervals_d1,\n",
    "             d2.intervals AS intervals_d2\n",
    "      FROM\n",
    "        (SELECT d1.trace_id,\n",
    "                d1.domain,\n",
    "                d1.intervals,\n",
    "                d1.arg_1_id\n",
    "         FROM DATA d1\n",
    "         WHERE d1.predicate='agent_holds'\n",
    "           AND (d1.arg_1_type='dodgeball_red')) d1\n",
    "      FULL JOIN\n",
    "        (SELECT d2.trace_id,\n",
    "                d2.domain,\n",
    "                d2.intervals,\n",
    "                d2.arg_1_id,\n",
    "                d2.arg_2_id\n",
    "         FROM DATA d2\n",
    "         WHERE d2.predicate='touch'\n",
    "           AND (d2.arg_1_type='floor')\n",
    "           AND (d2.arg_2_type='dodgeball_red')) d2 ON (d1.trace_id = d2.trace_id\n",
    "                                                       AND (d1.arg_1_id = d2.arg_2_id))) predicates\n",
    "   JOIN trace_length_and_domains tld ON predicates.trace_id = tld.trace_id),\n",
    "     missing_negated_predicate AS\n",
    "  (SELECT predicate.trace_id AS trace_id,\n",
    "          o0.object_id AS \"?r\",\n",
    "          o1.object_id AS \"floor\"\n",
    "   FROM predicate\n",
    "   JOIN object_type_to_id o0 ON predicate.domain = o0.domain\n",
    "   AND o0.type = 'dodgeball_red'\n",
    "   AND o0.object_id = predicate.\"?r\"\n",
    "   JOIN object_type_to_id o1 ON predicate.domain = o1.domain\n",
    "   AND o1.type = 'floor'\n",
    "   WHERE NOT EXISTS\n",
    "       (SELECT 1\n",
    "        FROM negated_predicate\n",
    "        WHERE predicate.trace_id = negated_predicate.trace_id\n",
    "          AND (o0.object_id = negated_predicate.\"?r\"\n",
    "               OR negated_predicate.\"?r\" IS NULL)\n",
    "          AND o1.object_id = negated_predicate.\"floor\")\n",
    "     AND NOT EXISTS\n",
    "       (SELECT 1\n",
    "        FROM negated_predicate\n",
    "        WHERE predicate.trace_id = negated_predicate.trace_id\n",
    "          AND (o0.object_id = negated_predicate.\"?r\"\n",
    "               OR o1.object_id = negated_predicate.\"floor\")\n",
    "          AND bit_count(~negated_predicate.intervals) = 0))\n",
    "\n",
    "   \n",
    "SELECT DISTINCT(trace_id) \n",
    "FROM predicate\n",
    "WHERE bit_count(predicate.intervals) > 0 AND predicate.trace_id = 'SFpazSkgQ7MFSwDEa3c9-gameplay-attempt-1-rerecorded'\n",
    "  AND (EXISTS\n",
    "         (SELECT *\n",
    "          FROM negated_predicate\n",
    "          WHERE predicate.trace_id = negated_predicate.trace_id\n",
    "            AND (predicate.\"?r\" = negated_predicate.\"?r\"\n",
    "                 OR negated_predicate.\"?r\" IS NULL)\n",
    "            AND negated_predicate.\"floor\" IS NOT NULL\n",
    "            AND bit_count(predicate.intervals) > bit_count(predicate.intervals & negated_predicate.intervals) )\n",
    "       OR EXISTS\n",
    "         (SELECT *\n",
    "          FROM missing_negated_predicate\n",
    "          WHERE predicate.trace_id = missing_negated_predicate.trace_id )\n",
    ")\n",
    "LIMIT 1\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "WITH predicate AS\n",
    "  (SELECT d0.trace_id,\n",
    "          d0.domain,\n",
    "          d0.intervals,\n",
    "          d0.arg_1_id AS \"?d\"\n",
    "   FROM DATA d0\n",
    "   WHERE d0.predicate='in_motion'\n",
    "     AND (d0.arg_1_type='dodgeball')),\n",
    "     negated_predicate AS\n",
    "  (SELECT d1.trace_id,\n",
    "          d1.domain,\n",
    "          d1.intervals,\n",
    "          d1.arg_1_id AS \"?t\"\n",
    "   FROM DATA d1\n",
    "   WHERE d1.predicate='agent_holds'\n",
    "     AND (d1.arg_1_type='teddy_bear')),\n",
    "     missing_negated_predicate AS\n",
    "  (SELECT trace_length_and_domains.trace_id AS trace_id,\n",
    "          o0.object_id AS \"?t\",\n",
    "          o1.object_id AS \"?d\"\n",
    "   FROM trace_length_and_domains\n",
    "   JOIN object_type_to_id o0 ON trace_length_and_domains.domain = o0.domain\n",
    "   AND o0.type = 'teddy_bear'\n",
    "   JOIN object_type_to_id o1 ON trace_length_and_domains.domain = o1.domain\n",
    "   AND o1.type = 'dodgeball'\n",
    "   WHERE NOT EXISTS\n",
    "       (SELECT 1\n",
    "        FROM negated_predicate\n",
    "        WHERE trace_length_and_domains.trace_id = negated_predicate.trace_id\n",
    "          AND o0.object_id = negated_predicate.\"?t\"))\n",
    "  (SELECT DISTINCT(missing_negated_predicate.trace_id)\n",
    "   FROM missing_negated_predicate)\n",
    "UNION\n",
    "  (SELECT DISTINCT(trace_id)\n",
    "   FROM predicate\n",
    "   WHERE bit_count(predicate.intervals) > 0\n",
    "     AND (EXISTS\n",
    "            (SELECT 1\n",
    "             FROM negated_predicate\n",
    "             WHERE predicate.trace_id = negated_predicate.trace_id\n",
    "               AND negated_predicate.\"?t\" IS NOT NULL )\n",
    "          OR EXISTS\n",
    "            (SELECT 1\n",
    "             FROM missing_negated_predicate\n",
    "             WHERE predicate.trace_id = missing_negated_predicate.trace_id\n",
    "               AND missing_negated_predicate.\"?t\" IS NOT NULL )) )\n",
    "UNION\n",
    "  (SELECT DISTINCT(negated_predicate.trace_id)\n",
    "   FROM negated_predicate\n",
    "   WHERE bit_count(negated_predicate.intervals) > 0\n",
    "     AND negated_predicate.\"?t\" IS NOT NULL )\n",
    "\n",
    "\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "SELECT d0.trace_id,\n",
    "                d0.domain,\n",
    "                d0.intervals,\n",
    "                d0.arg_1_id\n",
    "FROM DATA d0\n",
    "WHERE d0.predicate='in_motion'\n",
    "  AND (d0.arg_1_type='dodgeball')\n",
    "  AND d0.trace_id = 'ShyRlIFgTWW6vqNrd3K5-createGame-rerecorded'\n",
    "\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "WITH d0 AS (SELECT d0.trace_id,\n",
    "          d0.domain,\n",
    "          d0.intervals,\n",
    "          d0.arg_1_id AS \"?r\"\n",
    "   FROM DATA d0\n",
    "   WHERE d0.predicate='in_motion'\n",
    "     AND (d0.arg_1_type='dodgeball_red')\n",
    "),\n",
    "d1 AS (SELECT d1.trace_id,\n",
    "                d1.domain,\n",
    "                d1.intervals,\n",
    "                d1.arg_1_id AS \"?r\"\n",
    "         FROM DATA d1\n",
    "         WHERE d1.predicate='agent_holds'\n",
    "           AND (d1.arg_1_type='dodgeball_red')\n",
    "), d2 AS (SELECT d2.trace_id,\n",
    "                d2.domain,\n",
    "                d2.intervals,\n",
    "                d2.arg_1_id,\n",
    "                d2.arg_2_id AS \"?r\"\n",
    "         FROM DATA d2\n",
    "         WHERE d2.predicate='touch'\n",
    "           AND (d2.arg_1_type='floor')\n",
    "           AND (d2.arg_2_type='dodgeball_red')\n",
    ") \n",
    "\n",
    "SELECT d0.trace_id, d0.intervals as intervals_0, d1.intervals as intervals_1, d2.intervals as intervals_2, bit_count(d0.intervals & (~d1.intervals))\n",
    "FROM d0\n",
    "LEFT JOIN d1 on d0.trace_id = d1.trace_id AND d0.\"?r\" = d1.\"?r\"\n",
    "LEFT JOIN d2 on d0.trace_id = d2.trace_id AND d0.\"?r\" = d2.\"?r\"\n",
    "WHERE d0.trace_id = 'SFpazSkgQ7MFSwDEa3c9-gameplay-attempt-1-rerecorded'\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "WITH predicate AS\n",
    "  (SELECT d0.trace_id,\n",
    "          d0.domain,\n",
    "          d0.intervals,\n",
    "          d0.arg_1_id AS \"agent\",\n",
    "          d0.arg_2_id AS \"bed\"\n",
    "   FROM DATA d0\n",
    "   WHERE d0.predicate='adjacent'\n",
    "     AND (d0.arg_1_type='agent')\n",
    "     AND (d0.arg_2_type='bed')),\n",
    "     negated_predicate AS\n",
    "  (SELECT d1.trace_id,\n",
    "          d1.domain,\n",
    "          d1.intervals,\n",
    "          d1.arg_1_id AS \"?v1\",\n",
    "          d1.arg_2_id AS \"?v2\"\n",
    "   FROM DATA d1\n",
    "   WHERE d1.predicate='touch'\n",
    "     AND (d1.arg_1_type='dodgeball')\n",
    "     AND (d1.arg_2_type='pillow')),\n",
    "     missing_negated_predicate AS\n",
    "  (SELECT predicate.trace_id AS trace_id,\n",
    "          o0.object_id AS \"?v1\",\n",
    "          o1.object_id AS \"bed\",\n",
    "          o2.object_id AS \"agent\",\n",
    "          o3.object_id AS \"?v2\"\n",
    "   FROM predicate\n",
    "   JOIN object_type_to_id o0 ON predicate.domain = o0.domain\n",
    "   AND o0.type = 'dodgeball'\n",
    "   JOIN object_type_to_id o1 ON predicate.domain = o1.domain\n",
    "   AND o1.type = 'bed'\n",
    "   AND o1.object_id = predicate.\"bed\"\n",
    "   JOIN object_type_to_id o2 ON predicate.domain = o2.domain\n",
    "   AND o2.type = 'agent'\n",
    "   AND o2.object_id = predicate.\"agent\"\n",
    "   JOIN object_type_to_id o3 ON predicate.domain = o3.domain\n",
    "   AND o3.type = 'pillow'\n",
    "   WHERE NOT EXISTS\n",
    "       (SELECT 1\n",
    "        FROM negated_predicate\n",
    "        WHERE predicate.trace_id = negated_predicate.trace_id\n",
    "          AND (o0.object_id = negated_predicate.\"?v1\"\n",
    "               OR negated_predicate.\"?v1\" IS NULL)\n",
    "          AND (o3.object_id = negated_predicate.\"?v2\"\n",
    "               OR negated_predicate.\"?v2\" IS NULL))\n",
    "     AND NOT EXISTS\n",
    "       (SELECT 1\n",
    "        FROM negated_predicate\n",
    "        WHERE predicate.trace_id = negated_predicate.trace_id\n",
    "          AND bit_count(~negated_predicate.intervals) = 0))\n",
    "SELECT DISTINCT(trace_id)\n",
    "FROM predicate\n",
    "WHERE bit_count(predicate.intervals) > 0\n",
    "  AND (EXISTS\n",
    "         (SELECT *\n",
    "          FROM negated_predicate\n",
    "          WHERE predicate.trace_id = negated_predicate.trace_id\n",
    "            AND\n",
    "            AND bit_count(predicate.intervals) > bit_count(predicate.intervals & negated_predicate.intervals) )\n",
    "       OR EXISTS\n",
    "         (SELECT *\n",
    "          FROM missing_negated_predicate\n",
    "          WHERE predicate.trace_id = missing_negated_predicate.trace_id ))\n",
    "\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "WITH t283 AS\n",
    "        (SELECT trace_length_and_domains.trace_id AS trace_id,\n",
    "                trace_length_and_domains.domain AS DOMAIN,\n",
    "                trace_length_and_domains.intervals AS intervals,\n",
    "                object_assignments.\"?v2\" AS \"?v2\",\n",
    "                object_assignments.\"bed\" AS \"bed\"\n",
    "         FROM trace_length_and_domains\n",
    "         JOIN\n",
    "           (WITH t281 AS\n",
    "              (SELECT DOMAIN,\n",
    "                      object_id AS \"?v2\"\n",
    "               FROM object_type_to_id\n",
    "               WHERE TYPE = 'pillow'),\n",
    "                 t282 AS\n",
    "              (SELECT DOMAIN,\n",
    "                      object_id AS \"bed\"\n",
    "               FROM object_type_to_id\n",
    "               WHERE TYPE = 'bed') SELECT t281.domain,\n",
    "                                          t281.\"?v2\" AS \"?v2\",\n",
    "                                          t282.\"bed\" AS \"bed\"\n",
    "            FROM t281\n",
    "            JOIN t282 ON (t281.domain = t282.domain)\n",
    "            AND (t281.\"?v2\" != t282.\"bed\")) AS object_assignments ON (trace_length_and_domains.domain = object_assignments.domain)),\n",
    "           t284 AS\n",
    "        (SELECT trace_id,\n",
    "                DOMAIN,\n",
    "                intervals,\n",
    "                arg_1_id AS \"bed\",\n",
    "                arg_2_id AS \"?v2\"\n",
    "         FROM DATA\n",
    "         WHERE predicate='on'\n",
    "           AND (arg_1_type='bed')\n",
    "           AND (arg_2_type='pillow')) \n",
    "SELECT t283.trace_id AS trace_id,\n",
    "        t283.domain AS DOMAIN,\n",
    "        t283.\"?v2\" AS \"?v2\",\n",
    "        t283.\"bed\" AS \"bed\",\n",
    "        (~(t283.intervals | COALESCE(t284.intervals, t283.intervals))) AS intervals,\n",
    "        bit_count(~(t283.intervals | COALESCE(t284.intervals, t283.intervals))) AS bit_count\n",
    "FROM t283\n",
    "LEFT JOIN t284 ON t283.\"trace_id\"=t284.\"trace_id\"\n",
    "AND t283.\"?v2\"=t284.\"?v2\"\n",
    "AND t283.\"bed\"=t284.\"bed\"\n",
    "WHERE t283.trace_id = '1El1CmicSoKZKTLe8NpP-gameplay-attempt-1-rerecorded'\n",
    "\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "WITH t274 AS\n",
    "     (WITH t268 AS\n",
    "        (WITH t266 AS\n",
    "           (SELECT trace_length_and_domains.trace_id AS trace_id,\n",
    "                   trace_length_and_domains.domain AS DOMAIN,\n",
    "                   trace_length_and_domains.intervals AS intervals,\n",
    "                   object_assignments.\"?v1\" AS \"?v1\",\n",
    "                   object_assignments.\"?v2\" AS \"?v2\"\n",
    "            FROM trace_length_and_domains\n",
    "            JOIN\n",
    "              (WITH t264 AS\n",
    "                 (SELECT DOMAIN,\n",
    "                         object_id AS \"?v1\"\n",
    "                  FROM object_type_to_id\n",
    "                  WHERE TYPE = 'dodgeball'),\n",
    "                    t265 AS\n",
    "                 (SELECT DOMAIN,\n",
    "                         object_id AS \"?v2\"\n",
    "                  FROM object_type_to_id\n",
    "                  WHERE TYPE = 'pillow') SELECT t264.domain,\n",
    "                                                t264.\"?v1\" AS \"?v1\",\n",
    "                                                t265.\"?v2\" AS \"?v2\"\n",
    "               FROM t264\n",
    "               JOIN t265 ON (t264.domain = t265.domain)\n",
    "               AND (t264.\"?v1\" != t265.\"?v2\")) AS object_assignments ON (trace_length_and_domains.domain = object_assignments.domain)),\n",
    "              t267 AS\n",
    "           (SELECT trace_id,\n",
    "                   DOMAIN,\n",
    "                   intervals,\n",
    "                   arg_1_id AS \"?v1\",\n",
    "                   arg_2_id AS \"?v2\"\n",
    "            FROM DATA\n",
    "            WHERE predicate='touch'\n",
    "              AND (arg_1_type='dodgeball')\n",
    "              AND (arg_2_type='pillow')) SELECT t266.trace_id AS trace_id,\n",
    "                                                t266.domain AS DOMAIN,\n",
    "                                                t266.\"?v1\" AS \"?v1\",\n",
    "                                                t266.\"?v2\" AS \"?v2\",\n",
    "                                                (~(t266.intervals | COALESCE(t267.intervals, t266.intervals))) AS intervals\n",
    "         FROM t266\n",
    "         LEFT JOIN t267 ON t266.\"trace_id\"=t267.\"trace_id\"\n",
    "         AND t266.\"?v1\"=t267.\"?v1\"\n",
    "         AND t266.\"?v2\"=t267.\"?v2\") SELECT *\n",
    "      FROM t268\n",
    "      WHERE bit_count(intervals) != 0),\n",
    "        t275 AS\n",
    "     (WITH t273 AS\n",
    "        (WITH t271 AS\n",
    "           (SELECT trace_length_and_domains.trace_id AS trace_id,\n",
    "                   trace_length_and_domains.domain AS DOMAIN,\n",
    "                   trace_length_and_domains.intervals AS intervals,\n",
    "                   object_assignments.\"?v2\" AS \"?v2\",\n",
    "                   object_assignments.\"bed\" AS \"bed\"\n",
    "            FROM trace_length_and_domains\n",
    "            JOIN\n",
    "              (WITH t269 AS\n",
    "                 (SELECT DOMAIN,\n",
    "                         object_id AS \"?v2\"\n",
    "                  FROM object_type_to_id\n",
    "                  WHERE TYPE = 'pillow'),\n",
    "                    t270 AS\n",
    "                 (SELECT DOMAIN,\n",
    "                         object_id AS \"bed\"\n",
    "                  FROM object_type_to_id\n",
    "                  WHERE TYPE = 'bed') SELECT t269.domain,\n",
    "                                             t269.\"?v2\" AS \"?v2\",\n",
    "                                             t270.\"bed\" AS \"bed\"\n",
    "               FROM t269\n",
    "               JOIN t270 ON (t269.domain = t270.domain)\n",
    "               AND (t269.\"?v2\" != t270.\"bed\")) AS object_assignments ON (trace_length_and_domains.domain = object_assignments.domain)),\n",
    "              t272 AS\n",
    "           (SELECT trace_id,\n",
    "                   DOMAIN,\n",
    "                   intervals,\n",
    "                   arg_1_id AS \"bed\",\n",
    "                   arg_2_id AS \"?v2\"\n",
    "            FROM DATA\n",
    "            WHERE predicate='on'\n",
    "              AND (arg_1_type='bed')\n",
    "              AND (arg_2_type='pillow')) SELECT t271.trace_id AS trace_id,\n",
    "                                                t271.domain AS DOMAIN,\n",
    "                                                t271.\"?v2\" AS \"?v2\",\n",
    "                                                t271.\"bed\" AS \"bed\",\n",
    "                                                (~(t271.intervals | COALESCE(t272.intervals, t271.intervals))) AS intervals\n",
    "         FROM t271\n",
    "         LEFT JOIN t272 ON t271.\"trace_id\"=t272.\"trace_id\"\n",
    "         AND t271.\"?v2\"=t272.\"?v2\"\n",
    "         AND t271.\"bed\"=t272.\"bed\") SELECT *\n",
    "      FROM t273) \n",
    "      \n",
    "\n",
    "SELECT t274.trace_id,\n",
    "      t274.domain,\n",
    "      t274.\"?v1\",\n",
    "      t274.\"?v2\",\n",
    "      t275.\"bed\",\n",
    "      (t274.intervals | t275.intervals) AS intervals,\n",
    "      bit_count(t274.intervals | t275.intervals) AS bit_count\n",
    "FROM t274\n",
    "LEFT JOIN t275 ON (t274.trace_id=t275.trace_id)\n",
    "WHERE t274.trace_id = '1aTng0m9240WEAU975WE-gameplay-attempt-2-rerecorded'\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckdb.sql(\"\"\"SELECT * FROM data WHERE predicate='touch' AND arg_1_is_game_object IS TRUE AND arg_2_is_game_object IS TRUE\"\"\").df()\n",
    "new_trace_ids = stats.con.execute(\n",
    "\"\"\"\n",
    "WITH positive_predicate AS (\n",
    "    SELECT\n",
    "            trace_id,\n",
    "            domain,\n",
    "            intervals,\n",
    "            arg_1_id AS \"bed\",\n",
    "            arg_2_id AS \"?v1\"\n",
    "    FROM\n",
    "            data\n",
    "    WHERE\n",
    "            predicate = 'on'\n",
    "            AND (arg_1_type = 'bed')\n",
    "            AND (arg_2_type = 'dodgeball')\n",
    "),\n",
    "negative_predicate AS (\n",
    "    SELECT\n",
    "            trace_id,\n",
    "            domain,\n",
    "            intervals,\n",
    "            arg_1_id AS \"?v1\",\n",
    "            arg_2_id AS \"?v2\"\n",
    "    FROM\n",
    "            data\n",
    "    WHERE\n",
    "            predicate = 'touch'\n",
    "            AND (arg_1_type = 'dodgeball')\n",
    "            AND (arg_2_type = 'pillow')\n",
    "\n",
    "),\n",
    "missing_negative_predicates AS (\n",
    "    SELECT \n",
    "            positive_predicate.trace_id as trace_id,\n",
    "    FROM\n",
    "            positive_predicate\n",
    "    JOIN \n",
    "            object_type_to_id o1\n",
    "    ON\n",
    "            positive_predicate.domain = o1.domain\n",
    "            AND o1.type='dodgeball' \n",
    "            AND o1.object_id = positive_predicate.\"?v1\"\n",
    "    JOIN \n",
    "            object_type_to_id o2\n",
    "    ON\n",
    "            positive_predicate.domain = o2.domain\n",
    "            AND o2.type='pillow'\n",
    "    JOIN \n",
    "            negative_predicate    \n",
    "    ON \n",
    "            positive_predicate.trace_id = negative_predicate.trace_id\n",
    "            AND o1.object_id != negative_predicate.\"?v1\"\n",
    "            AND o2.object_id != negative_predicate.\"?v2\"\n",
    ")\n",
    "SELECT DISTINCT(trace_id) FROM positive_predicate\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT * FROM negative_predicate\n",
    "    WHERE\n",
    "            positive_predicate.trace_id = negative_predicate.trace_id\n",
    "            AND positive_predicate.\"?v1\" = negative_predicate.\"?v1\"\n",
    "            AND bit_count(positive_predicate.intervals) = bit_count(positive_predicate.intervals & negative_predicate.intervals)\n",
    ") OR EXISTS (\n",
    "    SELECT * FROM missing_negative_predicates\n",
    "    WHERE\n",
    "            positive_predicate.trace_id = missing_negative_predicates.trace_id\n",
    ")\n",
    "\n",
    "\n",
    ";\n",
    "\"\"\").fetchall()\n",
    "\n",
    "new_trace_ids = set(itertools.chain.from_iterable(new_trace_ids))\n",
    "print(len(new_trace_ids))\n",
    "current_results.symmetric_difference(new_trace_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT count(*) from trace_length_and_domains\n",
    "\"\"\".strip()\n",
    "\n",
    "stats.con.execute(refactored_query).df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT * FROM (\n",
    "SELECT\n",
    "            trace_id, \n",
    "            count(trace_id) as trace_count,\n",
    "            domain\n",
    "            /*\n",
    "            domain,\n",
    "            intervals,\n",
    "            arg_1_id AS \"bed\",\n",
    "            arg_2_id AS \"?v1\"\n",
    "            */\n",
    "    FROM\n",
    "            data\n",
    "    WHERE\n",
    "            predicate = 'touch'\n",
    "            AND (arg_1_type = 'pillow')\n",
    "            AND (arg_2_type = 'dodgeball')\n",
    "        GROUP BY trace_id, domain\n",
    ") WHERE \n",
    "(domain = 'few' AND trace_count = 2) OR \n",
    "(domain = 'medium' AND trace_count = 1) OR\n",
    "(domain = 'many' AND trace_count = 3)\n",
    "\"\"\".strip()\n",
    "\n",
    "for t in stats.con.execute(refactored_query).df().trace_id:\n",
    "    print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_neg = \"\"\"\n",
    "SELECT\n",
    "            trace_id,\n",
    "            domain,\n",
    "            intervals,\n",
    "            arg_1_id AS \"?v1\",\n",
    "    FROM\n",
    "            data\n",
    "    WHERE\n",
    "            predicate = 'agent_crouches'\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "stats.con.execute(q_neg).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_missing_neg = \"\"\"\n",
    "WITH positive_predicate AS (\n",
    "    SELECT\n",
    "            trace_id,\n",
    "            domain,\n",
    "            intervals,\n",
    "            arg_1_id AS \"bed\",\n",
    "            arg_2_id AS \"?v1\"\n",
    "    FROM\n",
    "            data\n",
    "    WHERE\n",
    "            predicate = 'on'\n",
    "            AND (arg_1_type = 'bed')\n",
    "            AND (arg_2_type = 'dodgeball')\n",
    "            AND (trace_id = '4WUtnD8W6PGVy0WBtVm4-editGame-rerecorded')\n",
    "),\n",
    "negative_predicate AS (\n",
    "    SELECT\n",
    "            trace_id,\n",
    "            domain,\n",
    "            intervals,\n",
    "            arg_1_id AS \"?v1\",\n",
    "    FROM\n",
    "            data\n",
    "    WHERE\n",
    "            predicate = 'touch'\n",
    "            AND (arg_1_type = 'dodgeball')\n",
    "            AND (arg_2_type = 'pillow')\n",
    "            AND (trace_id = '4WUtnD8W6PGVy0WBtVm4-editGame-rerecorded')\n",
    "\n",
    "),\n",
    "missing_negative_predicates AS (\n",
    "    SELECT \n",
    "            positive_predicate.trace_id as trace_id,\n",
    "            object_type_to_id.object_id as \"?v1\"\n",
    "    FROM\n",
    "            positive_predicate\n",
    "    JOIN \n",
    "            object_type_to_id\n",
    "    ON\n",
    "            positive_predicate.domain = object_type_to_id.domain\n",
    "            AND object_type_to_id.type='pillow'\n",
    "    JOIN \n",
    "            negative_predicate    \n",
    "    ON \n",
    "            positive_predicate.trace_id = negative_predicate.trace_id\n",
    "            AND object_type_to_id.object_id != negative_predicate.\"?v1\"\n",
    ")\n",
    "SELECT * FROM missing_negative_predicates \n",
    "WHERE (trace_id = '4WUtnD8W6PGVy0WBtVm4-editGame-rerecorded')\n",
    "\"\"\"\n",
    "\n",
    "stats.con.execute(q_missing_neg).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_joint = \"\"\"\n",
    "WITH positive_predicate AS (\n",
    "    SELECT\n",
    "            trace_id,\n",
    "            domain,\n",
    "            intervals,\n",
    "            arg_1_id AS \"bed\",\n",
    "            arg_2_id AS \"?v1\"\n",
    "    FROM\n",
    "            data\n",
    "    WHERE\n",
    "            predicate = 'on'\n",
    "            AND (arg_1_type = 'bed')\n",
    "            AND (arg_2_type = 'dodgeball')\n",
    "            AND (trace_id = '4WUtnD8W6PGVy0WBtVm4-editGame-rerecorded')\n",
    "),\n",
    "negative_predicate AS (\n",
    "    SELECT\n",
    "            trace_id,\n",
    "            domain,\n",
    "            intervals,\n",
    "            arg_1_id AS \"?v1\",\n",
    "    FROM\n",
    "            data\n",
    "    WHERE\n",
    "            predicate = 'touch'\n",
    "            AND (arg_1_type = 'dodgeball')\n",
    "            AND (arg_2_type = 'pillow')\n",
    "            AND (trace_id = '4WUtnD8W6PGVy0WBtVm4-editGame-rerecorded')\n",
    ")\n",
    "SELECT bit_count(positive_predicate.intervals & (~ negative_predicate.intervals)) \n",
    "FROM positive_predicate\n",
    "JOIN negative_predicate\n",
    "ON positive_predicate.trace_id = negative_predicate.trace_id\n",
    "AND positive_predicate.\"?v1\" = negative_predicate.\"?v1\"\n",
    "\"\"\"\n",
    "\n",
    "stats.con.execute(q_joint).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_trace_ids = stats.con.execute(\"\"\"WITH t10 AS (\n",
    "        WITH t8 AS (\n",
    "                SELECT\n",
    "                        trace_id,\n",
    "                        domain,\n",
    "                        intervals,\n",
    "                        arg_1_id AS \"bed\",\n",
    "                        arg_2_id AS \"?v1\"\n",
    "                FROM\n",
    "                        data\n",
    "                WHERE\n",
    "                        predicate = 'on'\n",
    "                        AND (arg_1_type = 'bed')\n",
    "                        AND (arg_2_type = 'dodgeball')\n",
    "        ),\n",
    "        t9 AS (\n",
    "                WITH t7 AS (\n",
    "                        WITH t5 AS (\n",
    "                                SELECT\n",
    "                                        trace_length_and_domains.trace_id AS trace_id,\n",
    "                                        trace_length_and_domains.domain AS domain,\n",
    "                                        trace_length_and_domains.intervals AS intervals,\n",
    "                                        object_assignments.\"?v2\" as \"?v2\",\n",
    "                                        object_assignments.\"?v1\" as \"?v1\"\n",
    "                                FROM\n",
    "                                        trace_length_and_domains\n",
    "                                        JOIN (\n",
    "                                                WITH t3 AS (\n",
    "                                                        SELECT\n",
    "                                                                domain,\n",
    "                                                                object_id AS \"?v2\"\n",
    "                                                        FROM\n",
    "                                                                object_type_to_id\n",
    "                                                        WHERE\n",
    "                                                                type = 'pillow'\n",
    "                                                ),\n",
    "                                                t4 AS (\n",
    "                                                        SELECT\n",
    "                                                                domain,\n",
    "                                                                object_id AS \"?v1\"\n",
    "                                                        FROM\n",
    "                                                                object_type_to_id\n",
    "                                                        WHERE\n",
    "                                                                type = 'dodgeball'\n",
    "                                                )\n",
    "                                                SELECT\n",
    "                                                        t3.domain,\n",
    "                                                        t3.\"?v2\" AS \"?v2\",\n",
    "                                                        t4.\"?v1\" AS \"?v1\"\n",
    "                                                FROM\n",
    "                                                        t3\n",
    "                                                        JOIN t4 ON (t3.domain = t4.domain)\n",
    "                                                        AND (t3.\"?v2\" != t4.\"?v1\")\n",
    "                                        ) AS object_assignments ON (\n",
    "                                                trace_length_and_domains.domain = object_assignments.domain\n",
    "                                        )\n",
    "                        ),\n",
    "                        t6 AS (\n",
    "                                SELECT\n",
    "                                        trace_id,\n",
    "                                        domain,\n",
    "                                        intervals,\n",
    "                                        arg_1_id AS \"?v1\",\n",
    "                                        arg_2_id AS \"?v2\"\n",
    "                                FROM\n",
    "                                        data\n",
    "                                WHERE\n",
    "                                        predicate = 'touch'\n",
    "                                        AND (arg_1_type = 'dodgeball')\n",
    "                                        AND (arg_2_type = 'pillow')\n",
    "                        )\n",
    "                        SELECT\n",
    "                                t5.trace_id as trace_id,\n",
    "                                t5.domain as domain,\n",
    "                                t5.\"?v2\" as \"?v2\",\n",
    "                                t5.\"?v1\" as \"?v1\",\n",
    "                                (\n",
    "                                        ~(\n",
    "                                                t5.intervals | COALESCE(t6.intervals, t5.intervals)\n",
    "                                        )\n",
    "                                ) AS intervals\n",
    "                        FROM\n",
    "                                t5\n",
    "                                LEFT JOIN t6 ON t5.\"trace_id\" = t6.\"trace_id\"\n",
    "                                AND t5.\"?v2\" = t6.\"?v2\"\n",
    "                                AND t5.\"?v1\" = t6.\"?v1\"\n",
    "                )\n",
    "                SELECT\n",
    "                        *\n",
    "                FROM\n",
    "                        t7\n",
    "                WHERE\n",
    "                        bit_count(intervals) != 0\n",
    "        )\n",
    "        SELECT\n",
    "                t8.trace_id,\n",
    "                t8.domain,\n",
    "                t8.\"bed\",\n",
    "                t8.\"?v1\",\n",
    "                t9.\"?v2\",\n",
    "                (t8.intervals & t9.intervals) AS intervals\n",
    "        FROM\n",
    "                t8\n",
    "                INNER JOIN t9 ON (t8.trace_id = t9.trace_id)\n",
    "                AND (t9.\"?v1\" = t8.\"?v1\")\n",
    ")\n",
    "SELECT\n",
    "        DISTINCT trace_id\n",
    "FROM\n",
    "        t10\n",
    "WHERE\n",
    "        bit_count(intervals) != 0;\"\"\").fetchall()\n",
    "\n",
    "old_trace_ids = set(itertools.chain.from_iterable(old_trace_ids))\n",
    "old_trace_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trace_ids - old_trace_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "SELECT * FROM DATA\n",
    "WHERE trace_id='4WUtnD8W6PGVy0WBtVm4-editGame-rerecorded' AND predicate='on' AND (arg_1_type = 'bed') AND (arg_2_type = 'dodgeball')\n",
    "                  \n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "SELECT * FROM DATA\n",
    "WHERE trace_id='4WUtnD8W6PGVy0WBtVm4-editGame-rerecorded' AND predicate='touch' AND (arg_1_type = 'dodgeball') AND (arg_2_type = 'pillow')\n",
    "                  \n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "WITH ball_touch_pillow AS (\n",
    "SELECT * FROM DATA\n",
    "WHERE trace_id='4WUtnD8W6PGVy0WBtVm4-editGame-rerecorded' AND predicate='touch' AND (arg_1_type = 'dodgeball') AND (arg_2_type = 'pillow')\n",
    "),\n",
    "ball_on_bed AS (\n",
    "SELECT * FROM DATA\n",
    "WHERE trace_id='4WUtnD8W6PGVy0WBtVm4-editGame-rerecorded' AND predicate='on' AND (arg_1_type = 'bed') AND (arg_2_type = 'dodgeball')\n",
    ")\n",
    "SELECT bit_count(ball_on_bed.intervals & (~ball_touch_pillow.intervals)) FROM ball_on_bed\n",
    "JOIN ball_touch_pillow\n",
    "ON ball_on_bed.arg_2_id = ball_touch_pillow.arg_1_id\n",
    "                  \n",
    "\"\"\").fetchdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.con.execute(\n",
    "\"\"\"\n",
    "SELECT * FROM object_type_to_id\n",
    "WHERE type='pillow'             \n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT SUM(LENGTH(intervals)) FROM data WHERE predicate not in ('equal_x_position', 'equal_z_position');\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=':memory:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SHOW TABLES;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT trace_id, arg_1_id, arg_2_id, overlap, d1_count, d2_count\n",
    "FROM (WITH \n",
    "d1 AS (SELECT trace_id, arg_1_id, arg_1_type, arg_2_id, arg_2_type, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building'), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building')\n",
    "SELECT d1.trace_id, d1.arg_1_id, d1.arg_1_type, d1.arg_2_id, d1.arg_2_type, bit_count(d1.intervals & d2.intervals) as overlap, bit_count(d1.intervals) as d1_count, bit_count(d2.intervals) as d2_count\n",
    "FROM d1\n",
    "INNER JOIN d2 ON d1.trace_id = d2.trace_id AND d1.arg_1_id = d2.arg_2_id AND d1.arg_2_id = d2.arg_1_id)\n",
    "WHERE overlap > 0\n",
    "\"\"\"\n",
    "reciprocal_on_df = duckdb.sql(refactored_query).df()  # .to_csv('temp_outputs/a_on_b_and_b_on_a.csv')\n",
    "reciprocal_on_df\n",
    "# reciprocal_on_df = reciprocal_on_df.assign(remove_d1=reciprocal_on_df.d1_count <= reciprocal_on_df.d2_count)\n",
    "# reciprocal_on_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('CREATE INDEX idx_data_arg_1_type ON data (arg_1_type)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_on_rows = bitstrings_df.predicate == 'on'\n",
    "indices_to_remove = []\n",
    "\n",
    "for _, row in reciprocal_on_df.iterrows():\n",
    "    row_filter = predicate_on_rows & (bitstrings_df.trace_id == row.trace_id)\n",
    "    if row.remove_d1:\n",
    "        row_filter &= (bitstrings_df.arg_1_id == row.arg_1_id) & (bitstrings_df.arg_2_id == row.arg_2_id)\n",
    "\n",
    "    else:\n",
    "        row_filter &= (bitstrings_df.arg_1_id == row.arg_2_id) & (bitstrings_df.arg_2_id == row.arg_1_id)\n",
    "\n",
    "    indices_to_remove.extend(bitstrings_df[row_filter].index)\n",
    "\n",
    "\n",
    "print(indices_to_remove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(indices_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_bitstrings_df = bitstrings_df.drop(index=indices_to_remove)\n",
    "# filtered_bitstrings_df = filtered_bitstrings_df.reset_index(drop=True)\n",
    "\n",
    "# print(filtered_bitstrings_df.shape, bitstrings_df.shape, filtered_bitstrings_df.index.max())\n",
    "\n",
    "# filtered_bitstrings_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT trace_id, arg_1_id, arg_2_id, overlap  \n",
    "FROM (WITH \n",
    "d1 AS (SELECT trace_id, arg_1_id, arg_1_type, intervals FROM data WHERE predicate='agent_holds'), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='adjacent' AND (arg_1_type='agent' OR arg_2_type='agent'))\n",
    "SELECT d2.trace_id, d2.arg_1_id, d2.arg_2_id, bit_count(d1.intervals & d2.intervals) as overlap\n",
    "FROM d2\n",
    "INNER JOIN d1 ON d1.trace_id = d2.trace_id AND (d1.arg_1_id = d2.arg_1_id OR d1.arg_1_id = d2.arg_2_id)\n",
    ")\n",
    "WHERE overlap > 0\n",
    "\"\"\"\n",
    "duckdb.sql(refactored_query).df()  # .to_csv('temp_outputs/a_on_b_and_b_on_a.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT count(*) FROM data\n",
    "WHERE predicate='agent_holds'\n",
    "\"\"\"\n",
    "duckdb.sql(refactored_query)  # .df()  # .to_csv('temp_outputs/a_on_b_and_b_on_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_arg_types = duckdb.sql(\"SELECT arg_types.* FROM (SELECT DISTINCT(arg_1_type, arg_2_type) as arg_types FROM data WHERE predicate='in');\").fetchall()\n",
    "in_arg_types = [tuple(x) for x in in_arg_types]\n",
    "first_arg_types, second_arg_types = zip(*in_arg_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in in_arg_types if t[0] == 'mug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_arg_types = duckdb.sql(\"SELECT arg_types.* FROM (SELECT DISTINCT(arg_1_type, arg_2_type) as arg_types FROM data WHERE predicate='on');\").fetchall()\n",
    "on_arg_types = [tuple(x) for x in on_arg_types]\n",
    "on_first_arg_types, on_second_arg_types = zip(*on_arg_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in on_arg_types if t[1] == 'desk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import room_and_object_types\n",
    "\n",
    "on_types_by_category = defaultdict(set)\n",
    "for t in on_first_arg_types:\n",
    "    on_types_by_category[room_and_object_types.TYPES_TO_CATEGORIES[t]].add(t)\n",
    "\n",
    "\n",
    "for cat in on_types_by_category:\n",
    "    print(cat)\n",
    "    print(on_types_by_category[cat])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT * FROM (WITH \n",
    "d1 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building'), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building')\n",
    "SELECT d1.trace_id, d1.arg_1_id, d1.arg_2_id, bit_count(d1.intervals & d2.intervals) as overlap\n",
    "FROM d1\n",
    "INNER JOIN d2 ON d1.trace_id = d2.trace_id AND d1.arg_1_id = d2.arg_2_id AND d1.arg_2_id = d2.arg_1_id)\n",
    "WHERE overlap > 100\n",
    "\"\"\"\n",
    "duckdb.sql(refactored_query).df().to_csv('temp_outputs/a_on_b_and_b_on_a.csv')\n",
    "\n",
    "# duckdb.sql(\"SELECT * FROM data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT * FROM (WITH \n",
    "d1 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building'), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='in' AND arg_1_type!='building' AND arg_2_type!='building')\n",
    "SELECT d1.trace_id, d1.arg_1_id, d1.arg_2_id, bit_count(d1.intervals & d2.intervals) as overlap\n",
    "FROM d1\n",
    "INNER JOIN d2 ON d1.trace_id = d2.trace_id AND d1.arg_1_id = d2.arg_2_id AND d1.arg_2_id = d2.arg_1_id)\n",
    "WHERE overlap > 100\n",
    "\"\"\"\n",
    "duckdb.sql(refactored_query).df().to_csv('temp_outputs/a_on_b_and_b_in_a.csv')\n",
    "\n",
    "# duckdb.sql(\"SELECT * FROM data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT trace_id, domain, arg_1_id, arg_2_id, bit_position('1'::BIT, joint_intervals) as \"first_index\" FROM (WITH \n",
    "d1 AS (SELECT trace_id, domain, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND (arg_1_type='building' OR arg_2_type='building')), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND (arg_1_type='building' OR arg_2_type='building'))\n",
    "SELECT d1.trace_id, d1.domain, d1.arg_1_id, d1.arg_2_id, d1.intervals & d2.intervals as joint_intervals\n",
    "FROM d1\n",
    "INNER JOIN d2 ON d1.trace_id = d2.trace_id AND d1.arg_1_id = d2.arg_2_id AND d1.arg_2_id = d2.arg_1_id)\n",
    "WHERE bit_count(joint_intervals) > 100\n",
    "\"\"\"\n",
    "buildings_df = duckdb.sql(refactored_query).df().to_csv('temp_outputs/a_on_b_and_b_on_a_buildings.csv')\n",
    "buildings_df\n",
    "# buildings_df[(buildings_df.trace_id == 'Q6a8AbiIdcLA9tJzAu14-createGame-rerecorded') & (buildings_df.arg_2_id == 'SmallSlide|-00.81|+00.14|-03.10')]\n",
    "\n",
    "# duckdb.sql(\"SELECT * FROM data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT * FROM data\n",
    "WHERE trace_id='Q6a8AbiIdcLA9tJzAu14-createGame-rerecorded' AND arg_1_id='building_1' and arg_2_id='SmallSlide|-00.81|+00.14|-03.10' AND predicate='on'\n",
    "\"\"\"\n",
    "\n",
    "d = duckdb.sql(refactored_query).df()\n",
    "print(d.loc[0, 'intervals'] == d.loc[1, 'intervals'])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = \"\"\"\n",
    "SELECT trace_id, domain, arg_1_id, arg_2_id, bit_position('1'::BIT, intervals) as 'first_index' FROM data\n",
    "WHERE predicate='on' and arg_2_type in ('bed', 'desk') and arg_1_type NOT IN ('floor', 'rug')\n",
    "\"\"\"\n",
    "duckdb.sql(refactored_query).df().to_csv('temp_outputs/bed_or_desk_on_object_that_is_not_floor_or_rug.csv')\n",
    "\n",
    "# duckdb.sql(\"SELECT * FROM data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = game_asts[1][4][1].preferences[0].definition.forall_pref.preferences.pref_body.body.then_funcs[0].seq_func.once_pred\n",
    "print(p.keys())\n",
    "ast_printer.ast_section_to_string(p, ast_parser.PREFERENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_query = stats.filter(p, {\"?b\": [\"ball\"], \"?t\": [\"hexagonal_bin\"]})\n",
    "print(refactored_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('PRAGMA force_index_join;')\n",
    "duckdb.sql(\"PRAGMA explain_output='OPTIMIZED_ONLY';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = \"\"\"\n",
    "#     SELECT t0.trace_id, t0.domain, t0.\"?b\", t1.\"door\", t1.\"agent\", (t0.intervals & t1.intervals) AS intervals\n",
    "#     FROM (SELECT trace_id, domain, intervals, arg_1_id AS \"?b\" FROM data WHERE predicate='agent_holds' AND (arg_1_type IN ('beachball'::arg_type, 'basketball'::arg_type, 'dodgeball'::arg_type, 'golfball'::arg_type))) as t0\n",
    "#     INNER JOIN (SELECT trace_id, domain, intervals, arg_1_id AS \"door\", arg_2_id AS \"agent\" FROM data WHERE predicate='adjacent' AND (arg_1_type='door') AND (arg_2_type='agent')) as t1\n",
    "#     ON (t0.trace_id=t1.trace_id)\n",
    "# \"\"\"\n",
    "\n",
    "refactored_query = \"\"\"\n",
    "    SELECT t0.trace_id, t0.domain, t0.arg_1_id AS \"?b\", t1.arg_1_id AS \"door\", t1.arg_2_id AS \"agent\", (t0.intervals & t1.intervals) AS intervals\n",
    "    FROM data AS t0\n",
    "    INNER JOIN data AS t1\n",
    "    ON (t0.trace_id=t1.trace_id)\n",
    "    WHERE t0.predicate='agent_holds' AND (t0.arg_1_type IN ('beachball'::arg_type, 'basketball'::arg_type, 'dodgeball'::arg_type, 'golfball'::arg_type)) AND\n",
    "    t1.predicate='adjacent' AND (t1.arg_1_type='door') AND (t1.arg_2_type='agent')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(duckdb.sql(f\"EXPLAIN ANALYZE ({refactored_query})\").fetchone()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"INSERT INTO domains VALUES ('few'), ('medium'), ('many')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckdb.sql(refactored_query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('CREATE INDEX idx_data_predicate ON data (predicate)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros(10, dtype=np.uint8)\n",
    "t[1:4] = 1\n",
    "b = t.tobytes()\n",
    "d = {b[0]: '0', b[1]: '1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.trace_id == '1El1CmicSoKZKTLe8NpP-preCreateGame-rerecorded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_from_df = np.unpackbits(np.frombuffer(interval_from_df, dtype=np.uint8))\n",
    "len(bits_from_df), bits_from_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = duckdb.sql(refactored_query).fetchall()\n",
    "\n",
    "for tup in results:\n",
    "    sub_df = df[(tup[0] == df['trace_id']) & (tup[2] == df['?b']) & (tup[3] == df['?t'])]\n",
    "    if len(sub_df) != 1:\n",
    "        print(f'Error: {tup[:-1]}')\n",
    "        print(len(sub_df))\n",
    "        break\n",
    "\n",
    "    expected_length = duckdb.sql(f\"SELECT length from trace_length_and_domains WHERE trace_id='{tup[0]}'\").fetchone()[0]\n",
    "\n",
    "    bits_from_df = np.unpackbits(np.frombuffer(sub_df.intervals.item(), dtype=np.uint8))\n",
    "    bits_from_db = np.fromiter(map(int, tup[-1]), dtype=np.uint8)\n",
    "\n",
    "    if len(bits_from_db) != expected_length:\n",
    "        print(f'Error: {tup[:-1]}')\n",
    "        print(len(bits_from_db))\n",
    "        print(expected_length)\n",
    "        print(len(bits_from_df))\n",
    "        break\n",
    "\n",
    "    if not np.all(bits_from_df[-expected_length:] == bits_from_db):\n",
    "        print(f'Error: {tup[:-1]}')\n",
    "        print(np.where(bits_from_df[-expected_length:] != bits_from_db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bits_from_df[-550:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_from_db = np.fromiter(map(int, t[-1]), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_from_df = np.unpackbits(np.frombuffer(df[df.trace_id == t[0]].intervals.item(), dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bits_from_db), len(bits_from_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1264 % 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bits_from_db), len(bits_from_df))\n",
    "np.where(bits_from_db[:] != bits_from_df[14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_from_db[500:540], bits_from_df[500:540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bits_from_db[:min_length] != bits_from_df[:min_length])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gameplan from here\n",
    "\n",
    "* For (setup, each preference):\n",
    "* Inspect to make sure all predicates implemeneted in the cache, it doesn't use a once-measure (and probably also not a hold-while, at least at first), etc. \n",
    "    * otherwise default to the basic implementation\n",
    "* If we're go to run on a particular thing:\n",
    "    * If it's a setup, it's trivial\n",
    "    * If it's a preference, check if it's an at-end or then\n",
    "        * If it's an at-end, it's trivial (can probably fold this into the query by doing `get_bit(intervals, length - 1)` if we join on the trace lengths table\n",
    "        * If it's a then, enumerate over the predicates of each modal, and query for them\n",
    "            * For each one, fetch the df for the query, use the trace lengths to transform the intervals to the expected format\n",
    "            * Add the index of the modal to the df\n",
    "            * Enumerate through all trace ids and assignments, and for each assignment where we have all modals represented: \n",
    "                * Create the joint state interval\n",
    "                * Iterate through the joint using the state machine logic\n",
    "                * Count satisfactions\n",
    "                * ...\n",
    "                * Profit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tatsu.ast\n",
    "import tatsu.grammars\n",
    "from ast_parser import ASTParser, SECTION_CONTEXT_KEY, VARIABLES_CONTEXT_KEY\n",
    "from ast_utils import simplified_context_deepcopy, deepcopy_ast, ASTCopyType, replace_child\n",
    "\n",
    "\n",
    "DEFAULT_UNSUPPORTED_RULES = [\n",
    "    'function_comparison',\n",
    "    'function_eval',\n",
    "    'predicate_adjacent_side_3',\n",
    "    'predicate_adjacent_side_4',\n",
    "    'predicate_between',\n",
    "    'predicate_faces',\n",
    "    'predicate_is_setup_object',\n",
    "    'predicate_opposite',\n",
    "    'predicate_rug_color_under',\n",
    "    'predicate_same_color',\n",
    "    'predicate_same_object',\n",
    "    'predicate_same_type',\n",
    "    'super_predicate_exists',\n",
    "    'super_predicate_forall',\n",
    "    'once_measure',\n",
    "    'while_hold',\n",
    "]\n",
    "\n",
    "\n",
    "def _pref_forall_pos_to_key(pos: int):\n",
    "    return f'pref_forall_{pos}'\n",
    "\n",
    "\n",
    "class MixedTraceFilterGameParser(ASTParser):\n",
    "    unsupported_rules: typing.Set[str]\n",
    "\n",
    "    def __init__(self, unsupported_rules: typing.Sequence[str] = DEFAULT_UNSUPPORTED_RULES):\n",
    "        super().__init__()\n",
    "        self.expected_keys = set()\n",
    "        self.unsupported_rules = set(unsupported_rules)\n",
    "\n",
    "    def __call__(self, ast, **kwargs):\n",
    "        initial_call = 'inner_call' not in kwargs or not kwargs['inner_call']\n",
    "        if initial_call:\n",
    "            kwargs['inner_call'] = True\n",
    "            kwargs['local_context'] = {'mapping': {VARIABLES_CONTEXT_KEY: {}}}\n",
    "            kwargs['global_context'] = {}\n",
    "            self.expected_keys = set()\n",
    "            self.unsupported_keys = set()\n",
    "            # self.traces_by_preference_or_section = {}\n",
    "            # self.preferences_or_sections_with_implemented_predicates = set()\n",
    "            # self.predicate_strings_by_preference_or_section = defaultdict(set)\n",
    "            # self.not_implemented_predicate_counts = defaultdict(int)\n",
    "\n",
    "        retval = super().__call__(ast, **kwargs)\n",
    "\n",
    "        if initial_call:\n",
    "            return self.unsupported_keys, self.expected_keys\n",
    "        else:\n",
    "            return retval\n",
    "\n",
    "    def _current_ast_to_contexts_hook(self, ast: tatsu.ast.AST, kwargs: typing.Dict[str, typing.Any]):\n",
    "        rule = typing.cast(str, ast.parseinfo.rule)  # type: ignore\n",
    "\n",
    "        if rule == 'pref_forall':\n",
    "            kwargs['local_context']['current_pref_forall_index'] = ast.parseinfo.pos\n",
    "\n",
    "        if rule == 'preference':\n",
    "            kwargs['local_context']['current_preference_name'] = ast.pref_name\n",
    "\n",
    "    def _handle_ast(self, ast: tatsu.ast.AST, **kwargs):\n",
    "        self._current_ast_to_contexts(ast, **kwargs)\n",
    "        kwargs['local_context']['mapping'] = ast_parser.update_context_variables(ast, kwargs['local_context']['mapping'])\n",
    "\n",
    "        current_key = None\n",
    "        if SECTION_CONTEXT_KEY in kwargs and kwargs[SECTION_CONTEXT_KEY] == ast_parser.SETUP:\n",
    "            current_key = kwargs[SECTION_CONTEXT_KEY]\n",
    "        elif 'current_pref_forall_index' in kwargs['local_context']:\n",
    "            current_key =_pref_forall_pos_to_key(kwargs['local_context']['current_pref_forall_index'])\n",
    "        elif 'current_preference_name' in kwargs['local_context']:\n",
    "            current_key = kwargs['local_context']['current_preference_name']\n",
    "        \n",
    "        if current_key is not None:\n",
    "            self.expected_keys.add(current_key)\n",
    "\n",
    "            if ast.parseinfo.rule in self.unsupported_rules:\n",
    "                self.unsupported_keys.add(current_key)\n",
    "\n",
    "        for key in ast:\n",
    "            if key != 'parseinfo':\n",
    "                child_kwargs = simplified_context_deepcopy(kwargs)\n",
    "                retval = self(ast[key], **child_kwargs)\n",
    "                self._update_contexts_from_retval(kwargs, retval)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_parser = MixedTraceFilterGameParser()\n",
    "for ast in game_asts:\n",
    "    unsupported, expected = game_parser(ast)\n",
    "    supported = expected - unsupported\n",
    "    print(f'Game {ast[1].game_name} has supported keys: {list(supported)} and unsupported keys: {list(unsupported)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "DUMMY_PREFERENCE_GAME = \"\"\"(define (game dummy-preference-game) (:domain many-objects-room-v1)\n",
    "(:constraints (and\n",
    "    (preference dummyPreference\n",
    "            (at-end (game-over))\n",
    "    )\n",
    "))\n",
    "(:scoring (count dummyPreference)\n",
    "))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ASTTraceFilterSplitter(ast_parser.ASTParser):\n",
    "    keep_keys: typing.Set[str]\n",
    "    remove_keys: typing.Set[str]\n",
    "    should_insert_dummy_preference: bool\n",
    "    \n",
    "    def __init__(self, grammar_parser: tatsu.grammars.Grammar):\n",
    "        self.grammar_parser = grammar_parser\n",
    "\n",
    "    def __call__(self, ast, **kwargs):\n",
    "        initial_call = 'inner_call' not in kwargs or not kwargs['inner_call']\n",
    "        if initial_call:\n",
    "            kwargs['inner_call'] = True\n",
    "            \n",
    "            if 'remove_keys' not in kwargs:\n",
    "                raise ValueError('remove_keys must be specified')\n",
    "            self.remove_keys = kwargs['remove_keys']\n",
    "\n",
    "            if len(self.remove_keys) == 0:\n",
    "                raise ValueError('remove_keys must be non-empty')\n",
    "\n",
    "            if 'keep_keys' not in kwargs:\n",
    "                raise ValueError('keep_keys must be specified')\n",
    "            self.keep_keys = kwargs['keep_keys']\n",
    "\n",
    "            if len(self.keep_keys) == 0:\n",
    "                raise ValueError('keep_keys must be non-empty')\n",
    "\n",
    "            ast = deepcopy_ast(ast)\n",
    "\n",
    "            # Handle the setup right here and now, if we're removing it\n",
    "            if ast_parser.SETUP in self.remove_keys:\n",
    "                ast = (*ast[:3], *ast[4:])\n",
    "                # If the only thin we're removing is the setup, we're done\n",
    "                if len(self.remove_keys) == 1:\n",
    "                    return ast\n",
    "\n",
    "            # check if we're only keeping the setup and inserting a dummy preference, because if so, we're done\n",
    "            if len(self.keep_keys) == 1 and ast_parser.SETUP in self.keep_keys:\n",
    "                dummy_preference_game = self.grammar_parser.parse(DUMMY_PREFERENCE_GAME)\n",
    "                return (*ast[:4], dummy_preference_game[3], *ast[4:])\n",
    "\n",
    "        super().__call__(ast, **kwargs)\n",
    "\n",
    "        if initial_call:\n",
    "            return ast\n",
    "        \n",
    "    def _handle_ast(self, ast: tatsu.ast.AST, **kwargs):\n",
    "        rule = ast.parseinfo.rule\n",
    "\n",
    "        if rule == 'preferences':\n",
    "            if isinstance(ast.preferences, tatsu.ast.AST):\n",
    "                raise ValueError(f'If removing a single preference, the initial call should handle it, so this should never occur')\n",
    "            \n",
    "            new_children = typing.cast(typing.List[tatsu.ast.AST], deepcopy_ast(ast.preferences, ASTCopyType.NODE))\n",
    "            indices_to_remove = []\n",
    "            for i, child in enumerate(new_children):\n",
    "                if child.parseinfo.rule == 'preference' and child.pref_name in self.remove_keys:\n",
    "                    print(f'Removing preference {child.pref_name}')\n",
    "                    indices_to_remove.append(i)\n",
    "                elif child.parseinfo.rule == 'pref_forall' and _pref_forall_pos_to_key(child.parseinfo.pos) in self.remove_keys:\n",
    "                    print(f'Removing pref_forall {_pref_forall_pos_to_key(child.parseinfo.pos)}')\n",
    "                    indices_to_remove.append(i)\n",
    "\n",
    "            for i in reversed(indices_to_remove):\n",
    "                new_children.remove(new_children[i])\n",
    "\n",
    "            replace_child(ast, 'preferences', new_children)\n",
    "\n",
    "        else:\n",
    "            for key in ast:\n",
    "                if key != 'parseinfo':\n",
    "                    self(ast[key], **kwargs)\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_parser = MixedTraceFilterGameParser()\n",
    "game_splitter = ASTTraceFilterSplitter(grammar_parser)  # type: ignore\n",
    "ast = game_asts[0]\n",
    "unsupported, expected = game_parser(ast)\n",
    "supported = expected - unsupported\n",
    "\n",
    "if len(supported) > 0 and len(unsupported) > 0:\n",
    "    print(f'Game {ast[1].game_name} has supported keys: {list(supported)} and unsupported keys: {list(unsupported)}')\n",
    "    supported_only = game_splitter(ast, keep_keys=supported, remove_keys=unsupported)\n",
    "    unsupported_only = game_splitter(ast, keep_keys=unsupported, remove_keys=supported)\n",
    "\n",
    "    print('=' * 80)\n",
    "    print(ast_printer.ast_to_string(supported_only, '\\n'))\n",
    "    print('=' * 80)\n",
    "    print(ast_printer.ast_to_string(unsupported_only, '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "categorical_type = pd.api.types.CategoricalDtype(sorted(chain.from_iterable(duckdb.sql(\"SELECT enum_range(NULL::trace_id)\").fetchone())), ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id_to_length_df = duckdb.sql('SELECT * FROM trace_length_and_domains').fetchdf()\n",
    "trace_id_to_length_df.drop(columns=['domain'], inplace=True)\n",
    "trace_id_to_length_df.rename(columns=dict(length='trace_length'), inplace=True)\n",
    "# trace_id_to_length_df.astype(dict(trace_id=categorical_type), copy=False)\n",
    "trace_id_to_length_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_intervals_to_array(row):\n",
    "    return np.unpackbits(np.frombuffer(row['intervals'], dtype=np.uint8))[-row['trace_length']:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype(dict(trace_id=categorical_type), copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.join(trace_id_to_length_df, on=['trace_id'], how='outer', rsuffix='_r')\n",
    "merged_df = df.merge(trace_id_to_length_df, on=['trace_id'], how='left')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_df = df.assign(intervals=merged_df.apply(_df_intervals_to_array, axis=1))\n",
    "assigned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigned_df.groupby('trace_id', as_index=True, observed=False).intervals.transform(lambda l: list(np.logical_or.reduce(*l)))\n",
    "\n",
    "series_1 = assigned_df.groupby('trace_id', as_index=True, observed=True).intervals.agg(lambda x: reduce(np.logical_or, x.values).astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_2 = series_1.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(series_1, series_2, left_index=True, right_index=True, how='inner')\n",
    "s = merged.agg(lambda row: np.logical_and(row['intervals_x'], row['intervals_y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
