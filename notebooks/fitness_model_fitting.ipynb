{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import gzip\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tabulate\n",
    "import tatsu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src import fitness_energy_utils as utils\n",
    "from src.fitness_energy_utils import NON_FEATURE_COLUMNS\n",
    "from src.ast_counter_sampler import *\n",
    "from src.ast_utils import cached_load_and_parse_games_from_file, load_games_from_file, _extract_game_id\n",
    "from src import ast_printer\n",
    "from src.fitness_features_preprocessing import NGRAM_SCORE_PATTERN\n",
    "from src.fitness_features_by_category import *\n",
    "\n",
    "import tqdm.notebook as tqdmn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='..'))\n",
    "# real_game_texts = [ast_printer.ast_to_string(ast, '\\n') for ast in game_asts]\n",
    "# regrown_game_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples.pddl'))\n",
    "\n",
    "# regrown_game_asts = list(cached_load_and_parse_games_from_file('../dsl/ast-real-regrowth-samples-1024.pddl', grammar_parser, True, relative_path='..'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df = utils.load_fitness_data('../data/fitness_features_1024_regrowths.csv.gz')\n",
    "print(fitness_df.src_file.unique())\n",
    "print(fitness_df.shape)\n",
    "original_game_counts = fitness_df.groupby('original_game_name').src_file.count().value_counts()\n",
    "if len(original_game_counts) == 1:\n",
    "    print(f'All original games have {original_game_counts.index[0] - 1} regrowths')\n",
    "else:\n",
    "    print('Some original games have different numbers of regrowths: {original_game_counts}')\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "relevant_columns = [c for c in fitness_df.columns if c not in NON_FEATURE_COLUMNS and not any(f'_n_{n}_' in c for n in range(1, 5))]\n",
    "\n",
    "g = fitness_df.groupby('real')[relevant_columns].mean()\n",
    "mean_diffs = g.loc[1] - g.loc[0]\n",
    "abs_mean_diffs = mean_diffs.abs()\n",
    "\n",
    "interaction_diffs = []\n",
    "\n",
    "for first_col, second_col in tqdm.tqdm(itertools.combinations(relevant_columns, 2), total=len(relevant_columns) * (len(relevant_columns) - 1) / 2):\n",
    "    interaction = fitness_df[first_col] * fitness_df[second_col]\n",
    "    interaction_real_mean = interaction[fitness_df.real == 1].mean()\n",
    "    interaction_regrown_mean = interaction[fitness_df.real == 0].mean()\n",
    "    interaction_mean_abs_diff = np.abs(interaction_real_mean - interaction_regrown_mean)\n",
    "    max_individual_diff = max(abs_mean_diffs[first_col], abs_mean_diffs[second_col])  # type: ignore\n",
    "    diff_in_diffs = interaction_mean_abs_diff - max_individual_diff\n",
    "\n",
    "    if diff_in_diffs > 0:\n",
    "        interaction_diffs.append((first_col, second_col,  interaction_mean_abs_diff / (max_individual_diff + epsilon), diff_in_diffs, interaction_mean_abs_diff))\n",
    "\n",
    "\n",
    "interaction_diffs.sort(key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-2\n",
    "threshold_diffs = [x for x in interaction_diffs if x[3] > threshold]\n",
    "\n",
    "print(tabulate.tabulate(threshold_diffs[:100], headers=['First Term', 'Second Term', 'Interaction/Max Individual Ratio', 'Interaction - Max Individual Difference', 'Absolute Interaction Difference'], tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_categories = [\"forall_less_important\", \"counting_less_important\", \"grammar_use_less_important\", \"predicate_under_modal\", \"predicate_role_filler\", \"compositionality\"]\n",
    "feature_columns = [c for c in fitness_df.columns if c not in NON_FEATURE_COLUMNS]\n",
    "all_ignore_features = set()\n",
    "\n",
    "for category in ignore_categories:\n",
    "    for feature in FEATURE_CATEGORIES[category]:\n",
    "        if isinstance(feature, re.Pattern):\n",
    "            all_ignore_features.update([f for f in feature_columns if feature.match(f)])\n",
    "        else:\n",
    "            all_ignore_features.add(feature)\n",
    "\n",
    "filtered_zero_mean_features = [c for c in zero_mean_features if c not in all_ignore_features]\n",
    "\n",
    "print(filtered_zero_mean_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = fitness_df.groupby('real')[[c for c in fitness_df.columns if c not in ('Index', 'real')]].mean()\n",
    "mean_diffs = g.loc[1] - g.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_g = g[[c for c in fitness_df.columns if 'setup_objects' in c]]\n",
    "min_g.loc[1] - min_g.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diffs[[c for c in fitness_df.columns if c in ('adjacent_once_found', 'no_adjacent_same_modal', 'starts_and_ends_once', 'once_in_middle_of_pref_found', 'pref_without_hold_found')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asm = fitness_df.groupby('real').no_adjacent_same_modal.value_counts() / fitness_df.groupby('real').no_adjacent_same_modal.count()\n",
    "asm[1] - asm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "logger.debug('Features with largest negative diffs:\\n' + str(mean_diffs.nsmallest(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVES_FILE = 'interactive-beta.pddl'\n",
    "NEGATIVES_FILE = 'ast-real-regrowth-samples-1024.pddl'\n",
    "\n",
    "def create_filtered_df(df: pd.DataFrame, \n",
    "    filter_data_src_files: typing.Sequence[str] = (POSITIVES_FILE, NEGATIVES_FILE),\n",
    "    ) -> pd.DataFrame:\n",
    "    f_df = fitness_df[fitness_df.src_file.isin(filter_data_src_files)].reset_index(drop=True)\n",
    "    f_df.loc[f_df.src_file == filter_data_src_files[0], 'real'] = 1\n",
    "    return f_df\n",
    "\n",
    "filtered_fitness_df = create_filtered_df(fitness_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_fitness_df.shape)\n",
    "filtered_fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_fitness_df[(filtered_fitness_df.real == 1) & (filtered_fitness_df.two_number_operation_found == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latest_model_paths import LATEST_FITNESS_FEATURIZER_PATH, LATEST_FITNESS_FUNCTION_DATE_ID\n",
    "from src.fitness_features import *\n",
    "\n",
    "def _load_pickle_gzip(path: str):\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "fitness_featurizer = _load_pickle_gzip(LATEST_FITNESS_FEATURIZER_PATH)\n",
    "fitnes_function, feature_names = utils.load_model_and_feature_columns(LATEST_FITNESS_FUNCTION_DATE_ID)\n",
    "real_game_feafure_dicts = [fitness_featurizer.parse(ast, return_row=True) for ast in game_asts]\n",
    "real_game_feature_lists = [[fd[name] for name in feature_names] for fd in real_game_feafure_dicts]\n",
    "real_game_feature_vectors = [np.array(fl, dtype=float) for fl in real_game_feature_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_N_DIM = 32\n",
    "SEED = 100\n",
    "PCA_KWARGS = dict(random_state=SEED)\n",
    "TSNE_KAWRGS = dict(init='pca', learning_rate='auto', random_state=SEED)\n",
    "\n",
    "def pca_and_tsne(data: np.ndarray, pca_n_dim: int = PCA_N_DIM, \n",
    "    pca_kwargs: typing.Optional[typing.Dict] = None, tsne_kwargs: typing.Optional[typing.Dict] = None):\n",
    "\n",
    "    if pca_kwargs is None:\n",
    "        pca_kwargs = PCA_KWARGS\n",
    "    else:\n",
    "        temp_kwargs = PCA_KWARGS.copy()\n",
    "        temp_kwargs.update(pca_kwargs)\n",
    "        pca_kwargs = temp_kwargs\n",
    "\n",
    "    if tsne_kwargs is None:\n",
    "        tsne_kwargs = TSNE_KAWRGS\n",
    "\n",
    "    else:\n",
    "        temp_kwrags = TSNE_KAWRGS.copy()\n",
    "        temp_kwrags.update(tsne_kwargs)\n",
    "        tsne_kwargs = temp_kwrags\n",
    "\n",
    "    if data.ndim > 2:\n",
    "        data = data.reshape(-1, data.shape[-1])\n",
    "\n",
    "    pca = PCA(n_components=pca_n_dim, **pca_kwargs)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    tsne = TSNE(n_components=1, **tsne_kwargs)\n",
    "    data_tsne = tsne.fit_transform(data_pca)\n",
    "\n",
    "    return data_tsne\n",
    "\n",
    "\n",
    "tsne_results = pca_and_tsne(np.array(real_game_feature_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_throwing = np.array([ True,  True, False,  True,  True,  True,  True,  True, False,\n",
    "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
    "        True,  True,  True, False,  True, False,  True,  True, False,\n",
    "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
    "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
    "       False,  True,  True, False, False,  True,  True,  True, False,\n",
    "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
    "        True,  True,  True, False,  True,  True,  True, False])\n",
    "\n",
    "\n",
    "game_types = np.array(['throwing', 'throwing', 'building', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'building', 'throwing',\n",
    "       'throwing', 'throwing', 'building_throwing', 'throwing',\n",
    "       'building', 'throwing', 'building_throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'organizing', 'throwing',\n",
    "       'organizing', 'throwing', 'building_throwing', 'organizing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'organizing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'building_organizing_throwing', 'throwing',\n",
    "       'organizing', 'throwing', 'throwing', 'organizing', 'building',\n",
    "       'throwing', 'throwing', 'organizing', 'building', 'throwing',\n",
    "       'building_throwing', 'throwing', 'building', 'throwing',\n",
    "       'organizing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'organizing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', '', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'building', 'throwing', 'throwing',\n",
    "       'throwing', 'organizing'], dtype=object)\n",
    "\n",
    "unique_game_types = set(game_types)\n",
    "game_types_code = {t: i for i, t in enumerate(unique_game_types)}\n",
    "game_types_list = [game_types_code[t] for t in game_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_type in unique_game_types:\n",
    "    indices = np.where(game_types == game_type)[0]\n",
    "    if not game_type:\n",
    "        game_type = 'uncategorized'\n",
    "    if game_type == 'building_organizing_throwing':\n",
    "        game_type = 'all'\n",
    "    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 0], label=game_type)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx, max_idx = tsne_results.argmin(), tsne_results.argmax()\n",
    "print(min_idx, max_idx)\n",
    "print()\n",
    "print(ast_printer.ast_to_string(game_asts[min_idx], '\\n'))\n",
    "print()\n",
    "print(ast_printer.ast_to_string(game_asts[max_idx], '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_games = filtered_fitness_df[filtered_fitness_df.src_file == NEGATIVES_FILE].reset_index()\n",
    "broadcasted_original = filtered_fitness_df.loc[[filtered_fitness_df.index[(filtered_fitness_df.game_name == original_name)][0] for original_name in fake_games.original_game_name], :].reset_index()\n",
    "\n",
    "original_regrown_diffs = (broadcasted_original.drop(NON_FEATURE_COLUMNS, axis=1) - fake_games.drop(NON_FEATURE_COLUMNS, axis=1))\n",
    "\n",
    "unchanged_games_prop = (original_regrown_diffs.drop('index', axis=1) == 0).all(axis=1).sum() / len(original_regrown_diffs)\n",
    "print(f'In {unchanged_games_prop * 100:.2f}% of the games, the regrown game was identical to the original game.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [str(c) for c in fitness_df.columns if c not in NON_FEATURE_COLUMNS]\n",
    "\n",
    "remove_all_ngram_scores = []  #  ('full',)\n",
    "for score_type in ('full', 'setup', 'constraints', 'terminal', 'scoring'):\n",
    "    col_names = [c for c in feature_columns if c.startswith(f'ast_ngram_{score_type}') and c.endswith('_score')]\n",
    "\n",
    "    if score_type not in remove_all_ngram_scores:\n",
    "        col_names = col_names[:-1]\n",
    "\n",
    "    for col in col_names:\n",
    "        feature_columns.remove(col)\n",
    "\n",
    "other_features = ['all_variables_defined', 'all_variables_used',\n",
    "    'starts_and_ends_once',  # 'setup_objects_used',\n",
    "    'all_preferences_used', 'no_adjacent_same_modal', 'adjacent_once_found',\n",
    "    'repeated_variables_found', 'nested_logicals_found', 'identical_logical_children_found', \n",
    "    'no_two_number_operations', 'tautological_expression_found', 'redundant_expression_found',]\n",
    "\n",
    "# Next up: compositionality_structure_, max_depth, mean_depth_, node_count_, predicate_under_modal_, max_number_variables_types_quantified_, max_quantification_count_, _arg_types_, length_of_then_modals_\n",
    "prefixes = ['section_', 'pref_forall_', 'compositionality_structure_', 'max_depth_', 'mean_depth_']\n",
    "\n",
    "feature_columns = [c for c in feature_columns if 'score' in c or any(c.startswith(prefix) for prefix in prefixes) or c in other_features]\n",
    "feature_columns_set = set(feature_columns)\n",
    "\n",
    "fake_games = filtered_fitness_df[filtered_fitness_df.src_file == NEGATIVES_FILE].reset_index()\n",
    "broadcasted_original = filtered_fitness_df.loc[[filtered_fitness_df.index[(filtered_fitness_df.game_name == original_name)][0] for original_name in fake_games.original_game_name], :].reset_index()\n",
    "\n",
    "original_regrown_diffs = (broadcasted_original.drop([c for c in broadcasted_original.columns if c not in feature_columns_set], axis=1) - fake_games.drop([c for c in fake_games.columns if c not in feature_columns_set], axis=1))\n",
    "\n",
    "if 'index' in original_regrown_diffs.columns:\n",
    "    original_regrown_diffs = original_regrown_diffs.drop('index', axis=1)\n",
    "\n",
    "unchanged_games_prop = (original_regrown_diffs == 0).all(axis=1).sum() / len(original_regrown_diffs)\n",
    "print(f'In {unchanged_games_prop * 100:.2f}% of the games, the regrown game was identical to the original game.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_change = (original_regrown_diffs.drop('index', axis=1) == 0).all(axis=0)\n",
    "for x in never_change.index[never_change]:\n",
    "    print(x)\n",
    "\n",
    "print(sum(never_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features_by_real = filtered_fitness_df[['real'] + [c for c in filtered_fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic model-fitting experiment approach\n",
    "We have a large dataset now, I can try to cross-validate over some of the choices I might make:\n",
    "* Change the random seed?\n",
    "* See if the GPU is faster\n",
    "* Try a different from of regularization?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_by_abs_diff_threshold(diffs: pd.Series, score_threshold: float):\n",
    "    feature_columns = list(diffs[diffs >= score_threshold].index)\n",
    "\n",
    "    remove_all_ngram_scores = []  \n",
    "    for score_type in ('full', 'setup', 'constraints', 'terminal', 'scoring'):\n",
    "        col_names = sorted([c for c in feature_columns if c.startswith(f'ast_ngram_{score_type}') and c.endswith('_score')])\n",
    "\n",
    "        if score_type not in remove_all_ngram_scores:\n",
    "            col_names = col_names[:-1]\n",
    "\n",
    "        for col in col_names:\n",
    "            feature_columns.remove(col)\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 1.0\n",
    "\n",
    "test_param_grid = [\n",
    "    {\n",
    "        'fitness__loss_function': [utils.fitness_softmin_loss, utils.fitness_softmin_loss_positive_to_all_negatives], # [utils.fitness_hinge_loss_with_cross_example],\n",
    "        # 'fitness__weight_decay': [0.0],  \n",
    "        # 'fitness__margin': [8, 16],\n",
    "        'fitness__beta': [BETA], #   [4, 8],\n",
    "        'fitness__lr': [4e-3, 1e-3, 3e-4],  #  [1e-2, 3e-3],  # [1e-1, 3e-2, 1e-2, 3e-3],\n",
    "        'fitness__k': [256, 512, 1024],  # [256, 512, 1024],  # 128\n",
    "        'fitness__batch_size': [1, 2, 4, 8, 16]  # , 16],  # [1, 4, 8, 16],\n",
    "        # 'fitness__alpha': [0, 0.25, 0.5, 0.75, 1], # [0, 0.1, 0.2, 0.3],  #\n",
    "        # 'fitness__dataset_energy_beta': [1, 3, 5],\n",
    "        # 'fitness__regularization_weight': [0.01, 0.05],\n",
    "    },\n",
    "]\n",
    "\n",
    "def build_regularization_function(ord: int = 1, threshold: float = 0):\n",
    "    def regularization_function(model: nn.Module) -> torch.Tensor:\n",
    "        w = model.fc1.weight.squeeze()  # type: ignore\n",
    "        return torch.linalg.norm(w * (w.abs() >= threshold), ord)\n",
    "    \n",
    "    return regularization_function\n",
    "\n",
    "regularizer = build_regularization_function(ord=1, threshold=0)\n",
    "\n",
    "scaler_kwargs = dict(passthrough=True)\n",
    "model_kwargs = dict(output_activation=nn.Identity())\n",
    "train_kwargs = dict(\n",
    "    negative_score_reduction='none', \n",
    "    n_epochs=20000, patience_epochs=200, \n",
    "    bias_init_margin_ratio=0.01,\n",
    "    device=torch.device('cuda:0'), \n",
    "    # regularizer=regularizer,\n",
    "    shuffle_negatives=True, \n",
    "    split_validation_from_train=True,\n",
    "    evaluate_opposite_shuffle_mode=False,\n",
    "    full_dataset_on_device=True,\n",
    "    # use_lr_scheduler=True,\n",
    "    )\n",
    "cv_kwargs = dict(refit='loss', error_score='raise')  # , n_jobs=6)  # , n_jobs=1)\n",
    "scoring = utils.build_multiple_scoring_function(\n",
    "    [utils.wrap_loss_function_to_metric(utils.fitness_sofmin_loss_positive_negative_split, dict(beta=BETA), True),  # type: ignore\n",
    "     utils.evaluate_fitness_overall_ecdf, utils.evaluate_fitness_single_game_rank, utils.evaluate_fitness_single_game_min_rank, \n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.01), True),  # type: ignore\n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.05), True),  # type: ignore\n",
    "     ],\n",
    "    ['loss', 'overall_ecdf', 'single_game_rank', 'single_game_min_rank', 'energy_of_negative@1%', 'energy_of_negative@5%'],\n",
    ")\n",
    "\n",
    "score_threshold = 0.02\n",
    "\n",
    "mean_features_by_real = filtered_fitness_df[['real'] + [c for c in filtered_fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()\n",
    "feature_columns = get_features_by_abs_diff_threshold(abs_diffs, score_threshold)\n",
    "\n",
    "# remove_all_ngram_scores = []  #  ('full',)\n",
    "# for score_type in ('full', 'setup', 'constraints', 'terminal', 'scoring'):\n",
    "#     col_names = [c for c in feature_columns if c.startswith(f'ast_ngram_{score_type}') and c.endswith('_score')]\n",
    "\n",
    "#     if score_type not in remove_all_ngram_scores:\n",
    "#         col_names = col_names[:-1]\n",
    "\n",
    "#     for col in col_names:\n",
    "#         feature_columns.remove(col)\n",
    "\n",
    "# other_features = ['all_variables_defined', 'all_variables_used',\n",
    "#     'starts_and_ends_once', 'setup_objects_used',\n",
    "#     'all_preferences_used', 'no_adjacent_same_modal', 'adjacent_once_found',\n",
    "#     'repeated_variables_found', 'nested_logicals_found', 'identical_logical_children_found', \n",
    "#     'no_two_number_operations', 'tautological_expression_found', 'redundant_expression_found',]\n",
    "\n",
    "\n",
    "# Next up: compositionality_structure_, max_depth, mean_depth_, node_count_, predicate_under_modal_, max_number_variables_types_quantified_, max_quantification_count_, _arg_types_, length_of_then_modals_\n",
    "# prefixes = ['section_', 'pref_forall_', 'compositionality_structure_', 'max_depth_', 'mean_depth_']  #  \n",
    "# middles = ['score']\n",
    "\n",
    "# feature_columns = [c for c in feature_columns if any(middle in c for middle in middles) or any(c.startswith(prefix) for prefix in prefixes) or c in other_features]\n",
    "# feature_columns = [c for c in feature_columns if 'score' in c or c.startswith('section_')]\n",
    "\n",
    "cv, (train_tensor, test_tensor), results = utils.model_fitting_experiment(\n",
    "    # [fitness_df, mle_samples_df], \n",
    "    fitness_df, \n",
    "    test_param_grid, feature_columns=feature_columns,\n",
    "    scoring_function=scoring, verbose=1, scaler_kwargs=scaler_kwargs, \n",
    "    model_kwargs=model_kwargs, train_kwargs=train_kwargs, cv_kwargs=cv_kwargs,\n",
    "    # energy_weighted_resampling=True, \n",
    "    # random_seed=121,\n",
    "    )\n",
    "\n",
    "utils.visualize_cv_outputs(cv, train_tensor, test_tensor, results, title_note='feature search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/fitness_cv/fitness_sweep_fixed_features_2023_03_22_2.pkl.gz', 'rb') as f:\n",
    "    fitness_sweep = pickle.load(f)\n",
    "\n",
    "\n",
    "cv = fitness_sweep['cv']\n",
    "train_tensor = fitness_sweep['train_tensor']\n",
    "test_tensor = fitness_sweep['test_tensor']\n",
    "results = fitness_sweep['results']\n",
    "feature_columns = fitness_sweep['feature_columns']\n",
    "\n",
    "utils.visualize_cv_outputs(cv, train_tensor, test_tensor, results, title_note='latest sweep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_ON_FULL_DATA = True\n",
    "SAVE_MODEL = True\n",
    "\n",
    "\n",
    "if FIT_ON_FULL_DATA:\n",
    "    full_tensor = utils.df_to_tensor(fitness_df, feature_columns)\n",
    "    cv.best_estimator_['fitness'].train_kwargs['split_validation_from_train'] = False\n",
    "    cv.best_estimator_.fit(full_tensor)\n",
    "    print(utils.evaluate_trained_model(cv.best_estimator_, full_tensor))\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    utils.save_model_and_feature_columns(cv, feature_columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cv.best_estimator_.named_steps['fitness'].model.fc1.weight.squeeze().detach().cpu()\n",
    "\n",
    "K = 15\n",
    "top_features = torch.topk(weights, K)\n",
    "bottom_features = torch.topk(weights, K, largest=False)\n",
    "\n",
    "lines = []\n",
    "\n",
    "lines.append('**Features with largest negative weights (most real):**')\n",
    "for i in range(K):\n",
    "    lines.append(f'{i+1}. {feature_columns[bottom_features.indices[i]]} ({bottom_features.values[i]:.4f})')\n",
    "\n",
    "lines.append('\\n**Features with largest positive weights (most fake):**')\n",
    "for i in range(K):\n",
    "    lines.append((f'{i+1}. {feature_columns[top_features.indices[i]]} ({top_features.values[i]:.4f})'))\n",
    "\n",
    "display(Markdown('\\n'.join(lines)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cv.best_estimator_.named_steps['fitness'].model.fc1.weight.squeeze().detach().cpu()\n",
    "weight_indices = torch.argsort(weights)\n",
    "found_positive = False\n",
    "\n",
    "lines = ['**Feature Weights (ascending):**\\n']\n",
    "for i, idx in enumerate(weight_indices):\n",
    "    lines.append(f'{i:>2}.  {feature_columns[idx]} = {weights[idx].item():.3f}')\n",
    "    if not found_positive and weights[weight_indices[i + 1]] > 0:\n",
    "        found_positive = True\n",
    "        lines.append('----')\n",
    "\n",
    "display(Markdown('\\n'.join(lines)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_fitness_df[(filtered_fitness_df.real == 1) & (filtered_fitness_df.section_without_pref_or_total_count_terminal > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = filtered_fitness_df.groupby('real')[[c for c in filtered_fitness_df.columns if 'max_quantification_count_' in c]].mean()\n",
    "np.abs(gb.loc[1] - gb.loc[0]).mean() * (98 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_fitness_df.groupby('real').section_without_pref_or_total_count_scoring.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_fitness_df.groupby('real').section_without_pref_or_total_count_terminal.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(c, weights[feature_columns.index(c)].item()) for c in feature_columns if 'exists' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_ON_FULL_DATA = True\n",
    "SAVE_MODEL = True\n",
    "\n",
    "\n",
    "if FIT_ON_FULL_DATA:\n",
    "    full_tensor = utils.df_to_tensor(fitness_df, feature_columns)\n",
    "    cv.best_estimator_['fitness'].train_kwargs['split_validation_from_train'] = False\n",
    "    cv.best_estimator_.fit(full_tensor)\n",
    "    print(utils.evaluate_trained_model(cv.best_estimator_, full_tensor))\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    utils.save_model_and_feature_columns(cv, feature_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic evaluation without cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_kwargs = dict(passthrough=True)\n",
    "model_kwargs = dict()\n",
    "train_kwargs = dict(\n",
    "    loss_function=utils.fitness_softmin_loss,\n",
    "    k=1024,\n",
    "    lr=1e-2,\n",
    "    beta=4.0, \n",
    "    negative_score_reduction='none', \n",
    "    n_epochs=3000, \n",
    "    shuffle_negatives=True, \n",
    "    bias_init_margin_ratio=0.01,\n",
    "    # device=torch.device('cuda:0'), \n",
    "    # regularizer=regularizer,\n",
    "    split_validation_from_train=True,\n",
    "    )\n",
    "\n",
    "sweep_param_grid = dict(\n",
    "    patience_epochs=range(10, 60, 10),\n",
    "    use_lr_scheduler=[False, True],\n",
    "    batch_size=[1, 2, 4, 8, 16],\n",
    "    score_threshold=[0, 0.005, 0.01, 0.02, 0.03, 0.04],\n",
    ")\n",
    "\n",
    "scoring = utils.build_multiple_scoring_function(\n",
    "    [utils.wrap_loss_function_to_metric(utils.fitness_sofmin_loss_positive_negative_split, dict(beta=BETA), True),\n",
    "     utils.evaluate_fitness_overall_ecdf, utils.evaluate_fitness_single_game_rank, utils.evaluate_fitness_single_game_min_rank, \n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.01), True),\n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.05), True),\n",
    "     ],\n",
    "    ['loss', 'overall_ecdf', 'single_game_rank', 'single_game_min_rank', 'energy_of_negative@1%', 'energy_of_negative@5%'],\n",
    ")\n",
    "\n",
    "mean_features_by_real = filtered_fitness_df[['real'] + [c for c in filtered_fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()\n",
    "\n",
    "sweep_models = {}\n",
    "sweep_results = {}\n",
    "\n",
    "for (patience_epochs, use_lr_scheduler, batch_size, score_threshold) in tqdm.tqdm(itertools.product(*sweep_param_grid.values())):\n",
    "    setting_train_kwargs = train_kwargs.copy()\n",
    "    setting_train_kwargs.update(dict(patience_epochs=patience_epochs, use_lr_scheduler=use_lr_scheduler, batch_size=batch_size))\n",
    "\n",
    "    feature_columns = list(abs_diffs[abs_diffs >= score_threshold].index)\n",
    "\n",
    "    remove_all_ngram_scores = []  \n",
    "    for score_type in ('full', 'setup', 'constraints', 'terminal', 'scoring'):\n",
    "        col_names = [c for c in feature_columns if c.startswith(f'ast_ngram_{score_type}') and c.endswith('_score')]\n",
    "\n",
    "        if score_type not in remove_all_ngram_scores:\n",
    "            col_names = col_names[:-1]\n",
    "\n",
    "        for col in col_names:\n",
    "            feature_columns.remove(col)\n",
    "\n",
    "    model, _, results = utils.initialize_and_fit_model(\n",
    "        fitness_df, split_test_set=True, feature_columns=feature_columns,\n",
    "        random_seed=DEFAULT_RANDOM_SEED,\n",
    "        scaler_kwargs=scaler_kwargs, model_kwargs=model_kwargs, train_kwargs=setting_train_kwargs,\n",
    "        # energy_weighted_resampling: bool = False, \n",
    "        # train_prop: float = DEFAULT_TRAINING_PROP,\n",
    "        scoring_function=scoring, \n",
    "    )\n",
    "\n",
    "    setting_key = (patience_epochs, use_lr_scheduler, batch_size, score_threshold, len(feature_columns))\n",
    "    sweep_models[setting_key] = model\n",
    "    sweep_results[setting_key] = results\n",
    "\n",
    "\n",
    "KEY_HEADERS = ['patience_epochs', 'use_lr_scheduler', 'batch_size', 'score_threshold', 'n_features']\n",
    "example_values = next(iter(sweep_results.values()))\n",
    "VALUE_HEADERS = [f'{outer_key}_{inner_key}' for outer_key in example_values for inner_key in example_values[outer_key]]\n",
    "\n",
    "rows = [list(key) + [results[outer_key][inner_key] for outer_key in results for inner_key in results[outer_key]]\n",
    "        for key, results in sweep_results.items()]\n",
    "\n",
    "sweep_results_df = pd.DataFrame(rows, columns=KEY_HEADERS + VALUE_HEADERS)\n",
    "sweep_results_df = sweep_results_df.assign(**{c: sweep_results_df[c].abs() for c in sweep_results_df.columns if 'ecdf' in c or 'loss' in c}, use_lr_scheduler=sweep_results_df.use_lr_scheduler.astype(int)\n",
    "sweep_results_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MAPPINGS = {\n",
    "    'patience_epochs': 'Patience Epochs',\n",
    "    'n_features': '# of Features Used',\n",
    "    'use_lr_scheduler': 'Use LR Scheduler',\n",
    "    'batch_size': 'Batch Size',\n",
    "    'train_ecdf': 'Train ECDF',\n",
    "    'test_ecdf': 'Test ECDF',\n",
    "    'train_game_rank': 'Train Game Rank',\n",
    "    'test_game_rank': 'Test Game Rank',\n",
    "}\n",
    "\n",
    "\n",
    "def plot_sweep_results(\n",
    "    results_df: pd.DataFrame, \n",
    "    x_key: str, \n",
    "    color_by_key: str,\n",
    "    column_by_key: typing.Optional[str] = None,\n",
    "    row_by_key: typing.Optional[str] = None,\n",
    "    filter_conditions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "    legend_ax_index: int = 0,\n",
    "    name_mappings: typing.Dict[str, str] = NAME_MAPPINGS,\n",
    "    metrics: typing.List[str] = ['train_overall_ecdf', 'test_overall_ecdf'],\n",
    "    cmap_name: str = 'tab20',\n",
    "    ylabel: typing.Optional[str] = None,\n",
    "    subplot_adjust_params: typing.Optional[typing.Dict[str, float]] = None,\n",
    "    suptitle: typing.Optional[str] = None,\n",
    "    ):\n",
    "\n",
    "    color_values = list(sorted(results_df[color_by_key].unique()))\n",
    "    x_values = list(sorted(results_df[x_key].unique()))\n",
    "\n",
    "    column_values = []\n",
    "    if column_by_key is not None:\n",
    "        column_values = list(sorted(results_df[column_by_key].unique()))\n",
    "\n",
    "    row_values = []\n",
    "    if row_by_key is not None:\n",
    "        row_values = list(sorted(results_df[row_by_key].unique()))\n",
    "\n",
    "    if filter_conditions is not None:\n",
    "        row_filter = np.ones(len(results_df), dtype=bool)\n",
    "        for col, val in filter_conditions.items():\n",
    "            row_filter &= (results_df[col] == val)\n",
    "\n",
    "        df = results_df[row_filter]\n",
    "    else:\n",
    "        df = results_df\n",
    "\n",
    "\n",
    "    groupby_fields = []\n",
    "    n_rows = n_columns = 1\n",
    "\n",
    "    if row_by_key is not None:\n",
    "        groupby_fields.append(row_by_key)\n",
    "        n_rows = len(row_values)\n",
    "\n",
    "    if column_by_key is not None:\n",
    "        groupby_fields.append(column_by_key)\n",
    "        n_columns = len(column_values)\n",
    "        \n",
    "    groupby_fields.append(color_by_key)\n",
    "    groupby_fields.append(x_key)    \n",
    "    results_groupby = df.groupby(groupby_fields)[metrics].mean()\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_columns, figsize=(6 * n_columns, 4 * n_rows), squeeze=False)\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "\n",
    "    for row_index, row_axes in enumerate(axes):\n",
    "        row_value = None if row_by_key is None else row_values[row_index]\n",
    "        for col_index, ax in enumerate(row_axes):\n",
    "            col_value = None if column_by_key is None else column_values[col_index]\n",
    "            \n",
    "            for color_index, color_value in enumerate(color_values):\n",
    "                key = []\n",
    "                if row_value is not None: key.append(row_value)\n",
    "                if col_value is not None: key.append(col_value)\n",
    "                key.append(color_value)\n",
    "\n",
    "                for metric_index, metric in enumerate(metrics):\n",
    "                    y_values = [results_groupby.loc[tuple(key + [x])][metric] for x in x_values]\n",
    "                    ax.plot(x_values, y_values, marker='o', linestyle='--', linewidth=2, \n",
    "                            color=cmap(color_index * len(metrics) + metric_index), \n",
    "                            label=name_mappings.get(color_value, color_value) if metric_index == 0 else None)\n",
    "\n",
    "            ax.set_xlabel(name_mappings.get(x_key, x_key))\n",
    "            if col_index == 0: ax.set_ylabel(ylabel if ylabel is not None else name_mappings.get(metrics[0], metrics[0]))\n",
    "            ax.set_xticks(x_values)\n",
    "            ax.set_xticklabels(x_values)\n",
    "            if (row_index * n_columns) + col_index  == legend_ax_index: ax.legend()\n",
    "            if column_by_key is not None: ax.set_title(f'{name_mappings.get(column_by_key, column_by_key)}={col_value}')\n",
    "\n",
    "    ylim_min = min(ax.get_ylim()[0] for ax in itertools.chain.from_iterable(axes))\n",
    "    ylim_max = max(ax.get_ylim()[1] for ax in itertools.chain.from_iterable(axes))\n",
    "    for ax in itertools.chain.from_iterable(axes):\n",
    "        ax.set_ylim(ylim_min, ylim_max)\n",
    "\n",
    "    if subplot_adjust_params is not None:\n",
    "        plt.subplots_adjust(**subplot_adjust_params)\n",
    "\n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle, fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='use_lr_scheduler',\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Scheduler')\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='use_lr_scheduler',\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Scheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='batch_size',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Batch Size')\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='batch_size',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Batch Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'batch_size', \n",
    "    column_by_key='patience_epochs',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Patience Epochs')\n",
    "\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'batch_size', \n",
    "    column_by_key='patience_epochs',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Patience Epochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the effect of regrowth depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [c for c in binarized_df.columns if c not in NON_FEATURE_COLUMNS]\n",
    "full_binarized_tensor = utils.df_to_tensor(binarized_df, feature_columns)\n",
    "full_tensor_scores = cv_no_scaling_sq_sq.best_estimator_.transform(full_binarized_tensor).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_diffs = full_tensor_scores[:, 1:] - full_tensor_scores[:, 0].unsqueeze(1)\n",
    "energy_diffs.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_regrowth_depth(game_text: str):\n",
    "    game_id_start = game_text.find('(game')\n",
    "    game_id_section = game_text[game_id_start:game_text.find(')', game_id_start)]\n",
    "    regrowth_depth = game_id_section[game_id_section.rfind('-') + 2:]\n",
    "    return int(regrowth_depth)\n",
    "\n",
    "regrowth_depts = [extract_regrowth_depth(g) for g in regrown_game_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(regrowth_depts, energy_diffs.ravel().numpy(), s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_samples_fitness_df = utils.load_fitness_data('../data/ast_mle_fitness_scores.csv')\n",
    "binarized_map_samples_fitness_df = binarize_features(map_samples_fitness_df)\n",
    "\n",
    "map_samples_binarized_arr = binarized_map_samples_fitness_df.loc[:, [c for c in binarized_map_samples_fitness_df.columns if c not in NON_FEATURE_COLUMNS]]\n",
    "map_samples_binarized_tensor = torch.from_numpy(map_samples_binarized_arr.values).float()\n",
    "\n",
    "map_samples_binarized_energies = cv_no_scaling_sq_sq.best_estimator_.transform(map_samples_binarized_tensor.unsqueeze(1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_log_y = True\n",
    "histogram_title_base = 'Binarized features with MAP games, square-square loss'\n",
    "\n",
    "train_positive_scores = cv_no_scaling_sq_sq.best_estimator_.transform(train_tensor_no_scaling_sq_sq[:, 0, :]).detach().squeeze().numpy()  # type: ignore\n",
    "test_positive_scores = cv_no_scaling_sq_sq.best_estimator_.transform(test_tensor_no_scaling_sq_sq[:, 0, :]).detach().squeeze().numpy()  # type: ignore\n",
    "train_negative_scores = cv_no_scaling_sq_sq.best_estimator_.transform(train_tensor_no_scaling_sq_sq[:, 1:, :]).detach().squeeze().numpy()  # type: ignore\n",
    "test_negative_scores = cv_no_scaling_sq_sq.best_estimator_.transform(test_tensor_no_scaling_sq_sq[:, 1:, :]).detach().squeeze().numpy()  # type: ignore\n",
    "\n",
    "hist_scores = [train_positive_scores, test_positive_scores, \n",
    "               train_negative_scores.flatten(), test_negative_scores.flatten(),\n",
    "               map_samples_binarized_energies.detach().numpy()] \n",
    "\n",
    "labels = ['Real (train)', 'Real (test)', 'Negatives (train)', 'Negatives (test)', 'MAP samples']\n",
    "\n",
    "cm = plt.get_cmap('tab20')  # type: ignore\n",
    "colors = cm.colors[:5]\n",
    "\n",
    "plt.hist(hist_scores, label=labels, stacked=True, bins=100, color=colors)  # type: ignore\n",
    "plt.title(histogram_title_base)\n",
    "\n",
    "plt.xlabel('Energy score')\n",
    "\n",
    "if histogram_log_y:\n",
    "    plt.ylabel('log(Count)')\n",
    "    plt.semilogy()\n",
    "else:\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_threshold = 1\n",
    "weights = cv_no_scaling_sq_sq.best_estimator_.named_steps['fitness'].model.fc1.weight.data.detach().squeeze()  # type: ignore\n",
    "weights_above_threshold = (weights.abs() > weight_threshold).numpy()\n",
    "feature_columns = [c for c in binarized_map_samples_fitness_df.columns if c not in NON_FEATURE_COLUMNS]\n",
    "features_with_weight_above_threshold = [feature_columns[i] for i in range(len(feature_columns)) if weights_above_threshold[i]]\n",
    "\n",
    "with open('../data/features_with_weight_above_threshold_2022_01_24.txt', 'w') as f:\n",
    "    f.write('\\n'.join(features_with_weight_above_threshold))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_binarized_tensor = utils.df_to_tensor(binarized_df, [c for c in binarized_df.columns if c not in NON_FEATURE_COLUMNS])\n",
    "test_negative_scores_tensor = torch.tensor(test_negative_scores)\n",
    "test_positive_scores_tensor = torch.tensor(test_positive_scores)\n",
    "feature_columns = [c for c in binarized_df.columns if c not in NON_FEATURE_COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk((test_negative_scores_tensor - test_positive_scores_tensor.unsqueeze(-1)).ravel(), 30, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_negative_scores_tensor.ravel()[836] == test_negative_scores_tensor[836 // 64, 836 % 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df[(fitness_df.real == 1) & (fitness_df.all_variables_used == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from src.fitness_features import *\n",
    "# from src.fitness_ngram_models import TextNGramModel, TextMultiNGramModel, ASTMultiNGramModel, NGramASTParser\n",
    "\n",
    "# with gzip.open('../models/fitness_featurizer_2023_02_02.pkl.gz', 'rb') as f:\n",
    "#     featurizer = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    no_binarize=False, \n",
    "    no_merge=False, \n",
    "    use_specific_objects_ngram_model=False,\n",
    "    include_predicate_under_modal_terms=False,\n",
    "    include_arg_types_terms=False,\n",
    "    include_compositionality_terms=False,\n",
    ")\n",
    "# featurizer = build_fitness_featurizer(args)\n",
    "\n",
    "preprocessors = []\n",
    "\n",
    "if not args.no_binarize:\n",
    "    preprocessors.append(BinarizeFitnessFeatures())\n",
    "\n",
    "if not args.no_merge and args.include_arg_types_terms:  # the merge is only used for the arg_types featuers\n",
    "    preprocessors.append(MergeFitnessFeatures(COMMON_SENSE_PREDICATES_FUNCTIONS))\n",
    "\n",
    "featurizer = ASTFitnessFeaturizer(args, preprocessors=preprocessors)\n",
    "\n",
    "\n",
    "class ScoringPreferencesUsedIdentically(FitnessTerm):\n",
    "    ignore_positions: typing.Set[int]\n",
    "    position_and_op_to_preference: typing.Dict[typing.Tuple[int, str], typing.Set[str]]\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(('scoring_neg_expr', 'scoring_binary_expr', 'scoring_multi_expr'), 'scoring_preferences_used_identically')\n",
    "        self.ignore_positions = set()\n",
    "        self.position_and_op_to_preference = defaultdict(set)\n",
    "        \n",
    "    def game_start(self) -> None:\n",
    "        self.ignore_positions.clear()\n",
    "        self.position_and_op_to_preference = defaultdict(set)\n",
    "\n",
    "    def _handle_multi_expr(self, ast: tatsu.ast.AST, op: str, pos: int):\n",
    "        for expr in ast.expr:  # type: ignore\n",
    "            expr = expr.expr\n",
    "            if expr.parseinfo.rule == 'scoring_expr':\n",
    "                if expr.expr.parseinfo.rule == 'preference_eval':\n",
    "                    self.position_and_op_to_preference[(pos, op)].add(expr.expr.count_method.name_and_types.pref_name)\n",
    "\n",
    "                # a product of a products is still a product; same with a sum\n",
    "                if expr.expr.parseinfo.rule == 'scoring_multi_expr' and expr.expr.op == op:\n",
    "                    self.ignore_positions.add(expr.expr.parseinfo.pos)\n",
    "                    self._handle_multi_expr(expr.expr, op, pos)\n",
    "\n",
    "    def update(self, ast: typing.Union[typing.Sequence, tatsu.ast.AST], rule: str, context: ContextDict):\n",
    "        op = '-' if rule == 'scoring_neg_expr' else typing.cast(str, ast.op)  # type: ignore\n",
    "        pos = typing.cast(int, ast.parseinfo.pos)  # type: ignore\n",
    "        \n",
    "        if rule == 'scoring_neg_expr':\n",
    "            expr = ast.expr.expr  # type: ignore\n",
    "            if expr.parseinfo.rule == 'scoring_expr' and expr.expr.parseinfo.rule == 'preference_eval':\n",
    "                self.position_and_op_to_preference[(pos, op)].add(expr.expr.count_method.name_and_types.pref_name)\n",
    "        \n",
    "        if rule == 'scoring_binary_expr':\n",
    "            for expr in (ast.expr_1.expr, ast.expr_2.expr):  # type: ignore\n",
    "                if expr.parseinfo.rule == 'scoring_expr' and expr.expr.parseinfo.rule == 'preference_eval':\n",
    "                    self.position_and_op_to_preference[(pos, op)].add(expr.expr.count_method.name_and_types.pref_name)  # type: ignore\n",
    "\n",
    "        if rule == 'scoring_multi_expr':\n",
    "            self._handle_multi_expr(ast, op, pos)  # type: ignore\n",
    "\n",
    "    def game_end(self):\n",
    "        # We flag when there's only one expression, that's a product, with multiple preferences\n",
    "        if len(self.position_and_op_to_preference) == 1:\n",
    "            key = next(iter(self.position_and_op_to_preference.keys()))\n",
    "            return len(self.position_and_op_to_preference[key]) != 1 and key[1] == '*'\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "featurizer.register(ScoringPreferencesUsedIdentically())\n",
    "\n",
    "# for section in ast_parser.SECTION_KEYS:\n",
    "#     featurizer.register(SectionNodeCount(section), section_rule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [featurizer.parse(game_asts[i], 'interactive-beta.pddl', return_row=False) for i in range(len(game_asts))]\n",
    "# _ = [featurizer.parse(game_asts[74], 'interactive-beta.pddl', return_row=False) for _ in range(1000)]\n",
    "# _ = [featurizer.parse(regrown_game_asts[i], return_row=False) for i in tqdmn.trange(len(regrown_game_asts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = featurizer.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.scoring_preferences_used_identically == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for c in d.columns[4:]:\n",
    "    vals = d[c].values\n",
    "    nonzero_vals = vals[vals != 0]\n",
    "\n",
    "    print(c, np.quantile(nonzero_vals, [0.2, 0.4, 0.6, 0.8]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'node_count_scoring'\n",
    "bins = [4, 21, 35, 80]\n",
    "\n",
    "bincounts = np.bincount(np.digitize(d[k], bins, right=True))\n",
    "idxs = np.nonzero(bincounts)[0]\n",
    "{i: bincounts[i] for i in idxs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.node_count_scoring.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max depth:\n",
    "# setup: [0, 11, 13, 18]\n",
    "# constraints: [15, 17, 19, 21]\n",
    "# terminal: [0, 5, 7, 11]\n",
    "# scoring: [3, 9, 11, 15]\n",
    "\n",
    "# node count: \n",
    "# setup: [0, 20, 50, 100]\n",
    "# constraints: [60, 90, 120, 250]\n",
    "# terminal: [0, 4, 9, 22]\n",
    "# scoring: [4, 21, 35, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[(d.disjoint_preferences_scoring_terminal_predicates > 0)][['game_name', 'disjoint_preferences_scoring_terminal_predicates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.predicate_found_in_data_small_logicals_prop != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[[c for c in d.columns if 'in_data' in c]].mean()\n",
    "\n",
    "# print(d.disjoint_modal_predicates_found.mean())\n",
    "# d[d.disjoint_modal_predicates_found == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "_ = [featurizer.parse(game_asts[i], 'interactive-beta.pddl', return_row=True, preprocess_row=False) for i in range(len(game_asts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.disjoint_at_end_found.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "(define (game evo-4044-212-0) (:domain medium-objects-room-v1)\n",
    "(:setup\n",
    "  (exists (?v0 - hexagonal_bin)\n",
    "    (game-conserved\n",
    "      (near west_sliding_door ?v0)\n",
    "   )\n",
    " )\n",
    ")\n",
    "(:constraints\n",
    "  (and\n",
    "    (preference preference0\n",
    "      (exists (?v1 - dodgeball ?v2 - hexagonal_bin)\n",
    "        (then\n",
    "          (once (agent_holds ?v1))\n",
    "          (hold (and (not (agent_holds ?v1)) (in_motion ?v1)))\n",
    "          (once (and (not (in_motion ?v1)) (in ?v2 ?v1)))\n",
    "       )\n",
    "     )\n",
    "   )\n",
    "    (preference preference1\n",
    "      (exists (?v0 - cylindrical_block ?v1 - pyramid_block_red ?v2 - cube_block)\n",
    "        (at-end\n",
    "          (and\n",
    "            (on ?v0 ?v2)\n",
    "            (on ?v0 ?v1)\n",
    "            (on ?v2 ?v1)\n",
    "         )\n",
    "       )\n",
    "     )\n",
    "   )\n",
    " )\n",
    ")\n",
    "(:scoring\n",
    "  (+ -6 (count preference1) (count preference0))\n",
    ")\n",
    ")\n",
    "\"\"\".strip()\n",
    "test_game = grammar_parser.parse(s)\n",
    "f = featurizer.parse(test_game, 'interactive-beta.pddl', preprocess_row=False, return_row=True)\n",
    "{k: v for k, v in f.items() if 'identically' in k}  #  or 'unnecessary' in k}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [2.0, 3.0, 4.0, 10]\n",
    "right = True\n",
    "bins = np.digitize(d.max_width_constraints, thresholds, right)\n",
    "np.unique(bins, return_counts=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[['node_count', 'unique_objects_referenced', 'unique_predicates_referenced']].quantile(np.linspace(0.1, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.plot(x='unique_objects_referenced', y='unique_predicates_referenced', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.plot(y='node_count', kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.assign(original_game_name=d.game_name)  # real=fitness_df.src_file == 'interactive-beta.pddl',\n",
    "d.original_game_name.where(\n",
    "    d.game_name.apply(lambda s: (s.count('-') <= 1) or (s.startswith('game-id') and s.count('-') >= 2)),\n",
    "    d.original_game_name.apply(lambda s: s[:utils._find_nth(s, '-', 2)]),\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.redundant_scoring_terminal_expression_found.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.02\n",
    "\n",
    "mean_features_by_real = filtered_fitness_df[['real'] + [c for c in filtered_fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()\n",
    "feature_columns = get_features_by_abs_diff_threshold(abs_diffs, score_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(game_asts)):\n",
    "    row = typing.cast(dict, featurizer.parse(game_asts[i], 'interactive-beta.pddl', return_row=True, preprocess_row=True))\n",
    "    df_row = d[d.game_name == row['game_name']]\n",
    "    for key in row:\n",
    "        if key in df_row and row[key] != df_row[key].values[0]:\n",
    "            print(f'In game {row[\"game_name\"]}, {key} mismatch: {row[key]} != {df_row[key].values[0]}')\n",
    "\n",
    "        elif key not in df_row:\n",
    "            print(f'In game {row[\"game_name\"]}, {key} not in df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_sum_features = []\n",
    "positive_mean_features = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature not in d.columns:\n",
    "        continue\n",
    "\n",
    "    if any(x in feature for x in ('arg_types', 'predicate_under_modal', 'max_number', 'max_quantification', 'compositionality_structure', 'depth', 'node_count')):\n",
    "        continue\n",
    "\n",
    "    if d.loc[d.real == True, feature].sum() == 0:\n",
    "        zero_sum_features.append(feature)\n",
    "    else:\n",
    "        positive_mean_features.append(feature)\n",
    "\n",
    "print(f'Zero sum features: {zero_sum_features}')\n",
    "print(f'Positive sum features: {positive_mean_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[[c for c in d.columns if 'max_q' in c]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_game_name in d.original_game_name.unique():\n",
    "    original_game_scoring_score = d[d.game_name == original_game_name].ast_ngram_scoring_n_5_score.max()\n",
    "    sub_frame = d.loc[(d.original_game_name == original_game_name) & (d.real == 0) & (d.ast_ngram_scoring_n_5_score > original_game_scoring_score), 'ast_ngram_scoring_n_5_score']\n",
    "    if len(sub_frame) > 0:\n",
    "        print(original_game_name)\n",
    "        print(sub_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby('real').ast_ngram_scoring_n_5_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.section_exists_setup == 0].ast_ngram_setup_n_5_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la /tmp/gd1279/fitness_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = featurizer.to_df()\n",
    "temp_df = pd.read_csv('/tmp/gd1279/fitness_features/fitness_features_1024_regrowths.csv.gz_0.temp.csv')\n",
    "temp_df.columns = d.columns\n",
    "temp_df = utils._add_original_game_name_column(temp_df)\n",
    "d = utils._add_original_game_name_column(d)\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_game_name in d.original_game_name.unique()[1:]:\n",
    "    original_game_scoring_score = d[d.game_name == original_game_name].ast_ngram_scoring_n_5_score.max()\n",
    "    if original_game_scoring_score == 0:\n",
    "        print(original_game_name)\n",
    "        sub_frame = temp_df.loc[(temp_df.original_game_name == original_game_name) & (temp_df.real == 0) & (temp_df.ast_ngram_scoring_n_5_score > original_game_scoring_score),\n",
    "                                ['game_name', 'original_game_name', 'ast_ngram_scoring_n_5_score']]\n",
    "        if len(sub_frame) > 0:\n",
    "            print(original_game_name, len(sub_frame))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_index = 65\n",
    "negative_index = 470\n",
    "\n",
    "pos_r = featurizer.parse(game_asts[game_index], 'test.pddl', return_row=True, preprocess_row=False)\n",
    "neg_r = featurizer.parse(regrown_game_asts[game_index * 1024 + negative_index], 'test.pddl', return_row=True, preprocess_row=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/ast_7_ngram_model_2023_03_06.pkl', 'rb') as f:\n",
    "    ngram_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast_parser import ASTParentMapper\n",
    "parent_mapper = ASTParentMapper()\n",
    "parent_mapper(game_asts[game_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = featurizer.parse(game_asts[17], 'test.pddl', return_row=True, preprocess_row=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in r.items() if k.startswith('section_without')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ARGS = argparse.Namespace(\n",
    "    grammar_file=os.path.join('..', DEFAULT_GRAMMAR_FILE),\n",
    "    parse_counter=False,\n",
    "    counter_output_path=os.path.join('..', DEFAULT_COUNTER_OUTPUT_PATH),\n",
    "    random_seed=DEFAULT_RANDOM_SEED,\n",
    ")\n",
    "\n",
    "grammar = open(DEFAULT_ARGS.grammar_file).read()\n",
    "grammar_parser = typing.cast(tatsu.grammars.Grammar, tatsu.compile(grammar))  # type: ignore\n",
    "counter = parse_or_load_counter(DEFAULT_ARGS, grammar_parser)\n",
    "\n",
    "sampler = ASTSampler(grammar_parser, counter, seed=DEFAULT_RANDOM_SEED) \n",
    "regrowth_sampler = RegrowthSampler(sampler, seed=DEFAULT_RANDOM_SEED, rng=sampler.rng)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.counters['predicate_near']['arg_2'].rule_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d207b42274502bc006609ff0f580407f35ab20e7889cda7ddd92e73aeb06c569"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
