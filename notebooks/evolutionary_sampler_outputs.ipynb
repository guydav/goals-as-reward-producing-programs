{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from argparse import Namespace\n",
    "import copy\n",
    "import gzip\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "import duckdb\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.axes\n",
    "import matplotlib.pyplot as plt\n",
    "from Levenshtein import distance as _edit_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import tatsu\n",
    "import tatsu.ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src.ast_utils import _extract_game_id, deepcopy_ast, replace_child\n",
    "from src.ast_printer import ast_to_lines\n",
    "from src import fitness_energy_utils as utils\n",
    "from src.fitness_energy_utils import NON_FEATURE_COLUMNS\n",
    "from src.fitness_features import *\n",
    "from src.ast_counter_sampler import *\n",
    "from src.evolutionary_sampler import *\n",
    "from src import fitness_features_by_category, latest_model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='..'))\n",
    "real_game_texts = [ast_printer.ast_to_string(ast, '\\n') for ast in game_asts]\n",
    "# regrown_game_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples.pddl'))\n",
    "# regrown_game_1024_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples-1024.pddl'))\n",
    "# print(len(real_game_texts), len(regrown_game_texts), len(regrown_game_texts) / 98, len(regrown_game_1024_texts), len(regrown_game_1024_texts) / 98)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df = utils.load_fitness_data('../data/fitness_features_1024_regrowths.csv.gz')\n",
    "print(fitness_df.src_file.unique())\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = ast_parser.ASTSamplePostprocessor()\n",
    "postprocessed_real_game_texts = [ast_printer.ast_to_string(postprocessor(ast), '\\n') for ast in game_asts]  # type: ignore\n",
    "\n",
    "real_game_edit_distances = np.zeros((len(game_asts), len(game_asts)), dtype=int)\n",
    "for i, j in itertools.combinations(range(len(game_asts)), 2):\n",
    "    real_game_edit_distances[i, j] = real_game_edit_distances[j, i] = edit_distance(postprocessed_real_game_texts[i], postprocessed_real_game_texts[j])\n",
    "\n",
    "\n",
    "def edit_distance(first_game_text: str, second_game_text: str):\n",
    "    first_game_text = first_game_text[first_game_text.find(')', first_game_text.find('(:domain')) + 1:]\n",
    "    second_game_text = second_game_text[second_game_text.find(')', second_game_text.find('(:domain')) + 1:]\n",
    "    return _edit_distance(first_game_text, second_game_text)\n",
    "\n",
    "\n",
    "def find_nearest_real_game_indices(game, k: int = 3) -> typing.Tuple[np.ndarray, np.ndarray]:\n",
    "    game_str = ast_printer.ast_to_string(game, '\\n')\n",
    "    distances = np.array([edit_distance(game_str, real_game) for real_game in postprocessed_real_game_texts])\n",
    "    nearest_real_game_indices = np.argsort(distances)[:k]\n",
    "    return nearest_real_game_indices, distances[nearest_real_game_indices]\n",
    "\n",
    "\n",
    "def print_nearest_real_games(game, k: int = 3):\n",
    "    indices, distances = find_nearest_real_game_indices(game, k)\n",
    "    for i, (idx, d) in enumerate(zip(indices, distances)):\n",
    "        real_game_distances = real_game_edit_distances[idx]\n",
    "        nearest_distance_indices = np.argsort(real_game_distances)[1:k + 1]  # index 0 is the game itself\n",
    "        display(Markdown(f'### Nearest real game #{i + 1}:'))\n",
    "        display(Markdown(f'Edit distance to sample {d}, real game nearest neighbor distances {np.array2string(real_game_distances[nearest_distance_indices], separator=\", \")}):\\n'))\n",
    "        display(Markdown(f'```pddl\\n{real_game_texts[idx]}\\n```'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTOGRAMS_SUBPLOTS_ADJUST_PARAMS = dict(hspace=0.3)\n",
    "\n",
    "\n",
    "def plot_value_histograms(results_by_feature_name_and_value: typing.Dict[str, typing.Dict[int, typing.List[float]]],\n",
    "    bins: int = 20, histogram_log_y: bool = False, \n",
    "    histogram_density: bool = True, layout: typing.Optional[typing.Tuple[int, int]] = None,\n",
    "    figsize: typing.Optional[typing.Tuple[float, float]] = None, \n",
    "    panel_width: float = 4, panel_height: float = 4, ylabel_once_per_row: bool = True,\n",
    "    subplots_adjust_params: typing.Optional[typing.Dict[str, float]] = HISTOGRAMS_SUBPLOTS_ADJUST_PARAMS,\n",
    "    title_fontsize: int = 12, title_split_threshold: int = 25,\n",
    "    cm: plt.get_cmap('tab20') = plt.get_cmap('tab20')):  # type: ignore\n",
    "    \n",
    "    k = len(results_by_feature_name_and_value.keys())\n",
    "\n",
    "    if layout is None:\n",
    "        largest_div = int(np.floor(k ** 0.5))\n",
    "        while k % largest_div != 0:\n",
    "            largest_div -= 1\n",
    "\n",
    "        layout = (largest_div, k // largest_div)\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (layout[1] * panel_width, layout[0] * panel_height)\n",
    "\n",
    "    fig, axes = plt.subplots(*layout, figsize=figsize)\n",
    "\n",
    "    for i, feature_name in enumerate(results_by_feature_name_and_value.keys()):\n",
    "        if layout[0] == 1:\n",
    "            ax = axes[i]\n",
    "        else:\n",
    "            ax = axes[i // layout[1]][i % layout[1]]\n",
    "\n",
    "        values_with = results_by_feature_name_and_value[feature_name][1]\n",
    "        values_without = results_by_feature_name_and_value[feature_name][0]\n",
    "\n",
    "        # print(f'Feature = 0 {(real_values == 0).mean() * 100:.2f}% of the time in real games, {(synthetic_values == 0).mean() * 100:.2f}% of the time in synthetic games')\n",
    "\n",
    "        ax.hist([values_with, values_without], label=[f'1 (n={len(values_with)})', f'0 (n={len(values_without)})'], \n",
    "            stacked=False, density=histogram_density, bins=bins, color=[cm.colors[0], cm.colors[2]])  # type: ignore\n",
    "        ax.set_xlabel('Fitness value')\n",
    "\n",
    "        if not ylabel_once_per_row or i % layout[1] == 0:\n",
    "            if histogram_density:\n",
    "                if histogram_log_y:\n",
    "                    ax.set_ylabel('log(Density)')\n",
    "                else:\n",
    "                    ax.set_ylabel('Density')\n",
    "            elif histogram_log_y:\n",
    "                ax.set_ylabel('log(Count)')\n",
    "            else:\n",
    "                ax.set_ylabel('Count')\n",
    "\n",
    "        if histogram_log_y:\n",
    "            ax.semilogy()\n",
    "        \n",
    "        title = f'#{i + 1}: {feature_name}'\n",
    "        ax.set_title(title, fontdict=dict(fontsize=title_fontsize))\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    if subplots_adjust_params is not None:\n",
    "        plt.subplots_adjust(**subplots_adjust_params)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_archive_fullness(model: MAPElitesSampler, mutually_exclusive_features: typing.Optional[typing.List[str]] = None,\n",
    "                             plot_kwargs: typing.Optional[typing.Dict[str, typing.Any]] = None):\n",
    "    if plot_kwargs is None:\n",
    "        plot_kwargs = {}\n",
    "    \n",
    "    results_by_feature_name = {feature_name: defaultdict(list) for feature_name in model.map_elites_feature_names}\n",
    "    results_by_feature_count = defaultdict(list)\n",
    "\n",
    "    for key, fitness_value in model.fitness_values.items():\n",
    "        for feature_name, feature_value in model._key_to_feature_dict(key).items():\n",
    "            results_by_feature_name[feature_name][feature_value].append(fitness_value)\n",
    "\n",
    "        if isinstance(key, int):\n",
    "            key_bits = count_set_bits(key)\n",
    "        else:\n",
    "            key_bits = sum(k != 0 for k in key)\n",
    "\n",
    "        results_by_feature_count[key_bits].append(fitness_value)\n",
    "\n",
    "    display(Markdown(f'## Archive fullness analysis'))\n",
    "    display(Markdown(f'Ttoal of {len(model.fitness_values)} samples in archive, {len(model.map_elites_feature_names)} features'))\n",
    "    display(Markdown(f'### Results by feature'))\n",
    "    lines = []\n",
    "    for feature_name, results in results_by_feature_name.items():\n",
    "        lines.append(f'- {feature_name}:')\n",
    "        for feature_value in sorted(results.keys()):\n",
    "            value_results = results[feature_value]\n",
    "            lines.append(f'    - ={feature_value}: {np.mean(value_results):.3f} ± {np.std(value_results):.3f} (n={len(value_results)})')\n",
    "\n",
    "    display(Markdown('\\n'.join(lines)))\n",
    "\n",
    "    display(Markdown(f'### Results by set feature count'))\n",
    "    lines = []\n",
    "    for feature_count in sorted(results_by_feature_count.keys()):\n",
    "        value_results = results_by_feature_count[feature_count]\n",
    "        lines.append(f'- set-count={feature_count}: {np.mean(value_results):.3f} ± {np.std(value_results):.3f} (n={len(value_results)} / {math.comb(len(model.map_elites_feature_names), feature_count)})')\n",
    "\n",
    "    display(Markdown('\\n'.join(lines)))\n",
    "\n",
    "    plot_value_histograms(results_by_feature_name, **plot_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBPLOTS_ADJUST_PARAMS = dict(top=0.925)\n",
    "DEFAULT_IGNORE_METRICS = ['Timestamp']\n",
    "\n",
    "\n",
    "FIGURE_TEMPLATE = r'''\\begin{{figure}}[!htb]\n",
    "% \\vspace{{-0.225in}}\n",
    "\\centering\n",
    "\\includegraphics[width=\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "% \\vspace{{-0.2in}}\n",
    "\\end{{figure}}\n",
    "'''\n",
    "WRAPFIGURE_TEMPLATE = r'''\\begin{{wrapfigure}}{{r}}{{0.5\\linewidth}}\n",
    "\\vspace{{-.3in}}\n",
    "\\begin{{spacing}}{{1.0}}\n",
    "\\centering\n",
    "\\includegraphics[width=0.95\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "\\end{{spacing}}\n",
    "% \\vspace{{-.25in}}\n",
    "\\end{{wrapfigure}}'''\n",
    "\n",
    "SAVE_PATH_PREFIX = './figures'\n",
    "\n",
    "\n",
    "def save_plot(save_path, bbox_inches='tight', should_print=False):\n",
    "    if save_path is not None:\n",
    "        save_path_no_ext = os.path.splitext(save_path)[0]\n",
    "        if should_print:\n",
    "            print('Figure:\\n')\n",
    "            print(FIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "            print('\\nWrapfigure:\\n')\n",
    "            print(WRAPFIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "            print('')\n",
    "        \n",
    "        if not save_path.startswith(SAVE_PATH_PREFIX):\n",
    "            save_path = os.path.join(SAVE_PATH_PREFIX, save_path)\n",
    "        \n",
    "        save_path = os.path.abspath(save_path)\n",
    "        folder, filename = os.path.split(save_path)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=bbox_inches, facecolor=plt.gcf().get_facecolor(), edgecolor='none')\n",
    "\n",
    "\n",
    "\n",
    "def plot_sampler_fitness_trajectory(\n",
    "        evo: PopulationBasedSampler, title: typing.Optional[str] = None, \n",
    "        axsize: typing.Tuple[int, int] = (8, 6),\n",
    "        plot_metrics: typing.Optional[bool] = None, \n",
    "        ignore_metrics: typing.Optional[typing.List[str]] = DEFAULT_IGNORE_METRICS,\n",
    "        subplots_adjust_params: typing.Dict[str, float] = SUBPLOTS_ADJUST_PARAMS,\n",
    "        min_real_game_fitness: typing.Optional[float] = None, max_real_game_fitness: typing.Optional[float] = None,\n",
    "        mean_real_game_fitness: typing.Optional[float] = None, \n",
    "        vertical: bool = True,\n",
    "        fontsize: int = 16,\n",
    "        save_path: typing.Optional[str] = None): \n",
    "    \n",
    "    if min_real_game_fitness is None or max_real_game_fitness is None:\n",
    "        raise ValueError('min_real_game_fitness and max_real_game_fitness must be specified')\n",
    "\n",
    "    if plot_metrics is None:\n",
    "        plot_metrics = hasattr(evo, 'archive_metrics_history') and len(evo.archive_metrics_history) > 0  # type: ignore\n",
    "\n",
    "    if ignore_metrics is None:\n",
    "        ignore_metrics = []\n",
    "    \n",
    "    if not plot_metrics:\n",
    "        layout = (1, 1)\n",
    "    elif vertical:\n",
    "        layout = (2, 1)\n",
    "    else:\n",
    "        layout = (1, 2)\n",
    "\n",
    "    figsize = (axsize[0] * layout[1], axsize[1] * layout[0])\n",
    "\n",
    "    fig, axes = plt.subplots(*layout, figsize=figsize)\n",
    "\n",
    "    mean, max_fit, std = [], [], []\n",
    "    for step_dict in evo.fitness_metrics_history:\n",
    "        mean.append(step_dict['mean'])\n",
    "        max_fit.append(step_dict['max'])\n",
    "        std.append(step_dict['std'])\n",
    "\n",
    "    mean = np.array(mean)\n",
    "    max_fit = np.array(max_fit)\n",
    "    std = np.array(std)\n",
    "\n",
    "    fitness_ax = typing.cast(matplotlib.axes.Axes, axes[0] if plot_metrics else axes)\n",
    "\n",
    "    fitness_ax.plot(mean, label='MAP-Elites fitness mean')\n",
    "    fitness_ax.fill_between(np.arange(len(mean)), mean - std, mean + std, alpha=0.2, label='MAP-Elites fitness std')  # type; ignore\n",
    "    fitness_ax.plot(max_fit, label='MAP-Elites fitness max')\n",
    "\n",
    "    fitness_ax.hlines(min_real_game_fitness, 0, len(mean), label='Real game fitness range', color='black', ls='--')\n",
    "    fitness_ax.hlines(max_real_game_fitness, 0, len(mean), color='black', ls='--')\n",
    "\n",
    "    if mean_real_game_fitness is not None:\n",
    "        fitness_ax.hlines(mean_real_game_fitness, 0, len(mean), label='Real game fitness mean', color='black', ls=':')\n",
    "\n",
    "    fitness_ax.set_xlabel('Generation', fontsize=fontsize)\n",
    "    fitness_ax.set_ylabel('Fitness', fontsize=fontsize)\n",
    "\n",
    "    fitness_ax.legend(loc='best', fontsize=fontsize)\n",
    "    fitness_ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "    \n",
    "    if plot_metrics:\n",
    "        metrics = {key: [] for key in evo.archive_metrics_history[0].keys() if key not in ignore_metrics}  # type: ignore\n",
    "        for step_dict in evo.archive_metrics_history:  # type: ignore\n",
    "            for key, value in step_dict.items():\n",
    "                if key in metrics:\n",
    "                    metrics[key].append(value)\n",
    "\n",
    "        metrics_ax = typing.cast(matplotlib.axes.Axes, axes[1])\n",
    "        for key, values in metrics.items():\n",
    "            metrics_ax.plot(values, label=key.title())\n",
    "\n",
    "        metrics_ax.set_xlabel('Generation', fontsize=fontsize)\n",
    "        metrics_ax.set_ylabel('Number of games reaching threshold', fontsize=fontsize)\n",
    "\n",
    "        metrics_ax.legend(loc='best', fontsize=fontsize)\n",
    "        metrics_ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "        plt.subplots_adjust(**subplots_adjust_params)\n",
    "        \n",
    "    if title is not None:\n",
    "        if plot_metrics:\n",
    "            plt.suptitle(title)\n",
    "        else:\n",
    "            plt.title(title)\n",
    "\n",
    "    if save_path is not None:\n",
    "        save_plot(save_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def count_games_above_fitness_threshold(evo: PopulationBasedSampler, threshold: float) -> int:\n",
    "    if isinstance(evo.fitness_values, dict):\n",
    "        fitness_values = evo.fitness_values.values()\n",
    "    else:\n",
    "        fitness_values = evo.fitness_values\n",
    "\n",
    "    return sum(1 for fitness in fitness_values if fitness >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPRESENTATIVE_GAME_INDICES = [26, 58, 42, 31, 48, 19, 14, 62, 93] \n",
    "\n",
    "\n",
    "NEW_REPRESENTATIVE_INDICES = [\n",
    "    42,  # throw\n",
    "    58,  # funny throw with setup\n",
    "    28,  # throw, throw attempt, with setup \n",
    "    88,  # throw, throw attempt, no setup\n",
    "    31,  # throw without in/on\n",
    "    64,  # dropping ball in bin, drop attempt\n",
    "    52,  # ball-less throwing game\n",
    "    86,  # weird throw, with setup\n",
    "    6,  # two weird throwing preferences, setup\n",
    "    17,  # anoter weird multi throwing preference game\n",
    "    96,  # three throwing preferences, setup\n",
    "    \n",
    "    14,  # castle building\n",
    "    45,  # weird building game\n",
    "    49,  # another weird building game\n",
    "    51,  # hybrid throwing/building game\n",
    "    \n",
    "    23,  # single placement preference\n",
    "    44,  # three different placement preferencs \n",
    "\n",
    "    26,  # single preferece that matches none of the others\n",
    "]\n",
    "\n",
    "WARNING_YELLOW_BG_COLOR = '#F1EB9C'\n",
    "\n",
    "GAME_INDEX = 0\n",
    "GAME_AST = 1\n",
    "GAME_FITNESS = 2\n",
    "GAME_KEY = 3\n",
    "SAMPLE_FITNESS = 4\n",
    "SAMPLE_AST = 5\n",
    "\n",
    "\n",
    "CUSTOM_BACKGROUND_COLOR_CLASSES = ('bg-info', 'bg-warning', 'bg-success', 'bg-danger', 'bg-primary', 'bg-secondary', 'bg-dark', 'bg-light')\n",
    "\n",
    "\n",
    "def _format_ast_for_html(ast, line_delimiter='<br>', increment='  ',):\n",
    "    return f'<pre><code>{ast_printer.ast_to_string(ast, line_delimiter, increment=increment)}</code></pre>'\n",
    "\n",
    "\n",
    "def make_representative_game_table(model: MAPElitesSampler, trace_filter_data: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "                                   representative_game_indices: typing.Optional[typing.List[int]] = REPRESENTATIVE_GAME_INDICES, \n",
    "                                   real_game_asts=game_asts, line_delimiter='<br>', increment='  ', tablefmt='unsafehtml'):\n",
    "    real_game_fitness_scores, real_game_fitness_features = zip(*[model._score_proposal(game, return_features=True) for game in real_game_asts])  # type: ignore\n",
    "    real_game_fitness_score_indices = np.argsort(real_game_fitness_scores)[::-1]  # type: ignore\n",
    "\n",
    "    # If not provided, take the highest fitness game in each cell\n",
    "    cells_occupied = set()\n",
    "    if representative_game_indices is None:\n",
    "        representative_game_indices = []\n",
    "\n",
    "        for real_game_index in real_game_fitness_score_indices:\n",
    "            real_game = real_game_asts[real_game_index]\n",
    "            real_game_key = model._features_to_key(real_game, real_game_fitness_features[real_game_index])\n",
    "            if real_game_key not in cells_occupied:\n",
    "                representative_game_indices.append(real_game_index)\n",
    "                cells_occupied.add(real_game_key)\n",
    "\n",
    "    rows = [[] for _ in range(6)]\n",
    "\n",
    "    for idx in representative_game_indices:\n",
    "        real_game = real_game_asts[idx]\n",
    "        real_game_key = model._features_to_key(real_game, real_game_fitness_features[idx])\n",
    "        if real_game_key is None:\n",
    "            continue\n",
    "\n",
    "        real_game_rank = np.where(real_game_fitness_score_indices == idx)[0][0]\n",
    "        rows[GAME_INDEX].append(f'Game #{idx} (fitness rank {real_game_rank})')\n",
    "\n",
    "        real_game_ast_str = _format_ast_for_html(real_game_asts[idx], line_delimiter=line_delimiter, increment=increment)\n",
    "        rows[GAME_AST].append(real_game_ast_str)\n",
    "\n",
    "        fitness = real_game_fitness_scores[idx]\n",
    "        rows[GAME_FITNESS].append(f'<strong>Real game fitness: {fitness:.4f}</<strong>')\n",
    "        \n",
    "        key_dict = model._key_to_feature_dict(real_game_key)\n",
    "        key_dict_str = '<br>'.join([f'<strong>Key {real_game_key}:</strong>'] + [f'{feature_name}: {feature_value}' for feature_name, feature_value in key_dict.items()])\n",
    "        rows[GAME_KEY].append(key_dict_str)\n",
    "\n",
    "        if real_game_key in model.population:\n",
    "            sample = model.population[real_game_key]\n",
    "            sample_fitness = model.fitness_values[real_game_key]\n",
    "            custom_class = 'placeholder'\n",
    "            postfix = ''\n",
    "\n",
    "            if trace_filter_data is not None and real_game_key in trace_filter_data['summary']:\n",
    "                sample_trace_filter_result = trace_filter_data['summary'][real_game_key]\n",
    "                postfix = f' (# traces : {sample_trace_filter_result}{\"+\" if sample_trace_filter_result >= 5 else \"\"})'\n",
    "\n",
    "                if sample_trace_filter_result == 0:\n",
    "                    full_trace_filter_result = trace_filter_data['full'][real_game_key]\n",
    "                    trace_count_by_pref = {pref: sum(1 for v in pref_counts.values() if v > 0) for pref, pref_counts in full_trace_filter_result.items()}\n",
    "                    trace_count_by_pref_str = ', '.join([f'{pref}: {count}' for pref, count in trace_count_by_pref.items()])\n",
    "                    postfix += f' ({trace_count_by_pref_str})'\n",
    "                \n",
    "                if sample_trace_filter_result == -1:\n",
    "                    custom_class = 'bg-info'\n",
    "                \n",
    "                elif sample_trace_filter_result == 0:\n",
    "                    custom_class = 'bg-warning'\n",
    "\n",
    "                else:\n",
    "                    custom_class = 'bg-success'\n",
    "\n",
    "            rows[SAMPLE_FITNESS].append(f'<div class=\"{custom_class}\"><strong>Sample fitness: {sample_fitness:.4f}{postfix}</strong></div>')\n",
    "\n",
    "            sample_ast_str = _format_ast_for_html(sample, line_delimiter=line_delimiter, increment=increment)\n",
    "            rows[SAMPLE_AST].append(sample_ast_str)\n",
    "\n",
    "        else:\n",
    "            rows[SAMPLE_FITNESS].append('No sample found')\n",
    "            rows[SAMPLE_AST].append('N/A')\n",
    "\n",
    "    return tabulate.tabulate(rows, headers='firstrow', tablefmt=tablefmt)\n",
    "\n",
    "\n",
    "\n",
    "def make_samples_only_game_table(model: MAPElitesSampler, sample_keys: typing.List[KeyTypeAnnotation],\n",
    "                                 trace_filter_data: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "                                 line_delimiter='<br>', increment='  ', tablefmt='unsafehtml'):\n",
    "\n",
    "    rows = defaultdict(list)\n",
    "\n",
    "    for sample_key in sample_keys:\n",
    "        key_dict = model._key_to_feature_dict(sample_key)\n",
    "        key_dict_str = '<br>'.join([f'<strong>Key {sample_key}:</strong>'] + [f'{feature_name}: {feature_value}' for feature_name, feature_value in key_dict.items()])\n",
    "        rows[GAME_KEY].append(key_dict_str)\n",
    "\n",
    "        sample = model.population[sample_key]\n",
    "        sample_fitness = model.fitness_values[sample_key]\n",
    "        custom_class = 'placeholder'\n",
    "        postfix = ''\n",
    "\n",
    "        if trace_filter_data is not None and sample_key in trace_filter_data['summary']:\n",
    "            sample_trace_filter_result = trace_filter_data['summary'][sample_key]\n",
    "            postfix = f' (# traces : {sample_trace_filter_result}{\"+\" if sample_trace_filter_result >= 5 else \"\"})'\n",
    "\n",
    "            if sample_trace_filter_result == 0:\n",
    "                full_trace_filter_result = trace_filter_data['full'][sample_key]\n",
    "                trace_count_by_pref = {pref: sum(1 for v in pref_counts.values() if v > 0) for pref, pref_counts in full_trace_filter_result.items()}\n",
    "                postfix += f' ({trace_count_by_pref})'\n",
    "            \n",
    "            if sample_trace_filter_result == -1:\n",
    "                custom_class = 'bg-info'\n",
    "            \n",
    "            elif sample_trace_filter_result == 0:\n",
    "                custom_class = 'bg-warning'\n",
    "\n",
    "            else:\n",
    "                custom_class = 'bg-success'\n",
    "\n",
    "        rows[SAMPLE_FITNESS].append(f'<div class=\"{custom_class}\"><strong>Sample fitness: {sample_fitness:.4f}{postfix}</strong></div>')\n",
    "\n",
    "        sample_ast_str = _format_ast_for_html(sample, line_delimiter=line_delimiter, increment=increment)\n",
    "        rows[SAMPLE_AST].append(sample_ast_str)\n",
    "\n",
    "    return tabulate.tabulate([rows[SAMPLE_FITNESS], rows[GAME_KEY], rows[SAMPLE_AST]], tablefmt=tablefmt)\n",
    "\n",
    "\n",
    "\n",
    "def make_representative_game_table_to_html(table_html: str, model_name: str, real_game_energy_range: typing.Tuple[float, float], output_path='representative_games.html'):\n",
    "    table_html = table_html.replace('<table>', '<table class=\"table table-striped table-bordered\">')\n",
    "    table_html = table_html.replace('<thead>', '<thead class=\"thead-dark\">')\n",
    "    table_html = table_html.replace('<th>', '<th scope=\"col\">')\n",
    "    for custom_class in CUSTOM_BACKGROUND_COLOR_CLASSES:\n",
    "        table_html = table_html.replace(f'<td><div class=\"{custom_class}\">', f'<td class=\"{custom_class}\"><div>')\n",
    "    \n",
    "    style = \"\"\"\n",
    "    <style>\n",
    "        .table td, .table th {\n",
    "            min-width: 40em;\n",
    "            max-width: 60em;\n",
    "        }\n",
    "        pre {\n",
    "            white-space: pre-wrap;\n",
    "            max-height: 60em;\n",
    "            overflow: auto;\n",
    "            display: inline-block;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    html_template = f\"\"\"\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\n",
    "        <title>Representative games comparison</title>\n",
    "        <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css\" integrity=\"sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm\" crossorigin=\"anonymous\">\n",
    "        {style}\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>\n",
    "            <h1>Representative games comparison for {model_name}</h1>\n",
    "            <h3>Real game energy range: min: {real_game_energy_range[0]:.4f}, max: {real_game_energy_range[1]:.4f}</h3>\n",
    "            Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            {table_html}\n",
    "        </div>\n",
    "        <script src=\"https://code.jquery.com/jquery-3.2.1.slim.min.js\" integrity=\"sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN\" crossorigin=\"anonymous\"></script>\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js\" integrity=\"sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q\" crossorigin=\"anonymous\"></script>\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js\" integrity=\"sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl\" crossorigin=\"anonymous\"></script>\n",
    "    </body>\n",
    "    \"\"\"\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(html_template)\n",
    "\n",
    "\n",
    "def make_and_save_representative_game_table(model, model_name, real_game_energy_range: typing.Tuple[float, float],\n",
    "                                            trace_filter_data: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "                                            representative_game_indices=REPRESENTATIVE_GAME_INDICES, \n",
    "                                            real_game_asts=game_asts, line_delimiter='<br>', increment='  ', tablefmt='unsafehtml'):\n",
    "    table_html = make_representative_game_table(model, trace_filter_data=trace_filter_data, representative_game_indices=representative_game_indices, \n",
    "                                                real_game_asts=real_game_asts, line_delimiter=line_delimiter, increment=increment, tablefmt=tablefmt)\n",
    "\n",
    "    prefix = 'representative_games' if representative_game_indices is not None else 'all_representative_games'\n",
    "    output_path = f'./output_htmls/{prefix}_{model.output_name}_{datetime.now().strftime(\"%Y_%m_%d\")}.html'\n",
    "    print(f'Saving representative games table to {os.path.abspath(output_path)}')\n",
    "    make_representative_game_table_to_html(table_html, model_name, real_game_energy_range, output_path)\n",
    "\n",
    "\n",
    "def plot_fitness_trajectory_and_make_game_table(model: MAPElitesSampler, title: str, \n",
    "                                                trace_filter_data: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "                                                representative_game_indices=REPRESENTATIVE_GAME_INDICES, \n",
    "                                                save_path: typing.Optional[str] = None, \n",
    "                                                make_table: bool = True, plot_metrics: typing.Optional[bool] = None):\n",
    "    min_real_game_fitness =  -1 * model.fitness_function.score_dict['max']\n",
    "    max_real_game_fitness = -1 * model.fitness_function.score_dict['min']\n",
    "    mean_real_game_fitness = -1 * model.fitness_function.score_dict['mean']\n",
    "    plot_sampler_fitness_trajectory(model, title, plot_metrics=plot_metrics,\n",
    "                                    min_real_game_fitness=min_real_game_fitness, \n",
    "                                    max_real_game_fitness=max_real_game_fitness, \n",
    "                                    mean_real_game_fitness=mean_real_game_fitness,\n",
    "                                    save_path=save_path)\n",
    "    if make_table:\n",
    "        make_and_save_representative_game_table(model, title, \n",
    "                                                real_game_energy_range=(min_real_game_fitness, max_real_game_fitness),\n",
    "                                                trace_filter_data=trace_filter_data,\n",
    "                                                representative_game_indices=representative_game_indices)\n",
    "\n",
    "\n",
    "def make_and_save_samples_game_table(model, model_name, sample_keys: typing.Optional[typing.List[KeyTypeAnnotation]] = None,\n",
    "                                     n_top_samples: int = 10, \n",
    "                                     trace_filter_data: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "                                     real_game_asts=game_asts, line_delimiter='<br>', increment='  ', tablefmt='unsafehtml'):\n",
    "    \n",
    "    if sample_keys is None:\n",
    "        real_game_fitness_features = [model._proposal_to_features(game) for game in real_game_asts]\n",
    "        cells_occupied = set([model._features_to_key(game, features) for game, features in zip(real_game_asts, real_game_fitness_features)])\n",
    "\n",
    "        sample_keys = []\n",
    "        fitness_values_and_keys = [(fitness, key) for key, fitness in model.fitness_values.items()]\n",
    "        fitness_values_and_keys.sort(reverse=True)\n",
    "\n",
    "        for _, key in fitness_values_and_keys:\n",
    "            if key not in cells_occupied:\n",
    "                sample_keys.append(key)\n",
    "                cells_occupied.add(key)\n",
    "\n",
    "            if len(sample_keys) >= n_top_samples:\n",
    "                break\n",
    "    \n",
    "    table_html = make_samples_only_game_table(model, sample_keys, trace_filter_data=trace_filter_data, \n",
    "                                              line_delimiter=line_delimiter, increment=increment, tablefmt=tablefmt)\n",
    "\n",
    "    prefix = 'map_elites_samples'\n",
    "    output_path = f'./output_htmls/{prefix}_{model.output_name}_{datetime.now().strftime(\"%Y_%m_%d\")}.html'\n",
    "    print(f'Saving representative games table to {os.path.abspath(output_path)}')\n",
    "\n",
    "    min_real_game_fitness =  -1 * model.fitness_function.score_dict['max']\n",
    "    max_real_game_fitness = -1 * model.fitness_function.score_dict['min']\n",
    "    make_representative_game_table_to_html(table_html, model_name, (min_real_game_fitness, max_real_game_fitness), output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_GAME_HEADER = 0\n",
    "REAL_GAME_TEXT = 1\n",
    "REAL_GAME = 2\n",
    "MAP_ELITES_HEADER = 3\n",
    "MAP_ELITES_TEXT = 4\n",
    "MAP_ELITES_GAME = 5\n",
    "\n",
    "\n",
    "LATEX_GAME_TEMPLATE = r\"\"\"\n",
    "\\begin{{lstlisting}}[aboveskip=-0.4 \\baselineskip,belowskip=-0.8 \\baselineskip]\n",
    "{ast_string}\n",
    "\\end{{lstlisting}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "LATEX_TABLE_TEMPLATE = r\"\"\"\n",
    "\\begin{{table}}[!htbp]\n",
    "    \\caption{{\\TODO[insert caption here]}}\n",
    "    \\begin{{adjustbox}}{{center}}\n",
    "    \\begin{{tabular}}{{|p{{\\gamecolumnwidth}}|p{{\\gamecolumnwidth}}|p{{\\gamecolumnwidth}}|}}\n",
    "    \\toprule\n",
    "    {table_text}\n",
    "    \\bottomrule\n",
    "    \\end{{tabular}}\n",
    "    \\end{{adjustbox}}\n",
    "    \\TODO[Insert any text below the table, if desired]\n",
    "    \\label{{tab:{table_label}}}\n",
    "\\end{{table}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "LATEX_DESCRIPTION_ONLY_TABLE_TEMPLATE = r\"\"\"\n",
    "\\begin{{table}}[!htbp]\n",
    "    \\caption{{\\TODO[insert caption here]}}\n",
    "    \\begin{{adjustbox}}{{center}}\n",
    "    \\begin{{tabular}}{{|p{{\\textcolumnwidth}}|p{{\\textcolumnwidth}}|}}\n",
    "    \\toprule\n",
    "    \\textbf{{Real Games}} & \\textbf{{MAP-Elites Samples}} \\\\\n",
    "    \\midrule\n",
    "    {table_text}\n",
    "    \\bottomrule\n",
    "    \\end{{tabular}}\n",
    "    \\end{{adjustbox}}\n",
    "    \\TODO[Insert any text below the table, if desired]\n",
    "    \\label{{tab:{table_label}}}\n",
    "\\end{{table}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "LATEX_SAMPLE_DESCRIPTION_ONLY_TABLE_TEMPLATE = r\"\"\"\n",
    "\\begin{{table}}[!htbp]\n",
    "    \\caption{{\\TODO[insert caption here]}}\n",
    "    \\begin{{adjustbox}}{{center}}\n",
    "    \\begin{{tabular}}{{|p{{\\textcolumnwidth}}|}}\n",
    "    \\toprule\n",
    "    \\textbf{{Novel MAP-Elites Samples}} \\\\\n",
    "    \\midrule\n",
    "    {table_text}\n",
    "    \\bottomrule\n",
    "    \\end{{tabular}}\n",
    "    \\end{{adjustbox}}\n",
    "    \\TODO[Insert any text below the table, if desired]\n",
    "    \\label{{tab:{table_label}}}\n",
    "\\end{{table}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def _format_ast_for_latex(ast):\n",
    "    ast_lines = ast_to_lines(ast, increment='  ')\n",
    "    reformatted_lines = []\n",
    "    buffer = []\n",
    "\n",
    "    combining_lines = False\n",
    "    for line in ast_lines:\n",
    "        current_line_is_only_paren = line.strip() == ')'\n",
    "\n",
    "        if current_line_is_only_paren:\n",
    "            combining_lines = True\n",
    "            buffer.append(line)\n",
    "\n",
    "        elif combining_lines:\n",
    "            combining_lines = False\n",
    "            new_line = buffer[-1].replace(')', ')' * len(buffer))\n",
    "            reformatted_lines.append(new_line)\n",
    "\n",
    "            reformatted_lines.append(line)\n",
    "\n",
    "            buffer = []\n",
    "\n",
    "        else:\n",
    "            reformatted_lines.append(line)       \n",
    "\n",
    "    if len(buffer) > 0:\n",
    "        new_line = buffer[-1].replace(')', ')' * len(buffer))\n",
    "        reformatted_lines.append(new_line)\n",
    "\n",
    "    ast_string = '\\n'.join(reformatted_lines)\n",
    "\n",
    "    return LATEX_GAME_TEMPLATE.format(ast_string=ast_string)\n",
    "\n",
    "\n",
    "def make_representative_game_table_latex(model, table_label: str,\n",
    "                                            trace_filter_data: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "                                            real_game_descriptions: typing.Optional[typing.Dict[int, str]] = None,\n",
    "                                            map_elites_sample_descriptions: typing.Optional[typing.Dict[int, str]] = None,\n",
    "                                            representative_game_indices=REPRESENTATIVE_GAME_INDICES, \n",
    "                                            real_game_asts=game_asts, description_text_size: str = 'tiny'):\n",
    "    \n",
    "    rows = defaultdict(list)\n",
    "    \n",
    "    for idx in representative_game_indices:\n",
    "        real_game_ast = real_game_asts[idx] \n",
    "        real_game_fitness_score, real_game_features = model._score_proposal(real_game_ast, return_features=True)\n",
    "\n",
    "        real_game_key = model._features_to_key(real_game_ast, real_game_features)\n",
    "        rows[REAL_GAME_HEADER].append(f'\\\\textbf{{Real Game \\\\#{idx} (fitness {real_game_fitness_score:.4f})}}')\n",
    "        rows[REAL_GAME_TEXT].append(r'\\TODO[describe real game here]' if real_game_descriptions is None else f'{{\\\\{description_text_size} {real_game_descriptions[idx]} }}')\n",
    "        rows[REAL_GAME].append(_format_ast_for_latex(real_game_ast))\n",
    "\n",
    "        map_elites_game = model.population[real_game_key]\n",
    "        map_elites_fitness_score = model.fitness_values[real_game_key]\n",
    "        rows[MAP_ELITES_HEADER].append(f'\\\\textbf{{MAP-Elites Sample (fitness {map_elites_fitness_score:.4f})}}')\n",
    "        rows[MAP_ELITES_TEXT].append(r'\\TODO[describe MAP-Elites sample here]' if map_elites_sample_descriptions is None else f'{{\\\\{description_text_size} {map_elites_sample_descriptions[idx]} }}')\n",
    "        rows[MAP_ELITES_GAME].append(_format_ast_for_latex(map_elites_game))\n",
    "\n",
    "    table_rows = []\n",
    "    table_rows.append(' & '.join(rows[REAL_GAME_HEADER]) + r' \\\\')\n",
    "    table_rows.append('\\\\midrule')\n",
    "    table_rows.append(' & '.join(rows[REAL_GAME_TEXT]) + r' \\\\')\n",
    "    table_rows.append(' & '.join(rows[REAL_GAME]) + r' \\\\')\n",
    "    table_rows.append('\\\\midrule')\n",
    "    table_rows.append(' & '.join(rows[MAP_ELITES_HEADER]) + r' \\\\')\n",
    "    table_rows.append('\\\\midrule')\n",
    "    table_rows.append(' & '.join(rows[MAP_ELITES_TEXT]) + r' \\\\')\n",
    "    table_rows.append(' & '.join(rows[MAP_ELITES_GAME]) + r' \\\\')\n",
    "    \n",
    "    if table_label.startswith('tab:'):\n",
    "        table_label = table_label[4:]\n",
    "\n",
    "    table_text = '\\n'.join(table_rows)\n",
    "    return LATEX_TABLE_TEMPLATE.format(table_text=table_text, table_label=table_label)\n",
    "    \n",
    "\n",
    "def make_samples_only_table_latex(model, table_label: str, sample_keys: typing.List[KeyTypeAnnotation], \n",
    "                                  map_elites_sample_descriptions: typing.Optional[typing.Dict[KeyTypeAnnotation, str]] = None,\n",
    "                                  description_text_size: str = 'tiny'):\n",
    "    \n",
    "\n",
    "    rows = defaultdict(list)\n",
    "    for key in sample_keys:\n",
    "        sample = model.population[key]\n",
    "        fitness = model.fitness_values[key]\n",
    "        rows[0].append(f'\\\\textbf{{MAP-Elites Sample (fitness {fitness:.4f})}}')\n",
    "        rows[1].append(r'\\TODO[describe sample here]' if map_elites_sample_descriptions is None else f'{{\\\\{description_text_size} {map_elites_sample_descriptions[key]} }}')\n",
    "        rows[2].append(_format_ast_for_latex(sample))\n",
    "\n",
    "    table_rows = []\n",
    "    table_rows.append(' & '.join(rows[0]) + r' \\\\')\n",
    "    table_rows.append('\\\\midrule')\n",
    "    table_rows.append(' & '.join(rows[1]) + r' \\\\')\n",
    "    table_rows.append(' & '.join(rows[2]) + r' \\\\')\n",
    "\n",
    "    if table_label.startswith('tab:'):\n",
    "        table_label = table_label[4:]\n",
    "\n",
    "    table_text = '\\n'.join(table_rows)\n",
    "    return LATEX_TABLE_TEMPLATE.format(table_text=table_text, table_label=table_label)\n",
    "\n",
    "\n",
    "\n",
    "def make_descriptions_only_table_latex(table_label: str,\n",
    "        real_game_descriptions: typing.Dict[int, str],\n",
    "        map_elites_sample_descriptions: typing.Dict[int, str],\n",
    "        text_size: str = 'tiny',):\n",
    "    \n",
    "    midrule = \"\\\\midrule\"\n",
    "\n",
    "    rows = [\n",
    "        f'{{ \\\\{text_size} {real_game_descriptions[index]} }} &  {{ \\\\{text_size} {map_elites_sample_descriptions[index]} }} \\\\\\\\ {midrule if i < len(real_game_descriptions) - 1 else \"\"}'\n",
    "        for i, index in enumerate(real_game_descriptions)\n",
    "    ]\n",
    "\n",
    "    table_text = '\\n'.join(rows)\n",
    "    return LATEX_DESCRIPTION_ONLY_TABLE_TEMPLATE.format(table_text=table_text, table_label=table_label)\n",
    "\n",
    "\n",
    "\n",
    "def make_samples_only_descriptions_only_table(table_label: str,\n",
    "                                              map_elites_sample_descriptions: typing.Dict[KeyTypeAnnotation, str],\n",
    "                                              text_size: str = 'tiny'):\n",
    "    \n",
    "    midrule = \"\\\\midrule\"\n",
    "\n",
    "    rows = [\n",
    "        f'{{ \\\\{text_size} {desc} }} \\\\\\\\ {midrule if i < len(map_elites_sample_descriptions) - 1 else \"\"}'\n",
    "        for i, (key, desc) in enumerate(map_elites_sample_descriptions.items())\n",
    "    ]\n",
    "\n",
    "    table_text = '\\n'.join(rows)\n",
    "    return LATEX_SAMPLE_DESCRIPTION_ONLY_TABLE_TEMPLATE.format(table_text=table_text, table_label=table_label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FITNESS_COMPARISON_SCATTER_KWARGS = dict()\n",
    "FITNESS_COMPARISON_LINE_KWARGS = dict()\n",
    "FITNESS_COMPARISON_SUBPLOT_ADJUST_KWARGS = dict(hspace=0.3, wspace=0.1)\n",
    "\n",
    "\n",
    "def plot_fitness_comparison_across_bins(\n",
    "    models: typing.List[MAPElitesSampler],\n",
    "    real_game_bins_only: bool = True,\n",
    "    real_game_asts: typing.List[tuple] = game_asts,  # type: ignore\n",
    "    n_rows: typing.Optional[int] = None,\n",
    "    n_cols: typing.Optional[int] = None,\n",
    "    color_by_margin: bool = True,\n",
    "    underperforming_bin_threshold: int = 2,\n",
    "    color_in_margin: str = 'purple',\n",
    "    color_outside_margin: str = 'orange',\n",
    "    width_per_bin: float = 2,\n",
    "    height_per_bin: float = 2.5,\n",
    "    baseline_fontsize: int = 8,\n",
    "    scatter_kwargs: typing.Optional[typing.Dict[str, typing.Any]] = FITNESS_COMPARISON_SCATTER_KWARGS,\n",
    "    line_kwargs: typing.Optional[typing.Dict[str, typing.Any]] = FITNESS_COMPARISON_LINE_KWARGS,\n",
    "    subplot_adjust_kwargs: typing.Optional[typing.Dict[str, typing.Any]] = FITNESS_COMPARISON_SUBPLOT_ADJUST_KWARGS, \n",
    "):\n",
    "    if scatter_kwargs is None:\n",
    "        scatter_kwargs = {}\n",
    "\n",
    "    if line_kwargs is None:\n",
    "        line_kwargs = {}\n",
    "\n",
    "    if subplot_adjust_kwargs is None:\n",
    "        subplot_adjust_kwargs = {}\n",
    "    \n",
    "    real_game_fitness_scores, real_game_features = zip(*[models[0]._score_proposal(game, return_features=True) for game in real_game_asts])  # type: ignore\n",
    "    real_game_iqr = np.percentile(real_game_fitness_scores, 75) - np.percentile(real_game_fitness_scores, 25)\n",
    "    real_game_keys = [models[0]._features_to_key(game, features) for game, features in zip(real_game_asts, real_game_features)]\n",
    "    bins_to_real_game_indices = defaultdict(list)\n",
    "    for idx, real_game_key in enumerate(real_game_keys):\n",
    "        bins_to_real_game_indices[real_game_key].append(idx)\n",
    "\n",
    "    if real_game_bins_only:\n",
    "        n_bins = len(bins_to_real_game_indices)\n",
    "        bins = list(sorted(bins_to_real_game_indices.keys()))\n",
    "        \n",
    "    else:\n",
    "        n_bins_by_model = [len(model.population) for model in models]\n",
    "        max_poulation_model_index = np.argmax(n_bins_by_model)\n",
    "        n_bins = n_bins_by_model[max_poulation_model_index]\n",
    "        bins = list(sorted(models[max_poulation_model_index].population.keys()))\n",
    "\n",
    "    if n_rows is not None and n_cols is not None:\n",
    "        assert n_rows * n_cols >= n_bins, f'Not enough rows and columns to fit {n_bins} bins'\n",
    "    elif n_rows is not None:\n",
    "        n_cols = math.ceil(n_bins / n_rows)\n",
    "    elif n_cols is not None:\n",
    "        n_rows = math.ceil(n_bins / n_cols)\n",
    "    else:\n",
    "        n_cols = math.ceil(math.sqrt(n_bins))\n",
    "        n_rows = math.ceil(n_bins / n_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * width_per_bin, n_rows * height_per_bin))\n",
    "\n",
    "    underperforming_bins = []\n",
    "\n",
    "    for bin_idx, bin_key in enumerate(bins):\n",
    "        ax = axes[bin_idx // n_cols, bin_idx % n_cols]\n",
    "        title = f'({\",\".join([str(x) for x in bin_key])})' if isinstance(bin_key, tuple) else str(bin_key)\n",
    "        \n",
    "        ax.set_yticks([])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=baseline_fontsize)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=baseline_fontsize - 2)\n",
    "\n",
    "        bin_fitness_values = [model.fitness_values[bin_key] for model in models]\n",
    "        \n",
    "        max_real_game_bin_fitness = None\n",
    "        if bin_key in bins_to_real_game_indices:\n",
    "            max_real_game_bin_fitness = max(real_game_fitness_scores[real_game_index] for real_game_index in bins_to_real_game_indices[bin_key])\n",
    "            for real_game_index in bins_to_real_game_indices[bin_key]:\n",
    "                real_game_fitness = real_game_fitness_scores[real_game_index]\n",
    "                ax.axvline(real_game_fitness, alpha=0.75 if real_game_fitness == max_real_game_bin_fitness else 0.25, **line_kwargs)\n",
    "\n",
    "        if color_by_margin and max_real_game_bin_fitness is not None:\n",
    "            # iqr_count = len([fitness for fitness in bin_fitness_values if fitness >= max_real_game_bin_fitness - real_game_iqr])\n",
    "            # title += f' (IQR: {iqr_count}/{len(bin_fitness_values)})'\n",
    "            games_within_margin = [fitness >= max_real_game_bin_fitness - real_game_iqr for fitness in bin_fitness_values]\n",
    "            if len(games_within_margin) - sum(games_within_margin) >= underperforming_bin_threshold:\n",
    "                underperforming_bins.append(bin_key)\n",
    "\n",
    "            c =  [color_in_margin if within_margin else color_outside_margin for within_margin in games_within_margin]\n",
    "        else:\n",
    "            c = color_outside_margin\n",
    "\n",
    "        ax.scatter(bin_fitness_values, np.arange(1, len(models) + 1), c=c, **scatter_kwargs)\n",
    "\n",
    "        ax.set_title(title, fontsize=baseline_fontsize + 2)\n",
    "\n",
    "    x_mins, x_maxes = zip(*[axes[bin_idx // n_cols, bin_idx % n_cols].get_xlim() for bin_idx in range(len(bins))])\n",
    "    x_min = min(x_mins)\n",
    "    x_max = max(x_maxes)\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "\n",
    "    for spare_bin_idx in range(len(bins), n_rows * n_cols):\n",
    "        ax = axes[spare_bin_idx // n_cols, spare_bin_idx % n_cols]\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(**subplot_adjust_kwargs)\n",
    "    plt.show()\n",
    "    return underperforming_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, names = [], []\n",
    "\n",
    "for i, (model_key, model_spec) in enumerate(latest_model_paths.MAP_ELITES_MODELS.items()):\n",
    "    if i < 2:\n",
    "        continue\n",
    "    \n",
    "    model = typing.cast(MAPElitesSampler, model_spec.load())\n",
    "    trace_filter_data = model_spec.load_trace_filter_data()\n",
    "    if trace_filter_data is not None:\n",
    "        summary_counts = Counter(trace_filter_data['summary'].values())\n",
    "        summary_counts_str = ', '.join([f'{k}: {v}' for k, v in sorted(summary_counts.items())])\n",
    "        print(f'{model_spec.name}: {summary_counts_str}')\n",
    "\n",
    "    # plot_fitness_trajectory_and_make_game_table(model, model_spec.name, \n",
    "    #     trace_filter_data=trace_filter_data,\n",
    "    #     representative_game_indices=NEW_REPRESENTATIVE_INDICES)\n",
    "\n",
    "    \n",
    "    models.append(model)\n",
    "    names.append(model_spec.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_RATES = {\n",
    "    'gen_regrowth_sample': 0.5 / 5,\n",
    "    'insert': 0.5 / 5, \n",
    "    'delete': 0.5 / 5,\n",
    "    'crossover': 0.5 / 5,\n",
    "    'crossover_insert': 0.5 / 5,\n",
    "    'resample_variable_types': 0.4 / 5,\n",
    "    'resample_first_condition': 0.4 / 5,\n",
    "    'resample_last_condition': 0.4 / 5,\n",
    "    'resample_variable_types_and_first_condition': 0.4 / 5,\n",
    "    'resample_variable_types_and_last_condition': 0.4 / 5,\n",
    "    'sample_or_resample_setup': 0.1 / 4,\n",
    "    'sample_or_resample_terminal': 0.1 / 4,\n",
    "    'resample_scoring': 0.1 / 4,\n",
    "    '_crossover_full_sections': 0.1 / 4,\n",
    "}\n",
    "\n",
    "BASE_RATES.update({f'_{key}': value for key, value in BASE_RATES.items()})\n",
    "\n",
    "\n",
    "def plot_success_by_generation(model, normalize: bool = False, k: int = 5, normalize_base_rates: bool = True,\n",
    "                               base_rates: typing.Optional[typing.Dict[str, float]] = BASE_RATES, \n",
    "                               cmap: str = 'tab10',\n",
    "                               figsize: typing.Tuple[int, int] = (16, 12), fontsize: int = 16):\n",
    "    all_keys = set()\n",
    "    for step_success in model.success_by_generation_and_operator:\n",
    "        all_keys.update(step_success.keys())\n",
    "\n",
    "    all_keys = list(sorted(all_keys))\n",
    "    successes_by_ket = {key: np.zeros(len(model.success_by_generation_and_operator)) for key in all_keys}\n",
    "\n",
    "    for i, step_success in enumerate(model.success_by_generation_and_operator):\n",
    "        if normalize:\n",
    "            if normalize_base_rates:\n",
    "                step_success = {key: value / base_rates[key] for key, value in step_success.items()}\n",
    "\n",
    "            step_sum = sum(step_success.values())\n",
    "            step_success = {key: value / step_sum for key, value in step_success.items()}\n",
    "\n",
    "        for key in all_keys:\n",
    "            successes_by_ket[key][i] = step_success.get(key, 0)\n",
    "\n",
    "    kernel = np.ones(k) / k\n",
    "    colormap = plt.cm.get_cmap(cmap)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for key in all_keys:\n",
    "        smoothed_successes = np.convolve(successes_by_ket[key], kernel, mode='same')\n",
    "        updated_key = key.strip('_')\n",
    "        plt.plot(smoothed_successes, label=updated_key, alpha=0.75, color=colormap(all_keys.index(key)))\n",
    "\n",
    "    plt.xlabel('Generation', fontsize=fontsize)\n",
    "    plt.ylabel('Successes' if not normalize else 'Proportion of successes', fontsize=fontsize)\n",
    "    plt.legend(loc='best', fontsize=fontsize)\n",
    "    plt.gca().tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_success_by_generation(models[-1], normalize=True, normalize_base_rates=False, k=50, cmap='tab20')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[-1]\n",
    "key_to_real_game_index = defaultdict(list)\n",
    "real_game_keys = []\n",
    "for i, ast in enumerate(game_asts):\n",
    "    features = model._proposal_to_features(ast)\n",
    "    key = model._features_to_key(ast, features)\n",
    "    key_to_real_game_index[key].append(i)\n",
    "    real_game_keys.append(key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_real_game_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_filter_results = utils.load_data_from_path('../samples/trace_filter_results_max_exemplar_preferences_by_bcs_with_expected_values_2023_11_29_2023_12_05_1.pkl.gz')\n",
    "trace_filter_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for key in key_to_real_game_index:\n",
    "    if key in trace_filter_results['summary'] and trace_filter_results['summary'][key] == 0:\n",
    "        print('*', key, [k for k, v in trace_filter_results['full'][key].items() if len(v) == 0])\n",
    "        count += 1\n",
    "\n",
    "print(count, len(key_to_real_game_index), count / len(key_to_real_game_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_entry = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for key, score in trace_filter_results['summary'].items():\n",
    "    if score == 0:\n",
    "        for i, entry in enumerate(key):\n",
    "            count_by_entry[i][entry] += 1\n",
    "\n",
    "for i, counts in count_by_entry.items():\n",
    "    print(i, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_count_counts = defaultdict(int)\n",
    "for key in models[-1].population:\n",
    "    preference_count_counts[key[2]] += 1\n",
    "\n",
    "for key, count in preference_count_counts.items():\n",
    "    print(key, count, count_by_entry[2][key] / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPRESENTATIVE_GAME_INDICES = [26, 58, 42, 31, 48, 19, 14, 62, 93] \n",
    "\n",
    "real_game_fitness_scores, real_game_fitness_features = zip(*[model._score_proposal(game, return_features=True) for game in game_asts])  # type: ignore\n",
    "real_game_fitness_score_indices = np.argsort(real_game_fitness_scores)[::-1]  # type: ignore \n",
    "\n",
    "relevant_indices = [-2, -1]\n",
    "\n",
    "for key, indices in key_to_real_game_index.items():\n",
    "    if len(relevant_indices) > 0:\n",
    "        if key is None or not any(key[idx] for idx in relevant_indices):\n",
    "            continue\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        formatted_indices = []\n",
    "        for idx in indices: \n",
    "            real_game_rank = np.where(real_game_fitness_score_indices == idx)[0][0]\n",
    "            game_in_representative_games = idx in REPRESENTATIVE_GAME_INDICES\n",
    "            marker = \"*\" if game_in_representative_games else \"\"\n",
    "            formatted_indices.append((real_game_rank, f'{marker}{idx}{marker} (rank: {real_game_rank})'))\n",
    "\n",
    "        formatted_indices.sort()\n",
    "\n",
    "        print(f'{key}: {\", \".join(t[1] for t in formatted_indices)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEW_REPRESENTATIVE_INDICES = [\n",
    "    42,  # throw\n",
    "    58,  # funny throw with setup\n",
    "    28,  # throw, throw attempt, with setup \n",
    "    88,  # throw, throw attempt, no setup\n",
    "    31,  # throw without in/on\n",
    "    64,  # dropping ball in bin, drop attempt\n",
    "    52,  # ball-less throwing game\n",
    "    86,  # weird throw, with setup\n",
    "    6,  # two weird throwing preferences, setup\n",
    "    17,  # anoter weird multi throwing preference game\n",
    "    96,  # three throwing preferences, setup\n",
    "    \n",
    "    14,  # castle building\n",
    "    45,  # weird building game\n",
    "    49,  # another weird building game\n",
    "    51,  # hybrid throwing/building game\n",
    "    \n",
    "    23,  # single placement preference\n",
    "    44,  # three different placement preferencs \n",
    "\n",
    "    26,  # single preferece that matches none of the others\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast_printer.ast_to_string(game_asts[64], '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_game_descriptions = {\n",
    "    14: \"Build a `castle' by grabbing a bridge block, a flat block, a tall cylindrical block, a cube block, and a pyramid block, and stack them on top of each other in that order\",\n",
    "    42: \"Throw dodgeballs into the bin. You get a point for each made throw.\",\n",
    "    62: \"Set up the game by placing the hexagonal bin near the center of the room and all dodgeballs on the desk. To play, throw dodgeballs into the bin while standing next to the desk. You get a point for each made throw.\",\n",
    "    93: \"Set up the game by placing the dodgeball near the center of the room. To play the game, make a building on top of the doggie bed, without objects touching the floor or a wall. You get a point for each object in the building.\",\n",
    "}\n",
    "\n",
    "map_elites_sample_descriptions = {\n",
    "    14: \"Make buildings by grabbing a green bridge block, and placing any block and a pyramid block on it. The game ends once you make 12 such structures, and you get a point for each.\",\n",
    "    42: \"Throw beachballs into the bin. The game ends after you've made 22 throws or 5 throws with different beachballs. You get a point for each made throw.\",\n",
    "    62: \"Set up the game by placing the hexagonal bin very close to the top shelf. To play, put pink dodgeballs on the floor. Also, put the hexagonal bin upside down, and then flip to not be upside down without touching it or holding it. The game ends after you've put 8 pink dodgeballs on the floor or flipped the bin over 16 times. You get a point for each bin flip.\",\n",
    "    93: \"Set up the game by placing the hexagonal bin very close to the east wall. To play, put blue cube blocks on shelves adjacent to the west wall, and place orange objects closer to the rug than to the door. The game ends after you've placed 23 blue cube blocks or 20 different orange objects, and you get a point for each blue cube block placed.\"\n",
    "}\n",
    "\n",
    "\n",
    "print(make_representative_game_table_latex(\n",
    "    all_ngrams_uniform_new_bc_l2_model, 'comparison', \n",
    "    real_game_descriptions=real_game_descriptions,\n",
    "    map_elites_sample_descriptions=map_elites_sample_descriptions,\n",
    "    representative_game_indices=[42, 14, 93]))  # [42, 14, 62, 93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_descriptions_only_table_latex('comparison-descriptions', real_game_descriptions, map_elites_sample_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_sample_keys = [\n",
    "    (1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1),\n",
    "    (1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0),\n",
    "    (1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1),\n",
    "    (1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0),\n",
    "    (1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1),\n",
    "    (0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1),\n",
    "    (1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1),\n",
    "]\n",
    "\n",
    "make_and_save_samples_game_table(all_ngrams_uniform_new_bc_l2_model, all_ngrams_uniform_new_bc_l2_name, sample_keys=candidate_sample_keys, n_top_samples=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_elites_novel_sample_descriptions = {\n",
    "    (1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1): \"Set up the game by placing the hexagonal bin very close to the rug. To play, start with it being in a diagonal orientation, and without touching it or holding it, get it to a different orientation. The game ends after you do so 14 times, and you get a point for each.\",\n",
    "    # (1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1): \"Throw yellow cube blocks. The game ends after you do so 10 times, and you get a point for each time.\",\n",
    "    (1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0): \"Place pillows on the bed and throw either dodgeballs or beach balls into the bin. The game ends after 5 succesful throws, and you get a point for each pillow on the bed.\",\n",
    "    (1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1): \"Find sets of four objects in the room, such that the second and third are the same color, the first is next to the second, and the first is inside the fourth. Additionally, throw dodgeballs or golf balls. The game ends after you've accomplished the first criterion twice, or thrown 3 different dodgeballs or golf balls. You get a point for each time you accomplish the first criterion.\",\n",
    "}\n",
    "\n",
    "\n",
    "print(make_samples_only_table_latex(all_ngrams_uniform_new_bc_l2_model, 'novel-samples', list(map_elites_novel_sample_descriptions.keys()), map_elites_novel_sample_descriptions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_samples_only_descriptions_only_table('novel-sample-texts', map_elites_novel_sample_descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [r'{\\small', r'\\begin{itemize}']\n",
    "for desc in fitness_features_by_category.FITNESS_FEATURE_DESCRIPTIONS:\n",
    "    lines.append('\\t' + desc.to_latex())\n",
    "\n",
    "lines.append(r'\\end{itemize}')\n",
    "lines.append('}')\n",
    "\n",
    "print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitness_features_by_category import FEATURE_CATEGORIES\n",
    "\n",
    "ignore_categories = [\n",
    "    \"forall_less_important\", \"counting_less_important\", \n",
    "    \"grammar_use_less_important\", \"predicate_under_modal\", \n",
    "    \"predicate_role_filler\", \"compositionality\"\n",
    "]\n",
    "\n",
    "fitness_model_paths = [\n",
    "    'in_data_prop_categories_full_seed_42_2023_11_22',\n",
    "    'in_data_prop_L2_categories_full_seed_42_2023_11_22',\n",
    "    'in_data_prop_categories_full_seed_42_2023_11_23',\n",
    "    'in_data_prop_L2_categories_full_seed_42_2023_11_23',\n",
    "]\n",
    "\n",
    "loaded_models = [utils.load_model_and_feature_columns(model_path) for model_path in fitness_model_paths]\n",
    "table_models, table_feature_names = zip(*loaded_models)\n",
    "table_names = ['No Regularization (before)', 'L2 (before)', 'No Regularization (after)', 'L2 (after)']\n",
    "\n",
    "all_feature_names = set()\n",
    "for feature_names in table_feature_names:\n",
    "    all_feature_names.update(feature_names)\n",
    "\n",
    "\n",
    "all_ignore_features = set()\n",
    "for category in ignore_categories:\n",
    "    for feature in FEATURE_CATEGORIES[category]:\n",
    "        if isinstance(feature, re.Pattern):\n",
    "            all_ignore_features.update([f for f in all_feature_names if feature.match(f)])\n",
    "        else:\n",
    "            all_ignore_features.add(feature)\n",
    "\n",
    "use_absolute_values = True\n",
    "\n",
    "weights_by_model = {}\n",
    "weight_ranks_by_model = {}\n",
    "\n",
    "\n",
    "for model, name, feature_names in zip(table_models, table_names, table_feature_names):\n",
    "    model_weights = model.named_steps['fitness'].model.fc1.weight.data.detach().squeeze()  # type: ignore\n",
    "    if use_absolute_values:\n",
    "        model_weights = torch.abs(model_weights)\n",
    "    model_weights_rank = stats.rankdata(model_weights.numpy())\n",
    "    if use_absolute_values:\n",
    "        model_weights_rank = len(model_weights_rank) - model_weights_rank + 1\n",
    "    weights_by_model[name] = {feature_names[i]: model_weights[i].item() for i in range(len(feature_names))}\n",
    "    weight_ranks_by_model[name] = {feature_names[i]: model_weights_rank[i] for i in range(len(feature_names))}\n",
    "\n",
    "\n",
    "feature_mean_rank = {\n",
    "    feature_name: np.nanmean([weights.get(feature_name, np.nan) for weights in weight_ranks_by_model.values()])\n",
    "    for feature_name in all_feature_names\n",
    "}\n",
    "\n",
    "\n",
    "mean_mean_rank_by_feature_number = defaultdict(list)\n",
    "for feature_name, mean_rank in feature_mean_rank.items():\n",
    "    if feature_name[-1].isdigit():\n",
    "        feature_name = feature_name[:-2]\n",
    "        mean_mean_rank_by_feature_number[feature_name].append(mean_rank)\n",
    "\n",
    "\n",
    "mean_rank_by_pref_forall_type = defaultdict(list)\n",
    "for feature_name, mean_rank in feature_mean_rank.items():\n",
    "    if 'pref_forall' in feature_name:\n",
    "        if feature_name.endswith('correct'):\n",
    "            mean_rank_by_pref_forall_type['correct'].append(mean_rank)\n",
    "\n",
    "        elif feature_name.endswith('incorrect'):\n",
    "            mean_rank_by_pref_forall_type['incorrect'].append(mean_rank)\n",
    "\n",
    "        elif feature_name.endswith('incorrect_count'):\n",
    "            mean_rank_by_pref_forall_type['incorrect_count'].append(mean_rank)\n",
    "\n",
    "\n",
    "feature_names_by_mean_rank = sorted(feature_mean_rank.keys(), key=lambda feature_name: feature_mean_rank[feature_name], reverse=False)\n",
    "\n",
    "headers = ['Feature', 'Ignored', 'Mean Rank'] + table_names\n",
    "rows = [[feature_name, 'Yes' if feature_name in all_ignore_features else 'No',  f'{feature_mean_rank[feature_name]:.3f}'] + [f'{weights_by_model[name].get(feature_name, np.nan):.3f} ({int(weight_ranks_by_model[name].get(feature_name, -1))})'   \n",
    "                          for name in table_names] \n",
    "        for feature_name in feature_names_by_mean_rank]\n",
    "\n",
    "# with open('temp_outputs/features_by_mean_weight.tsv', 'w') as f:\n",
    "#     f.write(tabulate.tabulate(rows, headers, tablefmt='tsv'))\n",
    "\n",
    "print(tabulate.tabulate(rows, headers, tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mean_ranks = [(np.mean(ranks), feature_name, len(ranks)) for feature_name, ranks in mean_mean_rank_by_feature_number.items()]\n",
    "mean_mean_ranks.sort()\n",
    "\n",
    "for mean_rank, feature_name, n_features in mean_mean_ranks:\n",
    "    print(f'{feature_name}: {mean_rank:.3f} ({n_features})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_pairwise_edit_distances(model: MAPElitesSampler):\n",
    "#     population_as_strings = {key: ast_printer.ast_to_string(ast) for key, ast in tqdm(model.population.items(), desc='Stringifying population')}  # type: ignore\n",
    "#     population_keys = list(model.population.keys())\n",
    "#     distances = []\n",
    "#     total_combinations = len(population_keys) * (len(population_keys) - 1) / 2\n",
    "#     for first_key, second_key in tqdm(itertools.combinations(population_keys, 2), desc='Computing pairwise distances', total=total_combinations):\n",
    "#         distances.append(_edit_distance(population_as_strings[first_key], population_as_strings[second_key]))\n",
    "\n",
    "#     return np.mean(distances), np.std(distances), np.std(distances) / np.sqrt(total_combinations), len(population_as_strings), len(distances)\n",
    "        \n",
    "\n",
    "# model_edit_distances = {model.output_name: compute_pairwise_edit_distances(model) for model in models}\n",
    "\n",
    "# for name, distances in model_edit_distances.items():\n",
    "#     print(f'{name}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific_object_model_edit_distances = {model.output_name: compute_pairwise_edit_distances(model) for model in specific_object_models}\n",
    "\n",
    "# for name, distances in specific_object_model_edit_distances.items():\n",
    "#     print(f'{name}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_pairwise_feature_distances(model: MAPElitesSampler, ord: int = 1):\n",
    "#     population_as_features = {key: model._proposal_to_features(ast) for key, ast in tqdm(model.population.items(), desc='Featurizing population')}  # type: ignore\n",
    "#     population_as_features = {key: np.array([features[n] for n in model.feature_names], dtype=float) for key, features in population_as_features.items()}\n",
    "#     population_keys = list(model.population.keys())\n",
    "#     distances = []\n",
    "#     total_combinations = len(population_keys) * (len(population_keys) - 1) / 2\n",
    "#     for first_key, second_key in tqdm(itertools.combinations(population_keys, 2), desc='Computing pairwise distances', total=total_combinations):\n",
    "#         distances.append(np.linalg.norm(population_as_features[first_key] - population_as_features[second_key], ord=ord))\n",
    "\n",
    "#     return np.mean(distances), np.std(distances), np.std(distances) / np.sqrt(total_combinations), len(population_as_features), len(distances)\n",
    "        \n",
    "\n",
    "# model_edit_distances = {model.output_name: compute_pairwise_feature_distances(model) for model in models}\n",
    "\n",
    "# for name, distances in model_edit_distances.items():\n",
    "#     print(f'{name}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific_object_model_edit_distances = {model.output_name: compute_pairwise_feature_distances(model) for model in specific_object_models}\n",
    "\n",
    "# for name, distances in specific_object_model_edit_distances.items():\n",
    "#     print(f'{name}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # models = [\n",
    "# #     predicate_and_object_groups_seed_33, \n",
    "# #     predicate_and_object_groups_seed_42,\n",
    "# #     predicate_and_object_groups_seed_66, \n",
    "# #     predicate_and_object_groups_seed_99\n",
    "# # ]\n",
    "\n",
    "\n",
    "# real_game_fitness_scores, real_game_features = zip(*[models[0]._score_proposal(game, return_features=True) for game in game_asts])  # type: ignore\n",
    "# real_game_keys = [models[0]._features_to_key(game, features) for game, features in zip(game_asts, real_game_features)]  # type: ignore\n",
    "# bins_to_real_game_indices = defaultdict(list)\n",
    "# for idx, real_game_key in enumerate(real_game_keys):\n",
    "#     bins_to_real_game_indices[real_game_key].append(idx)\n",
    "\n",
    "\n",
    "# # n_bins_by_model = [len(model.population) for model in models]\n",
    "# # max_poulation_model_index = np.argmax(n_bins_by_model)\n",
    "# # n_bins = n_bins_by_model[max_poulation_model_index]\n",
    "# # all_bins = list(sorted(models[max_poulation_model_index].population.keys()))\n",
    "\n",
    "# # scores_with_human_games = []\n",
    "# # scores_without_human_games = []\n",
    "\n",
    "# # for bin in all_bins:\n",
    "# #     target_list = scores_with_human_games if bin in bins_to_real_game_indices else scores_without_human_games\n",
    "# #     for model in models:\n",
    "# #         target_list.append(model.fitness_values[bin])\n",
    "\n",
    "# # print('With human games:', np.mean(scores_with_human_games), np.std(scores_with_human_games))\n",
    "# # print('Without human games:', np.mean(scores_without_human_games), np.std(scores_without_human_games))\n",
    "# # ttest_result = stats.ttest_ind(scores_with_human_games, scores_without_human_games)\n",
    "# # print('T-test:', ttest_result)\n",
    "\n",
    "\n",
    "# underperforming_bins = plot_fitness_comparison_across_bins(models, n_cols=7)\n",
    "# print(underperforming_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers = ['Underperforming Bins'] + [f'#{i + 1}' for i in range(len(underperforming_bins))] + ['Sum', 'Overall Count', '%']\n",
    "\n",
    "# underperforming_feature_counts = {}\n",
    "# overall_feature_counts = {}\n",
    "\n",
    "# rows = []\n",
    "# for i, feature_name in enumerate(models[0].map_elites_feature_names):\n",
    "#     underperforming_bin_values = [bin[i] for bin in underperforming_bins]\n",
    "#     underperforming_feature_counts[feature_name] = sum(underperforming_bin_values)\n",
    "#     overall_feature_counts[feature_name] = sum([bin[i] for bin in bins_to_real_game_indices.keys()])\n",
    "\n",
    "#     proportion = f'{underperforming_feature_counts[feature_name] / overall_feature_counts[feature_name]:.2f}'\n",
    "#     row = [feature_name.replace('|', ' OR ')] + underperforming_bin_values + [underperforming_feature_counts[feature_name], overall_feature_counts[feature_name], proportion]\n",
    "#     rows.append(row)\n",
    "    \n",
    "\n",
    "\n",
    "# table = tabulate.tabulate(rows, headers=headers, tablefmt='github')\n",
    "\n",
    "# display(Markdown(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for bin in underperforming_bins:\n",
    "#     models[0].print_key_features(bin)\n",
    "#     for game_index in bins_to_real_game_indices[bin]:\n",
    "#         ast = game_asts[game_index]\n",
    "#         ast_str = ast_printer.ast_to_string(ast, '\\n')\n",
    "#         display(Markdown(f'```pddl\\n{ast_str}\\n```\\n'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[-1]\n",
    "\n",
    "for i in range(200):\n",
    "    top_sample_key = model.top_sample_key(i)\n",
    "    if top_sample_key[1] == 1 and top_sample_key not in key_to_real_game_index:\n",
    "        game = model.population[top_sample_key]\n",
    "        constraints = game[3][1] if game[3][0] == '(:constraints' else game[4][1]\n",
    "        n_prefs = len(constraints.preferences)\n",
    "        if n_prefs <= 4:\n",
    "            print(i, top_sample_key, n_prefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = evo_sampler_map_elites_fitness_rank_no_at_end\n",
    "model = models[-2]\n",
    "index = 1\n",
    "n_features_on = None\n",
    "feature_keywords_to_print = None  # ['max_depth', 'mean_depth', 'node_count']\n",
    "n_similar_real_games_to_print = 3\n",
    "key_features = None  # dict(section_doesnt_exist_setup=0)\n",
    "\n",
    "sample_key = model.visualize_top_sample(index, feature_keywords_to_print=feature_keywords_to_print, n_features_on=n_features_on, \n",
    "    postprocess_sample=True, features=key_features)\n",
    "sample = model.population[sample_key] \n",
    "# print_nearest_real_games(sample, n_similar_real_games_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(models[-2].population.keys())\n",
    "[k for k in keys if k[:3] == (1, 0, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[-1]\n",
    "# sample_key = (1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0)\n",
    "# sample_key = (1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0)\n",
    "# sample_key = (1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0)\n",
    "\n",
    "sample_key = (1, 0, 4, 1, 0, 1, 0, 0, 0, 0, 0, 1)\n",
    "\n",
    "n_features_on = None\n",
    "feature_keywords_to_print = None  # ['max_depth', 'mean_depth', 'node_count']\n",
    "n_similar_real_games_to_print = 3\n",
    "key_features = None  # dict(section_doesnt_exist_setup=0)\n",
    "\n",
    "sample_key = model._visualize_sample_by_key(sample_key, display_overall_features=True,\n",
    "                                            feature_keywords_to_print=feature_keywords_to_print, postprocess_sample=True)\n",
    "sample = model.population[sample_key] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = model.postprocessor(sample)\n",
    "game_fitness, game_features = model._score_proposal(game, return_features=True)\n",
    "{k: v for k, v in game_features.items() if 'in_data' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fitness_featurizer.header_registry['predicate_found_in_data'].predicate_data_estimator.con.execute(\n",
    "\"\"\"\n",
    "SELECT arg_2_type, count(arg_2_type) FROM data\n",
    "WHERE predicate = 'on'\n",
    "AND arg_1_type in ('south_wall', 'north_wall', 'east_wall', 'west_wall') \n",
    "GROUP BY arg_2_type\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_game_str = \"\"\"\n",
    "(define (game evo-2044-199-1) (:domain few-objects-room-v1)\n",
    "(:setup\n",
    "  (forall (?v0 - ball)\n",
    "    (game-optional\n",
    "      (near bed ?v0)\n",
    "   )\n",
    " )\n",
    ")\n",
    "(:constraints\n",
    "  (and\n",
    "    (preference preference0\n",
    "      (exists (?v0 - dodgeball ?v1 - doggie_bed)\n",
    "        (then\n",
    "          (once (agent_holds ?v0))\n",
    "          (hold (and (not (agent_holds ?v0)) (in_motion ?v0)))\n",
    "          (once (and (not (in_motion ?v0)) (in ?v1 ?v0)))\n",
    "       )\n",
    "     )\n",
    "   )\n",
    "    (preference preference1\n",
    "      (exists (?v1 - dodgeball)\n",
    "        (then\n",
    "          (once (agent_holds ?v1))\n",
    "          (hold (and (not (agent_holds ?v1)) (in_motion ?v1)))\n",
    "          (once (not (in_motion ?v1)))\n",
    "       )\n",
    "     )\n",
    "   )\n",
    " )\n",
    ")\n",
    "(:scoring\n",
    "  (+ (count preference1) (* 80 (count preference0))\n",
    "    \n",
    " )\n",
    ")\n",
    ")\n",
    "\"\"\".strip()\n",
    "game = model.postprocessor(sample)\n",
    "game_fitness, game_features = model._score_proposal(game, return_features=True)\n",
    "game_features_tensor = model._features_to_tensor(game_features)\n",
    "\n",
    "edited_game = grammar_parser.parse(edited_game_str)\n",
    "edited_game_fitness, edited_game_features = model._score_proposal(edited_game, return_features=True)\n",
    "edited_game_features_tensor = model._features_to_tensor(edited_game_features)\n",
    "edited_key = model._features_to_key(edited_game, edited_game_features)\n",
    "\n",
    "if edited_key != sample_key:\n",
    "    print(f'Edited key {edited_key} does not match sample key {sample_key}')\n",
    "else:\n",
    "    print('Keys match!')\n",
    "\n",
    "print('\\nChanged features:')\n",
    "\n",
    "for feature_name in game_features:\n",
    "    if edited_game_features[feature_name] != game_features[feature_name]:\n",
    "        print(feature_name, game_features[feature_name], edited_game_features[feature_name])\n",
    "\n",
    "\n",
    "\n",
    "utils.evaluate_comparison_energy_contributions(\n",
    "    game_features_tensor, edited_game_features_tensor,\n",
    "    ast_printer.ast_to_string(game, '\\n'), edited_game_str,\n",
    "    model.fitness_function, model.feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast_printer.ast_to_string(sample, '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast_printer.ast_to_string(model._gen_regrowth_sample(sample, model.rng), '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '(preference preference1  (exists (?v0 - (either golfball dodgeball) ?v1 - hexagonal_bin)    (at-end      (in ?v1 ?v0)   ) ))'\n",
    "s2 = '(preference preference0  (exists (?v0 - (either golfball dodgeball) ?v1 - hexagonal_bin)    (at-end      (in ?v1 ?v0)   ) ))'\n",
    "strs = [s1, s2]\n",
    "cleaned_strs = []\n",
    "\n",
    "for s in strs:\n",
    "    pref_index = s.index('(preference')\n",
    "    space_index = s.index(' ', pref_index)\n",
    "    space_after_pref_name_index = s.index(' ', space_index + 1)\n",
    "    pref_without_name = s[space_after_pref_name_index:]\n",
    "    cleaned_strs.append(pref_without_name)\n",
    "\n",
    "print(len(set(cleaned_strs)))\n",
    "print(cleaned_strs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fitness_rank_predicates\n",
    "\n",
    "def real_games_to_keys(map_elites_sampler: MAPElitesSampler, real_asts: typing.List[tatsu.ast.AST]) -> typing.List[str]:\n",
    "    return set([map_elites_sampler._features_to_key(ast, map_elites_sampler._proposal_to_features(ast)) for ast in real_asts])\n",
    "    \n",
    "\n",
    "real_game_keys = real_games_to_keys(model, game_asts)\n",
    "high_quality_sample_keys = set([k for k, v in model.fitness_values.items() if v > 70])\n",
    "high_quality_no_real_game_keys = high_quality_sample_keys - real_game_keys\n",
    "print(len(real_game_keys), len(high_quality_sample_keys), len(real_game_keys.intersection(high_quality_sample_keys)), len(high_quality_no_real_game_keys))\n",
    "\n",
    "\n",
    "high_quality_no_real_game_keys = [t[0] for t in sorted([(k, model.fitness_values[k]) for k in high_quality_sample_keys if k not in real_game_keys], key=lambda x: x[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "sample_key = high_quality_no_real_game_keys[index]\n",
    "n_features_on = None\n",
    "feature_keywords_to_print = ['max_depth', 'mean_depth', 'node_count']\n",
    "n_similar_real_games_to_print = 3 \n",
    "key_features = dict(section_doesnt_exist_setup=0)\n",
    "\n",
    "sample_key = model._visualize_sample_by_key(sample_key, feature_keywords_to_print=feature_keywords_to_print, postprocess_sample=True)\n",
    "print_nearest_real_games(model.population[sample_key], n_similar_real_games_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_set_bits(n): \n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "     \n",
    "    return count\n",
    "\n",
    "\n",
    "key_bits_to_fitness = defaultdict(list) \n",
    "\n",
    "for sample_key, fitness in current_uniform.fitness_values.items():\n",
    "    key_bits_to_fitness[count_set_bits(sample_key)].append(fitness)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for n in sorted(key_bits_to_fitness):\n",
    "    scores = key_bits_to_fitness[n]\n",
    "    rows.append((n, len(scores), np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n",
    "display(Markdown(tabulate.tabulate(rows, headers=['n', 'count', 'fitness mean', 'fitness std'], tablefmt='github')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 70\n",
    "\n",
    "def count_set_bits(n):\n",
    " \n",
    "    count = 0\n",
    "    while (n):\n",
    "        n &= (n-1)\n",
    "        count+= 1\n",
    "     \n",
    "    return count\n",
    "\n",
    "\n",
    "key_bits_to_fitness = defaultdict(list) \n",
    "\n",
    "for sample_key, fitness in evo_sampler_map_elites_ucb.fitness_values.items():\n",
    "    if fitness > threshold:\n",
    "        key_bits_to_fitness[count_set_bits(sample_key)].append(fitness)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for n in sorted(key_bits_to_fitness):\n",
    "    scores = key_bits_to_fitness[n]\n",
    "    rows.append((n, len(scores), np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n",
    "display(Markdown(tabulate.tabulate(rows, headers=['n', 'count', 'fitness mean', 'fitness std'], tablefmt='github')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evolutionary_sampler_behavioral_features import build_behavioral_features_featurizer, BASIC_BINNED, BASIC_WITH_NODE_DEPTH\n",
    "\n",
    "\n",
    "featurizer = build_behavioral_features_featurizer(BASIC_WITH_NODE_DEPTH)\n",
    "_ = [featurizer.parse(current_thompson.population[current_thompson.top_sample_key(i)], 'interactive-beta.pddl', return_row=False) for i in range(1, 51)]\n",
    "d_generated = featurizer.to_df()\n",
    "d_generated.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = build_behavioral_features_featurizer(BASIC_WITH_NODE_DEPTH)\n",
    "_ = [featurizer.parse(game_asts[i], 'interactive-beta.pddl', return_row=False) for i in range(len(game_asts))]\n",
    "d_real = featurizer.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "generated_values = d_generated[featurizer.headers[4:-1]].values.T.astype(float)\n",
    "real_values = d_real[featurizer.headers[4:-1]].values.T.astype(float)\n",
    "generated_values += np.random.normal(0, 0.1, size=generated_values.shape)\n",
    "real_values += np.random.normal(0, 0.1, size=real_values.shape)\n",
    "ax.scatter(*real_values, s=10)\n",
    "ax.scatter(*generated_values, s=10)\n",
    "ax.set_xlabel('Node count')\n",
    "ax.set_ylabel('# Objects')\n",
    "ax.set_zlabel('# Predicates')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df.groupby('real')[[c for c in fitness_df.columns if 'length_of_then' in c]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../reward-machine/')\n",
    "\n",
    "import reward_machine_sample_filter\n",
    "trace_filter = reward_machine_sample_filter.TraceFinderASTParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_key, sample in models[1].population.items():\n",
    "    traces_by_key, expected_keys = trace_filter(sample)\n",
    "    if len(traces_by_key) == 0:\n",
    "        if all(key not in trace_filter.preferences_or_sections_with_implemented_predicates for key in expected_keys):\n",
    "            print(f'Key {sample_key} with fitness {models[1].fitness_values[sample_key]:.2f} has no traces because no predicates are implemented: {list(trace_filter.not_implemented_predicate_counts.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_key, sample in enumerate(game_asts):\n",
    "    traces_by_key, expected_keys = trace_filter(sample)\n",
    "    if len(traces_by_key) == 0:\n",
    "        if all(key not in trace_filter.preferences_or_sections_with_implemented_predicates for key in expected_keys):\n",
    "            print(f'Key {sample_key} with fitness {models[1].fitness_values[sample_key]:.2f} has no traces because no predicates are implemented: {list(trace_filter.not_implemented_predicate_counts.keys())}')\n",
    "        else:\n",
    "            print(f'Key {sample_key} with fitness {models[1].fitness_values[sample_key]:.2f} has no traces because no traces satisfy the implemented predicates: {list(trace_filter.not_implemented_predicate_counts.keys())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_machine_trace_filter import *\n",
    "trace_filter = TraceGameEvaluator(FULL_DATASET_TRACES_HASH, models[2].population, 'foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import collections\n",
    "\n",
    "ignore_keys = set(['fitness_function'])\n",
    "\n",
    "def find_instances(o, cls, path=None, visited=None):\n",
    "    if path is None:\n",
    "        path = []\n",
    "\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if id(o) in visited:\n",
    "        return\n",
    "\n",
    "    visited.add(id(o))\n",
    "\n",
    "    if isinstance(o, cls):\n",
    "        print('->'.join(path))\n",
    "\n",
    "    if isinstance(o, dict):\n",
    "        for k, v in o.items():\n",
    "            if k not in ignore_keys:\n",
    "                find_instances(v, cls, path + [str(k)], visited)\n",
    "\n",
    "    elif isinstance(o, (list, tuple)):\n",
    "        for i, v in enumerate(o):\n",
    "            find_instances(v, cls, path + [str(i)], visited)\n",
    "\n",
    "    elif hasattr(o, '__dict__'):\n",
    "        for k, v in vars(o).items():\n",
    "            if k not in ignore_keys:\n",
    "                find_instances(v, cls, path + [str(k)], visited)\n",
    "\n",
    "\n",
    "find_instances(models[-1], collections.OrderedDict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
