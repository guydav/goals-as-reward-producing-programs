{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import difflib\n",
    "import gzip\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tabulate\n",
    "import tatsu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src import fitness_energy_utils as utils\n",
    "from src.fitness_energy_utils import NON_FEATURE_COLUMNS\n",
    "from src.ast_counter_sampler import *\n",
    "from src.ast_utils import cached_load_and_parse_games_from_file, load_games_from_file, _extract_game_id\n",
    "from src import ast_printer\n",
    "from src.fitness_features_by_category import FEATURE_CATEGORIES, PREDICATE_ROLE_FILLER_PATTERN_DICT, COUNTING_FEATURES_PATTERN_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='..'))\n",
    "real_game_texts = [ast_printer.ast_to_string(ast, '\\n') for ast in game_asts]\n",
    "regrown_game_1024_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples-1024.pddl.gz'))\n",
    "print(len(real_game_texts), len(regrown_game_1024_texts), len(regrown_game_1024_texts) / 98)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_game_index(game_name: str):\n",
    "    first_dash = game_name.find('-')\n",
    "    second_dash = game_name.find('-', first_dash + 1)\n",
    "    index = game_name[first_dash + 1:second_dash] if second_dash != -1 else game_name[first_dash + 1:]\n",
    "    return int(index)\n",
    "\n",
    "\n",
    "def extract_negative_index(game_name: str):\n",
    "    first_dash = game_name.find('-')\n",
    "    second_dash = game_name.find('-', first_dash + 1)\n",
    "    if second_dash == -1:\n",
    "        return -1\n",
    "    \n",
    "    third_dash = game_name.find('-', second_dash + 1)\n",
    "    index = game_name[second_dash + 1:third_dash]\n",
    "    return int(index)\n",
    "\n",
    "\n",
    "fitness_df = utils.load_fitness_data('../data/fitness_features_1024_regrowths.csv.gz')\n",
    "\n",
    "# fitness_df = fitness_df.assign(real=fitness_df.real.astype('int'), game_index=fitness_df.game_name.apply(extract_game_index), \n",
    "#                                negative_index= fitness_df.game_name.apply(extract_negative_index), fake=~fitness_df.real.astype('int'))\n",
    "# fitness_df = fitness_df.sort_values(by=['fake', 'game_index', 'negative_index'], ignore_index=True).reset_index(drop=True)\n",
    "# fitness_df.drop(columns=['Index', 'fake', 'game_index', 'negative_index'], inplace=True)\n",
    "print(fitness_df.src_file.unique())\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model weights/ranks from different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitness_features_by_category import FEATURE_CATEGORIES\n",
    "\n",
    "ignore_categories = [\n",
    "    \"forall_less_important\", \"counting_less_important\", \n",
    "    \"grammar_use_less_important\", \"predicate_under_modal\", \n",
    "    \"predicate_role_filler\", \"compositionality\"\n",
    "]\n",
    "\n",
    "fitness_model_paths = [\n",
    "    # 'in_data_prop_L2_categories_full_seed_33_2023_11_23',\n",
    "    # 'in_data_prop_L2_categories_full_seed_42_2023_11_23',\n",
    "    # 'in_data_prop_L2_categories_full_seed_66_2023_11_23',\n",
    "    # 'in_data_prop_L2_categories_full_seed_66_2023_11_23',\n",
    "    'in_data_prop_L2_categories_minimal_counting_grammar_use_forall_seed_42_2023_12_22',\n",
    "]\n",
    "\n",
    "loaded_models = [utils.load_model_and_feature_columns(model_path) for model_path in fitness_model_paths]\n",
    "table_models, table_feature_names = zip(*loaded_models)\n",
    "table_names = ['L2 (seed 33)']  # , 'L2 (seed 42)', 'L2 (seed 66)', 'L2 (seed 99)']\n",
    "\n",
    "all_feature_names = set()\n",
    "for feature_names in table_feature_names:\n",
    "    all_feature_names.update(feature_names)\n",
    "\n",
    "\n",
    "all_ignore_features = set()\n",
    "for category in ignore_categories:\n",
    "    for feature in FEATURE_CATEGORIES[category]:\n",
    "        if isinstance(feature, re.Pattern):\n",
    "            all_ignore_features.update([f for f in all_feature_names if feature.match(f)])\n",
    "        else:\n",
    "            all_ignore_features.add(feature)\n",
    "\n",
    "use_absolute_values = True\n",
    "\n",
    "weights_by_model = {}\n",
    "weight_ranks_by_model = {}\n",
    "\n",
    "\n",
    "for model, name, feature_names in zip(table_models, table_names, table_feature_names):\n",
    "    model_weights = model.named_steps['fitness'].model.fc1.weight.data.detach().squeeze()  # type: ignore\n",
    "    if use_absolute_values:\n",
    "        model_weights = torch.abs(model_weights)\n",
    "    model_weights_rank = stats.rankdata(model_weights.numpy())\n",
    "    if use_absolute_values:\n",
    "        model_weights_rank = len(model_weights_rank) - model_weights_rank + 1\n",
    "    weights_by_model[name] = {feature_names[i]: model_weights[i].item() for i in range(len(feature_names))}\n",
    "    weight_ranks_by_model[name] = {feature_names[i]: model_weights_rank[i] for i in range(len(feature_names))}\n",
    "\n",
    "\n",
    "feature_mean_rank = {\n",
    "    feature_name: np.nanmean([weights.get(feature_name, np.nan) for weights in weight_ranks_by_model.values()])\n",
    "    for feature_name in all_feature_names\n",
    "}\n",
    "\n",
    "\n",
    "mean_mean_rank_by_feature_number = defaultdict(list)\n",
    "for feature_name, mean_rank in feature_mean_rank.items():\n",
    "    if feature_name[-1].isdigit():\n",
    "        feature_name = feature_name[:-2]\n",
    "        mean_mean_rank_by_feature_number[feature_name].append(mean_rank)\n",
    "\n",
    "\n",
    "mean_rank_by_pref_forall_type = defaultdict(list)\n",
    "for feature_name, mean_rank in feature_mean_rank.items():\n",
    "    if 'pref_forall' in feature_name:\n",
    "        if feature_name.endswith('incorrect'):\n",
    "            mean_rank_by_pref_forall_type['incorrect'].append(mean_rank)\n",
    "        \n",
    "        elif feature_name.endswith('correct'):\n",
    "            mean_rank_by_pref_forall_type['correct'].append(mean_rank)\n",
    "\n",
    "        elif feature_name.endswith('incorrect_count'):\n",
    "            mean_rank_by_pref_forall_type['incorrect_count'].append(mean_rank)\n",
    "\n",
    "\n",
    "feature_names_by_mean_rank = sorted(feature_mean_rank.keys(), key=lambda feature_name: feature_mean_rank[feature_name], reverse=False)\n",
    "\n",
    "headers = ['Feature', 'Ignored', 'Mean Rank'] + table_names\n",
    "rows = [[feature_name, 'Yes' if feature_name in all_ignore_features else 'No',  f'{feature_mean_rank[feature_name]:.3f}'] + [f'{weights_by_model[name].get(feature_name, np.nan):.3f} ({int(weight_ranks_by_model[name].get(feature_name, -1))})'   \n",
    "                          for name in table_names] \n",
    "        for feature_name in feature_names_by_mean_rank]\n",
    "\n",
    "# with open('temp_outputs/features_by_mean_weight.tsv', 'w') as f:\n",
    "#     f.write(tabulate.tabulate(rows, headers, tablefmt='tsv'))\n",
    "\n",
    "print(tabulate.tabulate(rows, headers, tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mean_ranks = [(np.mean(ranks), feature_name, len(ranks)) for feature_name, ranks in mean_mean_rank_by_feature_number.items()]\n",
    "mean_mean_ranks.sort()\n",
    "\n",
    "for mean_rank, feature_name, n_features in mean_mean_ranks:\n",
    "    print(f'{feature_name}: {mean_rank:.3f} ({n_features})')\n",
    "\n",
    "print()\n",
    "\n",
    "higher_level_means = defaultdict(list)\n",
    "higher_level_totals = defaultdict(int)\n",
    "for mean_rank, feature_name, n_features in mean_mean_ranks:\n",
    "    last_underscore_index = feature_name.rfind('_')\n",
    "    category_name = feature_name[:last_underscore_index]\n",
    "    higher_level_means[category_name].append(mean_rank * n_features)\n",
    "    higher_level_totals[category_name] += n_features\n",
    "\n",
    "higher_level_means = {category_name: np.sum(means) / higher_level_totals[category_name] for category_name, means in higher_level_means.items()}\n",
    "\n",
    "for category_name, mean_rank in sorted(higher_level_means.items(), key=lambda x: x[1]):\n",
    "    print(f'{category_name}: {mean_rank:.3f}')\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "for pref_forall_type, mean_ranks in mean_rank_by_pref_forall_type.items():\n",
    "    print(f'{pref_forall_type}: {np.mean(mean_ranks):.3f} ({len(mean_ranks)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Older stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_BINARIZED_FEATURES_MODEL = True\n",
    "\n",
    "# if USE_BINARIZED_FEATURES_MODEL:\n",
    "#     model_path = '../models/cv_binarized_model_2023_01_20.pkl.gz'\n",
    "#     data_df = binarized_df\n",
    "# else:\n",
    "#     model_path = '../models/cv_fitness_model_2023_01_20.pkl.gz'\n",
    "#     data_df = filtered_fitness_df\n",
    "from latest_model_paths import LATEST_FITNESS_FUNCTION_DATE_ID, LATEST_SPECIFIC_OBJECTS_FITNESS_FUNCTION_DATE_ID\n",
    "# model_date_id = LATEST_FITNESS_FUNCTION_DATE_ID\n",
    "# model_date_id = 'in_data_prop_categories_minimal_counting_grammar_use_seed_42_2023_09_19'\n",
    "# model_date_id = 'cv_fitness_model_in_data_prop_L1_categories_minimal_counting_grammar_use_seed_42_2023_09_19'\n",
    "model_date_id = '1_4_regrowths_in_data_prop_L2_categories_minimal_counting_grammar_use_seed_42_2023_09_19'\n",
    "\n",
    "\n",
    "if '1_4_regrowths' in model_date_id:\n",
    "    data_df = utils.load_fitness_data('../data/fitness_features_1024_regrowths_1_4_regrowths.csv.gz')\n",
    "\n",
    "else:\n",
    "    data_df = fitness_df\n",
    "\n",
    "cv_energy_model, feature_columns = utils.load_model_and_feature_columns(model_date_id)\n",
    "print(len(feature_columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tensor = utils.df_to_tensor(data_df, feature_columns)\n",
    "if 'wrapper' in cv_energy_model.named_steps: cv_energy_model.named_steps['wrapper'].eval()\n",
    "full_tensor_scores = cv_energy_model.transform(full_tensor).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_game_scores = full_tensor_scores[:, 0]\n",
    "\n",
    "print(f'Real game scores: {real_game_scores.mean():.4f} ± {real_game_scores.std():.4f}, min = {real_game_scores.min():.4f}, median = {torch.median(real_game_scores):.4f}, max = {real_game_scores.max():.4f}')\n",
    "\n",
    "negatives_scores = full_tensor_scores[:, 1:]\n",
    "torch.quantile(negatives_scores.ravel(), torch.linspace(0, 1, 11))\n",
    "print(f'30th percentile negative energy: {torch.quantile(negatives_scores.ravel(), 0.3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.evaluate_fitness_overall_ecdf(cv_energy_model, full_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "positive_scores_numpy = real_game_scores.numpy().reshape(-1)\n",
    "negative_scores_numpy = negatives_scores.numpy().reshape(-1)\n",
    "\n",
    "n_positives = len(positive_scores_numpy)\n",
    "n_negatives = len(negative_scores_numpy)\n",
    "\n",
    "labels = np.concatenate([np.ones(n_positives), np.zeros(n_negatives)])\n",
    "scores = np.concatenate([positive_scores_numpy, negative_scores_numpy]) * -1  # flipping the signs of the energies  # type: ignore\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "metrics.PrecisionRecallDisplay.from_predictions(labels, scores)  # type: ignore\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print(metrics.average_precision_score(labels, scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "metrics.RocCurveDisplay.from_predictions(labels, scores)  # type: ignore\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_mean, positives_variance = positive_scores_numpy.mean(), positive_scores_numpy.var()\n",
    "negatives_mean, negatives_variance = negative_scores_numpy.mean(), negative_scores_numpy.var()\n",
    "\n",
    "-1 * (positives_mean - negatives_mean) / np.sqrt(0.5 * (positives_variance + negatives_variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = torch.linspace(0, 1, 11)\n",
    "deciles = torch.quantile(negatives_scores.ravel(), steps)\n",
    "print(steps)\n",
    "print(f'Energy deciles: {deciles}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cv_energy_model.named_steps['fitness'].model.fc1.weight.data.detach().squeeze()  # type: ignore\n",
    "bias = cv_energy_model.named_steps['fitness'].model.fc1.bias.data.detach().squeeze()  # type: ignore\n",
    "print(f'Weights mean: {weights.mean():.4f}, std: {weights.std():.4f}, bias: {bias:.4f}')\n",
    "\n",
    "plt.hist(weights, bins=30)\n",
    "plt.title('Energy model weights')\n",
    "plt.xlabel('Weight magnitude')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in feature_columns if 'n_5' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.abs().max() / weights.abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20\n",
    "top_features = torch.topk(weights, K)\n",
    "bottom_features = torch.topk(weights, K, largest=False)\n",
    "\n",
    "lines = []\n",
    "\n",
    "lines.append('### Features with largest negative weights (most predictive of real games):')\n",
    "for i in range(K):\n",
    "    lines.append(f'{i+1}. {feature_columns[bottom_features.indices[i]]} ({bottom_features.values[i]:.4f})')\n",
    "\n",
    "lines.append('### Features with largest positive weights (most predictive of fake games):')\n",
    "for i in range(K):\n",
    "    lines.append((f'{i+1}. {feature_columns[top_features.indices[i]]} ({top_features.values[i]:.4f})'))\n",
    "\n",
    "display(Markdown('\\n'.join(lines)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "quantile_index = 0\n",
    "\n",
    "abs_weights = weights.abs()\n",
    "\n",
    "for magnitude in torch.linspace(0,abs_weights.max(), 5000):\n",
    "    n = torch.sum(abs_weights < magnitude).item()\n",
    "    if n / len(weights) >= quantiles[quantile_index]:\n",
    "        print(f'Approximately {quantiles[quantile_index] * 100}% ({n}, {n / len(weights) * 100:.2f}%) of the weights have magnitude < {magnitude:.4f}')\n",
    "        quantile_index += 1\n",
    "\n",
    "    if quantile_index >= len(quantiles):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fitness_features_by_category import *\n",
    "\n",
    "\n",
    "def print_weights_summary_by_category(model_weights: torch.Tensor, all_feature_columns: typing.List[str], return_lines: bool = False):\n",
    "    abs_weights = model_weights.abs()\n",
    "    sorted_feature_names = [t[1] for t in sorted([(abs_weights[i], all_feature_columns[i]) for i in range(len(all_feature_columns))], key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "    lines = []\n",
    "    all_assigned_features = set()\n",
    "\n",
    "    for category, features in FEATURE_CATEGORIES.items():\n",
    "        category_feature_list = []\n",
    "        for feature in features:\n",
    "            if isinstance(feature, re.Pattern):\n",
    "                category_feature_list.extend([f for f in feature_columns if feature.match(f)])\n",
    "\n",
    "            else:\n",
    "                category_feature_list.append(feature)\n",
    "\n",
    "        all_assigned_features.update(category_feature_list)\n",
    "        \n",
    "        mean_abs_weight = np.mean([abs_weights[feature_columns.index(feature)] for feature in category_feature_list])\n",
    "        sum_abs_weight = np.sum([abs_weights[feature_columns.index(feature)] for feature in category_feature_list])\n",
    "        mean_sorted_index = np.mean([sorted_feature_names.index(feature) for feature in category_feature_list])\n",
    "        prefix = f'For category {category} with {len(category_feature_list)} features'\n",
    "        line = f'{prefix:54} | mean abs weight is {mean_abs_weight:5.2f} | sum abs weight is {sum_abs_weight:6.2f} | mean sorted index is {mean_sorted_index:6.2f}'\n",
    "        if return_lines:\n",
    "            lines.append(line)\n",
    "        else:\n",
    "            print(line)\n",
    "\n",
    "    unassigned_features = [f for f in feature_columns if f not in all_assigned_features]\n",
    "    if len(unassigned_features) > 0:\n",
    "        print(f'Unassigned features: {unassigned_features}')\n",
    "\n",
    "    if return_lines:\n",
    "        return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mean_weight_by_category_pattern(model_weights: torch.Tensor, model_feature_columns: typing.List[str], \n",
    "                                          patterns: typing.Dict[str, re.Pattern], top_k: int = 3, prefix_width: int = 40,\n",
    "                                          sort_by: str = 'mean_abs_weight'):\n",
    "    \n",
    "    abs_weights = model_weights.abs()\n",
    "    lines_and_weights = []\n",
    "    for name, pattern in patterns.items():\n",
    "        feature_indices = [i for i, feature in enumerate(model_feature_columns) if pattern.match(feature)]\n",
    "        if len(feature_indices) == 0:\n",
    "            continue\n",
    "        mean_abs_weight = abs_weights[feature_indices].mean().item()\n",
    "        max_abs_weight = abs_weights[feature_indices].max().item()\n",
    "        k = top_k\n",
    "        if k > len(feature_indices):\n",
    "            k = len(feature_indices)\n",
    "        mean_top_k_abs_weight = abs_weights[feature_indices].topk(k).values.mean().item()\n",
    "        prefix = f'For \"{name}\" features'.ljust(prefix_width)\n",
    "        line = f'{prefix} | {len(feature_indices):2} features | mean weight = {mean_abs_weight:2.3f} | mean top {k} weights = {mean_top_k_abs_weight:2.3f} | max weight = {max_abs_weight:2.3f}'\n",
    "        if sort_by == 'mean_abs_weight':\n",
    "            sort_key = mean_abs_weight\n",
    "        elif sort_by == 'mean_top_k_abs_weight':\n",
    "            sort_key = mean_top_k_abs_weight\n",
    "        elif sort_by == 'max_abs_weight':\n",
    "            sort_key = max_abs_weight\n",
    "        else:\n",
    "            raise ValueError(f'Unknown sort_by value {sort_by}')\n",
    "        \n",
    "        lines_and_weights.append((line, sort_key))\n",
    "\n",
    "    lines_and_weights.sort(key=lambda x: x[1], reverse=True)\n",
    "    for line, _ in lines_and_weights:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "\n",
    "def print_top_features_by_category(model_weights: torch.Tensor, model_feature_columns: typing.List[str], category: str, k: int = 10):\n",
    "    category = category.lower()\n",
    "    if category not in FEATURE_CATEGORIES:\n",
    "        raise ValueError(f'Category {category} not found')\n",
    "    \n",
    "    category_features = []\n",
    "    for category_feature_or_pattern in FEATURE_CATEGORIES[category]:\n",
    "        if isinstance(category_feature_or_pattern, re.Pattern):\n",
    "            category_features.extend([f for f in model_feature_columns if category_feature_or_pattern.match(f)])\n",
    "        else:\n",
    "            category_features.append(category_feature_or_pattern)\n",
    "\n",
    "    sorted_features = sorted([(model_weights[i], model_feature_columns[i]) for i in range(len(model_feature_columns))], key=lambda x: x[0], reverse=False)\n",
    "    sorted_category_features = [f for f in sorted_features if f[1] in category_features]\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    lines.append(f'### {category.capitalize()} features with largest negative weights:')\n",
    "    for i in range(k):\n",
    "        lines.append(f'{i+1}. {sorted_category_features[i][1]} ({sorted_category_features[i][0]:.4f})')\n",
    "\n",
    "    lines.append(f'### {category.capitalize()} features with largest positive weights:')\n",
    "    for i in range(k):\n",
    "        lines.append(f'{i+1}. {sorted_category_features[-(i + 1)][1]} ({sorted_category_features[-(i + 1)][0]:.4f})')\n",
    "\n",
    "    display(Markdown('\\n'.join(lines)))\n",
    "\n",
    "\n",
    "def print_weights_in_category(model_weights: torch.Tensor, model_feature_columns: typing.List[str], category: str, model_note: typing.Optional[str] = None, \n",
    "                              small_threshold: float = 1e-2, large_threshold: float = 1e-1):\n",
    "    category = category.lower()\n",
    "    if category not in FEATURE_CATEGORIES:\n",
    "        raise ValueError(f'Category {category} not found')\n",
    "    \n",
    "    category_features = []\n",
    "    for category_feature_or_pattern in FEATURE_CATEGORIES[category]:\n",
    "        if isinstance(category_feature_or_pattern, re.Pattern):\n",
    "            category_features.extend([f for f in model_feature_columns if category_feature_or_pattern.match(f)])\n",
    "        else:\n",
    "            category_features.append(category_feature_or_pattern)\n",
    "\n",
    "    sorted_features = sorted([(model_weights[i], model_feature_columns[i]) for i in range(len(model_feature_columns))], key=lambda x: x[0], reverse=False)\n",
    "    sorted_category_features = [f for f in sorted_features if f[1] in category_features]\n",
    "\n",
    "    lines = []\n",
    "    if model_note is not None:\n",
    "        lines.append(f'### {category.capitalize()} features by weight ({model_note}):')\n",
    "    else:    \n",
    "        lines.append(f'### {category.capitalize()} features by weight:')\n",
    "    for i, (weight, feature) in enumerate(sorted_category_features):\n",
    "        threshold_marker = \"\"\n",
    "        if weight.abs() > large_threshold:\n",
    "            threshold_marker = \"**\"\n",
    "        elif weight.abs() > small_threshold:\n",
    "            threshold_marker = \"*\"\n",
    "        lines.append(f'{i+1}. {feature} ({weight:.4f}){threshold_marker}')\n",
    "\n",
    "    display(Markdown('\\n'.join(lines)))\n",
    "\n",
    "\n",
    "def get_model_weights(model):\n",
    "    return model.named_steps['fitness'].model.fc1.weight.data.detach().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights_in_category(get_model_weights(cv_energy_model), feature_columns, 'grammar_use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_and_names = [\n",
    "    ('2023_09_13', 'fitness_sweep_1_4_regrowths_no_in_data_all_L1_categories_minimal_counting_seed_33'),\n",
    "    ('2023_09_13', 'fitness_sweep_1_4_regrowths_no_in_data_all_L1_categories_minimal_counting_seed_42'),\n",
    "    ('2023_09_13', 'fitness_sweep_1_4_regrowths_no_in_data_all_L1_categories_minimal_counting_seed_66'),\n",
    "]\n",
    "\n",
    "final_model_names = [\n",
    "    '1_4_regrowths_no_in_data_all_L1_categories_minimal_counting_seed_33_2023_09_13',\n",
    "    '1_4_regrowths_no_in_data_all_L1_categories_minimal_counting_seed_42_2023_09_13',\n",
    "    '1_4_regrowths_no_in_data_all_L1_categories_minimal_counting_seed_66_2023_09_13',\n",
    "]\n",
    "\n",
    "\n",
    "for model_identifier in final_model_names:\n",
    "# for model_identifier in dates_and_names:\n",
    "    # display(Markdown(f'## {name}'))\n",
    "    current_cv_data = None\n",
    "    if isinstance(model_identifier, tuple):\n",
    "        date_id, name = model_identifier\n",
    "        current_cv_data = utils.load_data(date_id, 'data/fitness_cv', name)\n",
    "        current_model = current_cv_data['cv'].best_estimator_\n",
    "        current_feature_columns = current_cv_data['feature_columns']\n",
    "\n",
    "    else:\n",
    "        current_model, current_feature_columns = utils.load_model_and_feature_columns(model_identifier)\n",
    "    # print_top_features_by_category(get_model_weights(l1_with_counting_cv_data['cv'].best_estimator_), l1_with_counting_cv_data['feature_columns'], 'counting', k=10)\n",
    "\n",
    "    # print(f'For model {model_identifier}: {current_cv_data[\"cv\"].best_params_}')\n",
    "\n",
    "    model_note = None\n",
    "    if current_cv_data is not None:\n",
    "        model_note = f\"L1 = {current_cv_data['cv'].best_params_['fitness__regularization_weight']:1.2f}\"\n",
    "\n",
    "    print_weights_in_category(get_model_weights(current_model), current_feature_columns, 'grammar_use',\n",
    "                              model_note=model_note)\n",
    "\n",
    "    # print_mean_weight_by_category_pattern(\n",
    "    #     get_model_weights(current_model),\n",
    "    #     current_feature_columns, COUNTING_FEATURES_PATTERN_DICT,\n",
    "    #     sort_by='mean_abs_weight',\n",
    "    # )\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_and_names = [\n",
    "    ('2023_09_13', 'fitness_sweep_no_in_data_all_L1_categories_minimal_counting_seed_33'),\n",
    "    ('2023_09_13', 'fitness_sweep_no_in_data_all_L1_categories_minimal_counting_seed_42'),\n",
    "    ('2023_09_13', 'fitness_sweep_no_in_data_all_L1_categories_minimal_counting_seed_66'),\n",
    "]\n",
    "\n",
    "final_model_names = [\n",
    "    'no_in_data_all_L1_categories_minimal_counting_seed_33_2023_09_13',\n",
    "    'no_in_data_all_L1_categories_minimal_counting_seed_42_2023_09_13',\n",
    "    'no_in_data_all_L1_categories_minimal_counting_seed_66_2023_09_13',\n",
    "]\n",
    "\n",
    "\n",
    "# for model_identifier in final_model_names:\n",
    "for model_identifier in dates_and_names:\n",
    "    # display(Markdown(f'## {name}'))\n",
    "    current_cv_data = None\n",
    "    if isinstance(model_identifier, tuple):\n",
    "        date_id, name = model_identifier\n",
    "        current_cv_data = utils.load_data(date_id, 'data/fitness_cv', name)\n",
    "        current_model = current_cv_data['cv'].best_estimator_\n",
    "        current_feature_columns = current_cv_data['feature_columns']\n",
    "\n",
    "    else:\n",
    "        current_model, current_feature_columns = utils.load_model_and_feature_columns(model_identifier)\n",
    "    # print_top_features_by_category(get_model_weights(l1_with_counting_cv_data['cv'].best_estimator_), l1_with_counting_cv_data['feature_columns'], 'counting', k=10)\n",
    "\n",
    "    # print(f'For model {model_identifier}: {current_cv_data[\"cv\"].best_params_}')\n",
    "\n",
    "    model_note = None\n",
    "    if current_cv_data is not None:\n",
    "        model_note = f\"L1 = {current_cv_data['cv'].best_params_['fitness__regularization_weight']:1.2f}\"\n",
    "\n",
    "    print_weights_in_category(get_model_weights(current_model), current_feature_columns, 'grammar_use',\n",
    "                              model_note=model_note)\n",
    "\n",
    "    # print_mean_weight_by_category_pattern(\n",
    "    #     get_model_weights(current_model),\n",
    "    #     current_feature_columns, COUNTING_FEATURES_PATTERN_DICT,\n",
    "    #     sort_by='mean_abs_weight',\n",
    "    # )\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_counting_dates_and_names = [\n",
    "    ('2023_09_05', 'fitness_sweep_full_features_no_in_data_all_L1_categories_with_counting'),\n",
    "    ('2023_09_07_1', 'fitness_sweep_full_features_no_in_data_all_L1_categories_with_role_filler'),  # I mucked up the names here\n",
    "    ('2023_09_07', 'fitness_sweep_full_features_no_in_data_all_L1_categories_with_role_filler'), # I mucked up the names here\n",
    "]\n",
    "\n",
    "with_counting_final_model_date_ids = [\n",
    "    'full_features_no_in_data_all_L1_categories_with_counting_2023_09_05',\n",
    "    'full_features_no_in_data_all_L1_categories_with_role_filler_seed_42_2023_09_07', # I mucked up the names here\n",
    "    'full_features_no_in_data_all_L1_categories_with_role_filler_seed_66_2023_09_07' # I mucked up the names here\n",
    "]\n",
    "\n",
    "# for model_identifier in with_counting_dates_and_names:\n",
    "for model_identifier in with_counting_final_model_date_ids:\n",
    "    # display(Markdown(f'## {name}'))\n",
    "    if isinstance(model_identifier, tuple):\n",
    "        date_id, name = model_identifier\n",
    "        current_cv_data = utils.load_data(date_id, 'data/fitness_cv', name)\n",
    "        current_model = current_cv_data['cv'].best_estimator_\n",
    "        current_feature_columns = current_cv_data['feature_columns']\n",
    "\n",
    "    else:\n",
    "        current_model, current_feature_columns = utils.load_model_and_feature_columns(model_identifier)\n",
    "    # print_top_features_by_category(get_model_weights(l1_with_counting_cv_data['cv'].best_estimator_), l1_with_counting_cv_data['feature_columns'], 'counting', k=10)\n",
    "\n",
    "    print_mean_weight_by_category_pattern(\n",
    "        get_model_weights(current_model),\n",
    "        current_feature_columns, COUNTING_FEATURES_PATTERN_DICT,\n",
    "        sort_by='mean_abs_weight',\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_role_filler_dates_and_names = [\n",
    "    ('2023_09_05', 'fitness_sweep_full_features_no_in_data_all_L1_categories_with_role_filler'),\n",
    "    ('2023_09_07_1', 'fitness_sweep_full_features_no_in_data_all_L1_categories_with_counting'),  # I mucked up the names here\n",
    "    ('2023_09_07', 'fitness_sweep_full_features_no_in_data_all_L1_categories_with_counting'), # I mucked up the names here\n",
    "]\n",
    "\n",
    "with_role_filler_final_model_date_ids = [\n",
    "    'full_features_no_in_data_all_L1_categories_with_role_filler_2023_09_05',\n",
    "    'full_features_no_in_data_all_L1_categories_with_counting_seed_42_2023_09_07', # I mucked up the names here\n",
    "    'full_features_no_in_data_all_L1_categories_with_counting_seed_66_2023_09_07' # I mucked up the names here\n",
    "]\n",
    "\n",
    "# for model_identifier in with_role_filler_dates_and_names:\n",
    "for model_identifier in with_role_filler_final_model_date_ids:\n",
    "    # display(Markdown(f'## {name}'))\n",
    "    if isinstance(model_identifier, tuple):\n",
    "        date_id, name = model_identifier\n",
    "        current_cv_data = utils.load_data(date_id, 'data/fitness_cv', name)\n",
    "        current_model = current_cv_data['cv'].best_estimator_\n",
    "        current_feature_columns = current_cv_data['feature_columns']\n",
    "\n",
    "    else:\n",
    "        current_model, current_feature_columns = utils.load_model_and_feature_columns(model_identifier)\n",
    "    # print_top_features_by_category(get_model_weights(l1_with_counting_cv_data['cv'].best_estimator_), l1_with_counting_cv_data['feature_columns'], 'counting', k=10)\n",
    "\n",
    "    print_mean_weight_by_category_pattern(\n",
    "        get_model_weights(current_model),\n",
    "        current_feature_columns, PREDICATE_ROLE_FILLER_PATTERN_DICT,\n",
    "        sort_by='mean_abs_weight',\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at comparing the models on the negatives from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reg_sweep_data = utils.load_data('2023_08_29', 'data/fitness_cv', 'fitness_sweep_full_features_no_in_data_all')\n",
    "print(no_reg_sweep_data.keys())\n",
    "no_reg_test_scores = no_reg_sweep_data['cv'].best_estimator_.transform(no_reg_sweep_data['test_tensor'])\n",
    "no_reg_test_real_game_scores = no_reg_test_scores[:, 0]\n",
    "no_reg_test_negatives_scores = no_reg_test_scores[:, 1:]\n",
    "\n",
    "no_reg_argsort = torch.argsort(no_reg_test_negatives_scores.ravel())\n",
    "no_reg_position_to_index = torch.zeros_like(no_reg_argsort)\n",
    "no_reg_position_to_index[no_reg_argsort] = torch.arange(len(no_reg_argsort))\n",
    "\n",
    "no_reg_indices_with_better_negative = set(torch.where((no_reg_test_negatives_scores < no_reg_test_real_game_scores[:, None, :]).ravel())[0].numpy())\n",
    "print(len(no_reg_indices_with_better_negative))\n",
    "\n",
    "\n",
    "\n",
    "l1_sweep_data = utils.load_data('2023_08_29', 'data/fitness_cv', 'fitness_sweep_full_features_no_in_data_all_L1')\n",
    "l1_test_scores = l1_sweep_data['cv'].best_estimator_.transform(l1_sweep_data['test_tensor'])\n",
    "l1_test_real_game_scores = l1_test_scores[:, 0]\n",
    "l1_test_negatives_scores = l1_test_scores[:, 1:]\n",
    "\n",
    "l1_argsort = torch.argsort(l1_test_negatives_scores.ravel())\n",
    "l1_position_to_index = torch.zeros_like(l1_argsort)\n",
    "l1_position_to_index[l1_argsort] = torch.arange(len(l1_argsort))\n",
    "\n",
    "l1_indices_with_better_negative = set(torch.where((l1_test_negatives_scores < l1_test_real_game_scores[:, None, :]).ravel())[0].numpy())\n",
    "print(len(l1_indices_with_better_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_energy_histogram(\n",
    "    no_reg_sweep_data['cv'], \n",
    "    no_reg_sweep_data['train_tensor'], \n",
    "    no_reg_sweep_data['test_tensor'],\n",
    "    histogram_title_note='no regularization')\n",
    "\n",
    "utils.plot_energy_histogram(\n",
    "    l1_sweep_data['cv'],\n",
    "    l1_sweep_data['train_tensor'],\n",
    "    l1_sweep_data['test_tensor'],\n",
    "    histogram_title_note='L1 regularization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_negative_both = no_reg_indices_with_better_negative.intersection(l1_indices_with_better_negative)\n",
    "better_negative_l1_only = l1_indices_with_better_negative.difference(no_reg_indices_with_better_negative)\n",
    "better_negative_no_reg_only = no_reg_indices_with_better_negative.difference(l1_indices_with_better_negative)\n",
    "\n",
    "print(f'Both: {len(better_negative_both)}, L1 only: {len(better_negative_l1_only)}, No reg only: {len(better_negative_no_reg_only)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_diffs = no_reg_position_to_index - l1_position_to_index\n",
    "index_diffs.topk(10, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.evaluate_energy_contributions(cv_energy_model, full_tensor, 987, \n",
    "        feature_columns, full_tensor, real_game_texts, regrown_game_1024_texts, display_features_diff=False, min_display_threshold=0.001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.evaluate_energy_contributions(l1_model, full_tensor, 987, \n",
    "        feature_columns, full_tensor, real_game_texts, regrown_game_1024_texts, display_features_diff=False, min_display_threshold=0.001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_categories_sweep_data = utils.load_data('2023_09_01', 'data/fitness_cv', 'fitness_sweep_full_features_no_in_data_all_L1_categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_cv_outputs(\n",
    "    l1_categories_sweep_data['cv'],\n",
    "    l1_categories_sweep_data['train_tensor'],\n",
    "    l1_categories_sweep_data['test_tensor']\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
